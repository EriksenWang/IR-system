{
    "title": "N/A",
    "publication_date": "1991",
    "authors": [],
    "abstract": "N/A",
    "full_text": "TABLE PAGE 31 Chemistry study items and number of quiz categories 32 Access to correct answer 41 Users from the experimental and control group 42 Users Website transaction 43 Group Statistics for user transaction time elapsed rates 44 Independent Sample t-test for user transaction time elapsed rates 45 Independent Sample t-test for number of user interactions with the Website 46 Independent Sample t-test for number of user interactions with the Website 47 Group Statistics for correctly answered practice items 48 Independent Sample t-test for correctly answered practice items 49 Group Statistics for Incorrectly answered practice items 410 Independent Sample t-test for incorrectly answered practice items 411 Group Statistics for tutor access 412 Independent Sample t-test for tutor access 413 Descriptive Statistics for worked example-1 and worked example-2 Group Statistics for eight item practice quizzes 415 Independent Sample t-test for eight item practice quizzes 416 Group Statistics for surprise quizzes 417 Independent Sample t-test for surprise quizzes FIGURE PAGE 31 Descriptive Chemistry Website log-in page 32 Informed consent 33 Descriptive chemistry study site showing three entry areas 34 A chemistry quiz question item 35 Worked example 36 A Chemistry quiz question item feedback answer 37 A chemistry quiz question item feedback answer with reported errors and accepted answers information 38 Student review of transaction record 39 Sample Surprise Quiz 310 Student transaction record XML file 41 Month-wise user first logins 42 Month-wise serious user registrations 43 Box plot of user time elapsed rates 44 Users vs Number of interactions with the chemistry Website 45 Number of worked examples accessed by users 46 Number of times users reviewed their records 51 Surprise test items\n\nThe College Board, a not-for-profit organization, has offered the Advanced Placement Program ® (AP) to millions of students in United States and other countries since 1955 allowing them to take college-level courses and exams for earning college credit or placement while still in high school (College Board, 2006). According to the College Board (2006), 68 percent of United States public schools now participate in AP.\n\nSince 2000, students from all 50 states and the District of Columbia have succeeded on the AP exam. In 2005, The College Board reported a national total of 57,102 exam takers who were qualified for receiving chemistry college credit or advanced placement.\n\nThe AP chemistry course is broken down into five major topic areas indicated by percentage of approximate proportion of multiple choice questions that pertain to each topic: structure of matter (20%), states of matter (20%), reactions (35-40%), descriptive chemistry (10-15%), and laboratory (5-10%) (College Board, 2006). Crippen (2000) states: \"Anecdotal evidence suggests that the descriptive chemistry section of the AP chemistry exam is traditionally difficult for high school students. The exam's difficulty can be attributed to the nature of the material and the current structure of the AP curriculum. Descriptive chemistry is difficult to teach because it requires either a large amount of memorization or experience; it tends to be disjointed within the traditional curriculum.\" The College Board outlines descriptive chemistry as follows (College Board, 2006):\n\n\"Knowledge of specific facts of chemistry is essential for an understanding of principles and concepts. These descriptive facts, including the chemistry involved in environmental and societal issues, should not be isolated from the principles being studied but should be taught throughout the course to illustrate and illuminate the principles. The following areas should be covered:\n\n1. Chemical reactivity and products of chemical reactions 2. Relationships in the periodic table: horizontal, vertical, and diagonal with examples from alkali metals, alkaline earth metals, halogens, and the first series of transition elements 3. Introduction to organic chemistry: hydrocarbons and functional groups (structure, nomenclature, chemical properties). Physical and chemical properties of simple organic compounds should also be included as exemplary material for the study of other areas such as bonding, equilibria involving weak acids, kinetics, colligative properties, and stoichiometric determinations of empirical and molecular formulas.\"\n\nThis study was based on the review of three research studies carried out in 1999\n\nand 2000, and 2005 on the Descriptive Chemistry Website.\n\nThe chemistry Website was developed in 1997 and made accessible since then as a learning aid to serve users all over the world. The Website was dedicated solely to the descriptive portion of the advanced placement chemistry exam. About 200 descriptive chemistry questions in the form of quiz items were devised. These were intended to help high school students prepare for the AP exam, and for chemistry teachers to use as a learning or teaching resource. The questions were stored in a database. They were served to the user upon request. Students were allowed to practice chemistry quizzes repeatedly.\n\nFeedback was provided for student's errors in responding to the quiz questions. The following is an example of a descriptive chemistry quiz question:\n\nA solution of tin (II) chloride is added to a solution of iron (III) chloride.\n\nA correct answer response to the above question would be:\n\nThe 1999 study is reported by Crippen (2000) who notes:\n\n\"An Analysis of the Web server log for 1999 suggests the following conclusions:\n\n• The maximum number of hits in a single day (n = 1,336) occurred on Sunday, May 16, 1999, the day before the 1999 AP exam.\n\n• While the site shows consistent use, its use tended to be cyclic around the AP exams. For the 1999 term, site use reached a peak within the week before the AP exam. Use culminated the day before the exam, and dropped off significantly after the exam.\n\n• 77.7 percent of the users requested to have hints sent with their quizzes. An almost equal number of hits were for exams as for grading responses, including answer keys; 94.2% of the exams were graded.\n\n• The site had consistent use between the hours of 10:00 a.m. and 11:00 p.m.;\n\nSunday and Wednesday were peak days, though use is consistent throughout the week. These two statistics suggest strongly that the site was used in classrooms at schools.\n\n• Self-reported AP chemistry students make up the largest proportion of the users. AP students are using the site between 10:00 a.m. and 11:00 p.m. with heaviest use closer to 10:00 a.m. High school teachers are using the site at the same time as the AP students, yet their use is heavier towards the later hours of the school day.\"\n\nOne of the major conclusions from the 1999 data suggests that AP chemistry students and/or teachers used the chemistry Website extensively as a learning tool. In the 2000 study, Crippen questions that the 1999 study did not provide documentation about how the Website was used as a learning tool and what learning occured. Another conclusion from the 1999 study suggests that the Website provided tutoring components and feedback to the users. Crippen (2000) states that not only the effectiveness of the tutorial components but also the implications for the use of tutoring components by users for chemistry learning were not documented in the 1999 study.\n\nHence, in the 2000 study, user patterns including user access to tutorials and feedback were tracked and stored in the database. This was carried out in order to understand the users' perspective in learning descriptive chemistry. The Website operated as follows. Once a user logged in and requested a quiz, a randomly generated set of eightitems was drawn from 14 arbitrary categories defined by the researcher. Each quiz item came from a different category, and quiz items were not duplicated. The user might or might not opt for tutoring when requesting quizzes. The tutoring included detailed text instruction, sometimes with images, and the category from which the quiz item was posed to the user.\n\nFurther, user perceptions were analyzed by conducting a post-AP-examination survey. Comparisons were made between individual users' perception about the effectiveness of the site and site usage. The study demonstrated that effective teaching and learning could be conducted over the Web through repetitive practice and feedback.\n\nHowever, in this study survey, respondents indicated problems with formula typing and bias in the scoring due to formula entry problems. Further, the quiz design lacked flexibility in providing tutoring and item-specific feedback. Crippen (2000) recommended enhancements in three areas for the AP Descriptive Chemistry Website for maximizing student learning. First was the redesign of the user interface to provide appropriate options for users to access chemistry Website learning materials. The options include menus that help in constructing formulas or formula typing for quizzes and eliminating bias in graded quizzes. Second, enhanced feedback and tutorial components were developed for each quiz item. This included providing appropriate tutoring material related to each quiz item and feedback given to students based on their responses to the quizzes. Third, users were not permitted to resubmit graded quiz items.\n\nA more recent study at the modified Website, Crippen and Brooks (2005) analyzed over 250,000 incorrect responses made by students.\n\nStudent errors were recorded and categorized. It was found that the highest percentages of errors were misconceptions (28%) and typographical errors (28%). Next highest percentages of errors included inappropriate \"chemical\" dissociations (27%) and/or writing incorrect formulas. The rest of the errors included not recognizing weak electrolytes and including spectator ions. Purpose of the Study The current study is based on the conclusions and recommendations from the past studies. The rationale for conducting this study is to minimize student errors and misconceptions in order to improve learning and overall performance of students preparing for the AP exam. The redesigned Website focuses on providing tutoring components for quiz categories and specific performance-related feedback for each quiz item. The Descriptive Chemistry Website was redesigned by adopting effective instructional design methods based on the cognitive design principles for fostering learning. While providing worked examples and feedback was the main target of this study, tutoring and a redesign also were included. Research Questions 1. Does a student's performance in descriptive chemistry improve with descriptive practicing chemistry problems? 2. Does the use of worked examples decrease the rate of errors when solving descriptive chemistry problems? Significance of the Study\n\nThis study is important for two primary reasons. One is the notion of using a\n\nWeb-based tool and the application of technology to enhance teaching and learning. The other is the development of teaching and learning strategies that help in improving the performance of students solving chemistry problems.\n\nIn this study, three major arguments are hypothesized.\n\nFirst, it is hypothesized that the repetitive practicing with worked examples will enhance learning in users relative to practicing quiz items alone.\n\nSecond, it is hypothesized that tutoring components in the study will help students in learning the chemistry material.\n\nThird, it is hypothesized that specific feedback provided in response to the submitted answers will improve learners' performance.\n\nThe definition of learning used in the study by Crippen (2000) will be used here:\n\n\"Learning is defined as an increase in the quiz score, or item score as a function of time or the number of graded quizzes returned.\" Data analysis is performed from the automatically recorded transaction records of the users.\n\nThe current design of the study has limitations and advantages. The redesigned Descriptive Chemistry Website attempts to retain the advantages but to minimize the limitations raised in the past studies.\n\nThe Website allows anonymous user log-ins with valid or invalid email address and password, and the researcher is unable to verify such user information. As mentioned by Crippen (2000), \"the advantage of covertly tracking a self-motivated user's actions in an environment designed to produce learning makes this study appealing.\"\n\n\"Hardware and software have the potential to limit any study where they are significant components. Computers crash, hard drives fail, and software programs do not function as advertised. All of these hardware/software issues have the potential to limit the study\" (Crippen, 2000). The current Website is served to the World Wide Web via a computer server for increasing the speed of information transfer through the Internet.\n\nThis was made possible by utilizing greater bandwidth and transmission speed modem lines. Unlike in the previous studies, where the Websites were developed on HyperCard™-based programming techniques, the current study was redesigned completely by utilizing the scripting programming methods of Runtime Revolution™.\n\nSimilar to the 2000 study, this design strategy provided a powerful environment for automatically tracking user information, generating and grading quizzes, and capturing users' actions while interacting with the site. It allowed the researcher to remain removed from the users and the data until the completion of the study.\n\nOther similar aspects retained from the previous studies were:\n\n• the unique characteristics of the users that include accessing the Internet and having basic computing skills for learning online material.\n\n• the researcher remains un-associated with the Website.\n\nThe current study did not collect user's perspectives of the Website. That is, no follow-up surveys were undertaken.\n\nThe following defined terms are required the purpose of this study:\n\nFeedback: \"Any message generated in response to a learner's action. The outcomes of feedback include helping learners to identify errors and become aware of misconceptions\" (Mason & Bruning, 2000).\n\nRedesigned user interface: Enhancement in the appearance of the Website features to make a more user-friendly Website as well as to provide more user options.\n\nRuntime Revolution™: A Scripting language and developmental tool used to develop the current AP Descriptive Chemistry Website.\n\nServer Request: Commonly referred to as hit on the Web server. A server request involves a user asking the Web server to do something. This includes sending/grading a quiz or tutoring. Al user-initiated interactions with the Web server are defined as server requests.\n\nTutoring: Tutoring is a detailed instruction of the category from which the chemistry question was posed to the user.\n\nWorked Example: \"A worked example is a step-by-step demonstration of how to solve a problem or perform a task\" (Clark & Mayer, 2003).\n\nResearchers divided memory processes into stages of acquisition, storage, and retrieval of information. Sweller's (1999) 'modal model' of memory consists of three types of memory: sensory memory, working memory, and long-term memory. Sensory memory refers to the perception of things by the incoming stimuli from our senses.\n\nInformation is initially processed in sensory memory and then passed to working memory. Working memory refers to the conscious cognitive processing of information that occurs and has a very limited capacity in terms of the amount or complexity of information that can be retained or processed at once. According to Miller (1956), humans can process only seven plus or minus two elements at a time. If the limit exceeded, learners experience what is called \"cognitive overload\" and no learning takes place. However, in the recent literature, Sweller points out that working memory can handle only a very limited number (possibly no more than two or three) novel interacting elements (Paas et al., 2003). They further note that working memory can process simple information very easily when carrying out cognitive activities. Long term memory stores general world knowledge. The information that is present can be very sophisticated and enable learners to perceive, think, and problem-solve. It is more than a mere memorization of learned facts. The mental structures formed in memory are called schema or schemata. They compose the knowledge base which plays a crucial role in the human thinking process.\n\nAccording to Sweller (1999), cognitive load theory assumes that some learning environments impose greater demand than others and, as a consequence, impose a higher information processing load on limited cognitive resources in working memory.\n\nCognitive load depends upon the degree to which one efficiently rehearses tasks and skills over various domains and gains knowledge over a period of time. Human cognitive architecture interacts with instructional material in various ways. Information present in human memory varies on a continuum of low-high interactivity. \"Each element of low interactivity material can be understood and learned individually without consideration of any other elements. The elements of high interactivity material can be learned individually, but they cannot be understood until all of the elements and their interactions are processed simultaneously. As a consequence, high interactivity materials are difficult to understand\" (Paas et al., 2003).\n\nBased on different sources of cognitive load, Sweller (1999) distinguished three types of load: Intrinsic, extraneous, and germane load.\n\nElement interactivity is the driver of this category because demands on working memory capacity imposed by element interactivity are intrinsic to the material being learned. Intrinsic cognitive load cannot be altered by instructional manipulations. A simpler learning task that omits some interacting elements can be chosen to reduce ICL.\n\nThe manner in which information is presented to the learners and the learning activities required of learners can also impose cognitive load. When that load is unnecessary (ECL), it may interfere with schema acquisition and automation.\n\nInstructional procedures developed without any consideration or knowledge of the structure of information or cognitive architecture impose heavy ECL because working memory resources must be used for activities that are irrelevant to schema acquisition and automation. Instructional designs intended to reduce cognitive load are most effective when the ICL is high. When ICL is low, instructional designs intended to reduce cognitive load have little or no effect.\n\nThe manner in which information is presented to the learners and the learning activities required of learners can also impose cognitive load. While ECL interferes with learning, GCL may enhance learning. Instead of working memory resources being used to engage in searching, for example when dealing with ECL, GCL results in those being With respect to multimedia learning, Mayer and Moreno (2003) explain that, when processing demands evoked by the learning task exceed the processing capacity of the cognitive system or working memory capacity per-se, the result is cognitive overload.\n\nReducing Cognitive Load and Enhancing Learning Cooper (1998) illustrates how extraneous load occurs in instruction: \"When intrinsic cognitive load is low (simple content) sufficient mental resources may remain to enable a learner to learn from 'any' type of instructional material, even that which imposes a high level of extraneous cognitive load. If the intrinsic cognitive load is high (difficult content) and the extraneous cognitive load is also high, then total cognitive load will exceed mental resources and learning may fail to occur. Modifying the instructional materials to engineer a lower level of extraneous cognitive load will facilitate learning if the resulting total cognitive load falls to a level that is within the bounds of mental resources.\" Feedback Mason and Bruning (2000) explain that, \"Feedback is any message generated in response to a learner's action. The outcomes of feedback include helping learners to identify errors and become aware of misconceptions.\" Research shows that, in everyday classrooms and online learning environments, feedback is provided in one or several methods of instruction to improve student learning and achievement. Brooks et al., (2005) cited methods that provide feedback including repetitive testing with immediate feedback, encouraging in-class pair discussions where students evaluate and provide feedback to each other, promoting cooperative learning in team-led groups, just-in-time dynamic expert feedback provided in computer assisted instructional settings, scaffolding, and self-explanation. Further, Brooks et al. (2005) strongly suggest that \"performance-related feedback is the hallmark of efficient instruction\" and teachers can utilize one or more forms of feedback in their instruction to improve learning. Immediate and quick feedback without delays is effective in increasing student comprehension on the learning materials. (Bangert-Drowns, Kulik, Kulik, & Morgan, 1991).\n\nStudent learning increases when feedback is corrective in nature (Walberg, 1999).\n\nAsking students to work repeatedly on a task until they are successful enhances student achievement (Marzano, Pickering, & Pollock, 2001). Crippen et al. (2000) assert that \"Feedback from practice need not be confined to the correctness of an answer. Since feedback points out missing or misunderstood knowledge, it provides an ideal teaching moment. Thus, any feedback that further explains what students do not know adds to the value of feedback.\"\n\nInstructional strategies and course design play an important role in achieving higher learner outcomes for content, especially that taught over the Web (Clark & Mayer, 2003;Dick et al., 2001;Gagne et al., 1992;Sweller, 1999). A good instructional design accommodates complex information, reduces cognitive load on working memory, and enables learners to form effective schema and automate their learning processes. Recent developments in instructional design have investigated ways to reduce cognitive load (Paas et al., 2003;Mayer & Moreno, 2003). Cooper (1998) stresses that \"The quality of instructional design will be raised if greater consideration is given to the role and limitations of working memory.\"\n\nWorked examples are among the earliest and probably the best-known cognitive load reducing techniques (Paas et al. 2003). Traditional ways of learning by solving lots of problems load working memory. Worked examples help learners to reduce this load by freeing working memory resources and building new knowledge (Clark & Mayer, 2003).\n\nThe effectiveness of worked examples depends on the previous domain knowledge of students. When students cannot form schemas within a disciplinary area of study, it is difficult for teachers to find suitable aspects of the area for them to explore (Sweller & Tuovinen, 1999).  (Renkl & Atkinson, 2003). For learners who are already familiar with the skills of problem-solving, interpreting a worked example may be redundant and impose a greater cognitive load than simply providing a solution to the problem (Kalyuga et al., 2003). One way to circumvent this problem is to design worked examples as \"completion\" problems. van Merrienboer et al. (2003) suggest that the intrinsic cognitive load can be decreased by practicing the simplest version of the whole task encountered by experts in the real world and progress towards increasingly more complex versions.\n\nThe second approach is used to decrease extraneous load by scaffolding worked examples followed by completion problems and then full problems. Sweller (1999) suggests that worked examples are effective only under conditions where students do not have to mentally integrate disparate sources of mutually referring information as well as to eliminate redundant information during problem-solving. As learners acquire cognitive skills and gain experience, using worked examples will cause redundancy. Devoting working memory to redundant information leads to allocating limited cognitive capacity to the redundancy and results in little or no learning. Redundant information may even interfere with the schemas constructed by experienced learners and may also have negative consequences. This phenomenon is known as the expertise reversal effect (Kalyuga et al., 2003). Renkl and Atkinson (2003)  In summary, this study conducts a test of instructional design strategies and evaluates the results for enhancing learning.\n\nThe AP Descriptive Chemistry Website was initially designed in 1997 and was redesigned in 2000. This Website was visited by AP chemistry users extensively over the past years for AP chemistry exam preparation (Crippen et al., 2000). Crippen (2000) in his literature states that \"The data set for 1997-99 is limited due to the design of the site. Those data are limited because it does not allow for tracking any one individual's use of the site. In addition to quizzing, the site offered a tutoring component. The tutoring component is used but is not correlated to users and their scores.\" In order to overcome the problems in 1999 study, the 2000 study redesigned the HyperCard Website and for tracking of an individual's use and surveying user perceptions.\n\nThe current Website was once again redesigned for understanding the effectiveness of the site as a learning tool. The Website was reconstructed by adopting instructional design methods based on the cognitive design principles for fostering learning. Worked examples with feedback and feedback only are the two main approaches administered in the current chemistry Website. The rationale for identifying these approaches in this study was to minimize student errors and misconceptions in order to improve learning and overall performance.\n\n1. Obtain Institutional Review Board (IRB) approval.\n\n2. Redesign the HyperCard database using Runtime Revolution.\n\n3. Announce the site to the AP chemistry community.\n\n4. Gather user access data.\n\n5. Perform data analysis.\n\nThe research sample for this study represents individuals interested in descriptive chemistry, especially as it applies to the AP chemistry examination. A total number of 1373 subjects were recruited for this study. Subjects presumably were AP chemistry students and teachers.\n\nThe sample was recruited by word of mouth, conference presentations, email, and newsgroup postings (misc. education. science, K-12.education.science). The URL for the chemistry Website http://dwb2.unl.edu/apchem/main.html was made available for access for the users worldwide. The homepage of the Descriptive Chemistry Website indicates to the users that the site is a teaching site for chemistry and that research is conducted on learning.\n\nAs shown in Figure 3.1, users logged-in to the chemistry Website using their email address and password. If the user was logging in for the first time, they were requested to provide an email and password that they wished to use for future access to the Website. Consent was obtained from the site registrant after their first log-in attempt to the Website. Once informed consent was obtained (see Figure 3.2), each new user had access to the chemistry quizzes and all the content in the Website. Each time a user logged in, all of his/her transactions were recorded together with the e-mail login. The access time, access address (computer IP number), specific identity of the chemistry items accessed in the Website, and responses made all were recorded automatically. In this study, users were randomly assigned to one of two groups. One group had access to worked examples and the other group did not. The group that received the worked examples was the experimental group and the group that did not was the control group. Presenting worked examples was the independent variable while the learning performance of the students was the dependent variable in this study. The current chemistry Website allowed students to practice chemistry problems repetitively. Based upon the students' performance (correct or incorrect responses), immediate feedback was Tutoring (access to related, appropriate text material) was accessible to both groups.\n\nAs shown in Figure 3.3, the main Webpage contained three entry areas. The first area is the Study section where users can access Chemistry Study Items.\n\nThe second area provides access to Eight-item sample tests (a model AP question), and the third area provides access to Review their transactions with the Website.\n\nThe Chemistry study items were drawn from 14 arbitrary categories defined by David Brooks and based upon his review of the AP questions over nearly four decades.\n\nThey are displayed in Table 3.1 along with the number of items available.\n\nAcid-Base anhydrides 20\n\nAcid -Base Hydrolysis 16 Redox Aqueous 35 Redox Metal Aqueous 5 Metals/Other 5 Redox/Other 10 Organic 12 Combustions 10 Precipitations 26 Complex Ion 21 Thermal decomposition 8 Mixed 5 Table 3.1. Chemistry study items and number of quiz categories.\n\nstudy item area drop-down list. Once the user makes a selection of a category, he/she is presented with a chemistry quiz item along with a tutoring component (see Figure 3.4).\n\nEach chemistry category contained a varied number of quiz items (see Table 3.1).\n\nA chemistry quiz item consisted of the question number, question, three text field boxes each for reactants and products, and a submission link for submitting answers. Students entered their answers in the text field boxes and submitted them for evaluation.\n\nInstructions for entering the answer formulas and a graphic of the periodic table were provided to students in the same Webpage for reference. Students could choose to study the material in the tutoring before answering the quiz question. Specific tutoring was provided to users based on the quiz item category. Users could access tutoring any number of times and return back for answering the same quiz question.\n\nWhen users submitted the answers correctly, both the experimental and control group received new quiz items based on their selection from the Chemistry Study Items area.\n\nHowever, when users from the worked examples group submitted an answer incorrectly, they were provided with a completed worked example for the first two attempts and a correct answer thereafter (see Figure 3.5). Control group users were provided with a correct answer if they failed to answer a quiz item (See Table 3.1).\n\nEight-item Sample Tests is the second entry area in the main chemistry Webpage.\n\nThis item format has been used up until 2006 for the AP descriptive chemistry question.\n\n(The AP is changing this format beginning in 2007.) When the user selects this option, s/he is provided with a set of eight quiz items. These items are randomly generated from the database when the user requests them. Hence, each user may get a different set of items each time they access. Users answer all eight-items and submit their answers for evaluation. Unlike the Chemistry study items, users do not have access to the tutoring component. Feedback Feedback was provided for submitted answers for both the study item quiz questions and the eight-item sample tests. Feedback included indicating the number of wrong responses submitted and the number of expected correct responses (see Figures 3.6 & 3.7). Figure 3.7. A chemistry quiz question item feedback answer with reported errors and accepted answers information. Review Review is another entry area in the main chemistry Webpage. Users can view all transactions they made in the Website by selecting this option (see Figure 3.8). Review contains the most recent transaction down to the first transaction with the Website. Surprise Quizzes\n\nBesides the eigh-item quizzes, both the experimental and control group users received the same 3-item \"surprise\" quizzes.\n\nFive such quizzes were developed, and these are presented in sequence after the 20 th , 40 th , 60 th , 80 th , and 100 th interactions. Unlike other items such as the chemistry study quizzes or eight-item tests whose selection is based upon either user choice or random choice, all users saw the same \"surprise quizzes\" in the same order (see Figure 3.9). Design of Website The AP Descriptive Chemistry Website was re-designed using Runtime Revolution™, JavaScript, cgi and XML technologies. Runtime Revolution™ and Javascript were the main two scripting languages used for rewriting the code to redevelop the old HyperCard-based Website. cgi is the engine used for interfacing connections with the Website program and the Web server. XML was the file format used for storing the user transactions with the Website. Each user has an XML-like record, which contains transaction information (see Figure 3.10). MySQL was used during data analysis after the collection of the XML files. That is, data from the XML files were aggregated into a MySQL database. Data File Structure The transaction data were stored in a folder named apchem in the Webserver folder under the Documents folder. The apchem folder contained two folders named Adata and Bdata. Adata folder contained transaction record files of all the users assigned to the worked-example, and the Bdata folder contained transaction record files of all the users assigned to the no-worked-example group. Each transaction record is an XML-like file and contained information about the time of access, date of access, IP address, and information about the user transaction within the three entry areas along with the quiz items accessed and answers submitted (see Figure 3.10).\n\n\"A user's use pattern is defined by a compilation of statistics from the database\" (Crippen, 2000). Similar to the 2000 study, descriptive statistics for each user were compiled based upon the use patterns. The average usage patterns were then used to understand the site's usage and the effectiveness of the site as a learning tool.\n\nStatistics for creating a use pattern for each user of the Descriptive Chemistry Website. The variables under analysis from the transaction records will be usage patterns of practice items, usage patterns of the eight-item sample tests, and usage patterns of surprise quizzes. Each usage pattern involves descriptive statistics of the subject such as the times and dates the Website was accessed. In particular, statistics for practice items including average number of practice items taken, average number of practice items graded, average practice scores and average request of practice items by topic were collected. For the experimental group, the average number of worked examples accessed for each practice item was measured. Similarly, statistics for an eight-item sample test included the number of correct answers, the average number of eight-item sample tests taken, and the average scores of the eight-item sample tests. Further, statistics for surprise quizzes were the average number of surprise quizzes taken and average scores on the surprise quizzes.\n\nThe results obtained from the experimental variables (transaction record data)\n\nwere gathered in a database and analyzed using SPSS software. The final results are discussed using ANOVA and regression analysis methods on the data.\n\nFailed to answer quiz question correctly in the second attempt and/or need more help or tutoring.\n\nFailed to answer the quiz question correctly in the third attempt.\n\nProvide first worked example\n\nProvide correct answer\n\nProvide correct answer Provide correct answer Table 3.2. Access to correct answer Table 4.1 illustrates that a total 50 out of 687 users from worked-example group and a total of 66 out of 686 from the no-worked-example group completed all five surprise tests. Approximately 93% of the students in the experimental group and 90% in the control group did not complete all five surprise tests.\n\nThe usage pattern includes the dates users accessed the Website, the total amount of time spent using the Website for practicing chemistry problems, and the number of user logins to the descriptive chemistry Website.\n\nUser transactions were collected from January until August, 2006. Students used the Website increasingly up through the month of May. Very minimal usage occurred thereafter during the months of June, July and August (Table 4.2 and Figure 4.1).\n\n20-Jan-06 3-Aug-06 Jan Feb Mar Apr May Jun Jul Aug Workedexample 94 124 119 166 148 17 15 4 687 No-Workedexample 95 124 117 166 147 17 16 4 686 Total 189 248 236 332 295 34 31 8 1373 Table 4.2. Users Website transaction.\n\nThe Website for this research study was designed to provide chemistry materials to users interested in learning descriptive chemistry. The selection of (experimental [WE] and control group [NWE]) users in the study was based on users who consistently used the Website and reached the point of attempting all the five sets of surprise tests. This group of 116 users is called the serious users. Although data analysis shows that a number of users interested in learning chemistry materials had registered to the AP descriptive chemistry Website, the final results indicate that only a small fraction of the total number of users met the selection criterion of \"serious user.\" That is, of the 1373 users for whom some data were available, only 116 were serious enough to persist in using the Website. Usage patterns and user associations such as the dates accessed the total amount of time spent, and number of each user logins were considered to understand the users' improvement in practicing chemistry problems and thus the performance.  chemistry website from the beginning of the study in the month of January until the day of the actual AP exam in the month of May.\n\nThe box plot (Figure 4.3) shows that the elapsed time variability between the worked example and the no-worked-example group. The worked example group has slightly greater variability than the no-worked example group. The figure also indicates that there are some outliers and extreme variables. The median time elapsed rates for worked example and no-worked example group are 5.72 and 4.52 hours respectively. The maximum (177.37 and 193.85) and minimum elapsed rates (0.71 and 0.94) respectively indicate the variability in the time spent for practicing chemistry problems in the Website.\n\nBoth the groups are skewed to the right (i.e., toward shorter time intervals)\n\n2.00 1.00 (1.00) Worked Example and (2.00) No-Worked Example Group 200.00 150.00 100.00 50.00 0.00 Elapsed Time rates Overall, no statistical significant difference was found between the groups for elapsed time (mean values M WE = 12.12; MN WE =13.05).\n\nTime Elapsed -Group Statistics 50 12.1202 27.78812 3.92983 66 13.0539 24.91376 3.06667 Group 1.00 2.00 Value N Mean Std. Deviation Std. Error Mean Table 4.3. Group Statistics for user transaction time elapsed rates Time Elapsed -Independent Samples Test .342 .560 -.190 114 .850 -.93374 4.90990 -10.66022 8.79274 -.187 99.135 .852 -.93374 4.98478 -10.82447 8.95699 Equal variances assumed Equal variances not assumed Value F Sig. Levene's Test for Equality of Variances t df Sig. (2-tailed) Mean Difference Std. Error Difference Lower Upper 95% Confidence Interval of the Difference t-test for Equality of Means\n\nTable 4.4. Independent Sample t-test for user transaction time elapsed rates. The group statistics indicate the sample sizes (N), means, and standard deviations of both the control and experimental groups. Independent Samples t-test Table 4.4 shows the Levene's Test for the Equality of Variances in the two groups for elapsed time rates.\n\nThe significance column shows that assumption is not violated (p = 0.56) is not significant. Because homogeneity can be assumed, the two-tailed significance, p = 0.85 indicates that the observed difference in the means between the WE and NWE is not significant. The output also indicates that the observed difference in the means is not significant, t(114) = -0.19, p = 0.85.\n\nThe total number of transactions each user made with the descriptive chemistry Website was measured. Users practiced on chemistry practice quizzes, eight-item quizzes, surprise tests, and viewed their usage transaction record. Each such interaction was counted towards the total number of transactions (Figure 4.4). The group statistics indicate that the mean value of interactions for the WE group is 259.48 and NWE is 294.05. Independent Samples t-test Table 4.5 shows the Levene's Test for the Equality of Variances in the two groups for number of interactions with the website. The significance column shows that assumption is not violated because the significance (p = 0.07) is not significant. 6 4 6 1 5 8 5 5 5 2 4 9 4 6 4 3 4 0 3 7 3 4 3 1 2 8 2 5 2 2 1 9 1 6 1 3 1 0 7 4 1 User 1400.00 1300.00 1200.00 1100.00 1000.00 900.00 800.00 700.00 600.00 500.00 400.00 300.00 200.00 100.00 0.00 Number of interactions with the chemistry website No Worked Example Group Worked Example group   Users practiced chemistry quiz items of their choice from a set of the descriptive chemistry materials and then submitted the quiz item for grading. Each quiz item carried a maximum of three possible reactant answers and three possible product answers. A final \"score\" for each practice quiz item was calculated based on correctly answering the chemistry quiz item, i.e. submitting all the correct reactant and product answers. Credit was not given to the user if one or more incorrect answers were submitted. The answer was then considered incorrect.\n\nTwo worked examples (worked example-1, WE1 and worked example-2, WE2)\n\nwere provided for first few sets of chemistry quiz items. The worked example group alone had access to these examples when practicing chemistry quiz items. The mean value for worked-example 1 is 16.28 and for worked-example 2 is 6.74 (Figure 4.5 and Number of worked examples accessed Example2 Example1 Figure 4.5. Number of worked examples accessed by users. Descriptive Statistics 50 2.00 56.00 16.2800 11.47853 50 .00 28.00 6.7400 5.96182 50 Example1 Example2 Valid N (listwise) N Minimum Maximum Mean Std. Deviation The eight practice questions are a set of eight practice quiz items. The final score for the practice quiz item is calculated based on the total number of reactant and product answers submitted correctly out of the maximum possible correct answers for each quiz item. Total score of the each set of practice item answered is considered for the analysis.\n\nThe group statistics indicate that the mean value of eight-item practice quiz items for the WE group is 3.37 and NWE is 3.78 ( Table 4.14. Group Statistics for eight-item practice quizzes The Independent Samples t-test Table 4.15 shows the Levene's Test for the Equality of Variances in the two groups for eight-item practice quizzes. The significance column shows that assumption was not violated, (p= 0.70) was not significant. Because homogeneity can be assumed, the two-tailed significance, p = 0.31 indicates that the observed difference in the means between the WE and NWE was not significant. The\n\n0 5 10 15 20 25 1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33 Users Number of times record reviewed Worked-Example No-Worked-Example Surprise Tests A total of 15 surprise test items were analyzed. The users had no control over the appearance of surprise tests, and the surprise test items appeared only once (in the context of a surprise test). Thus the surprise test items were the measurable features that all users of the Website saw in common and at the same relative time during the site usage. The group statistics of the surprise tests were calculated as shown in the Table 4.16.\n\nThe Independent Samples t-test Table 4.17 shows the Levene's Test for the Equality of Variances in the two groups for surprise quiz items. The significance column is considered to determine whether the assumption is violated or not violated. Based on the homogeneity of the assumption and the two-tailed significance value, the significance date of the first Website visit was skewed toward the time of the AP examination with more users making their first visit at a time nearer that of the examination. More users were prone to practice chemistry problems just before the approaching AP chemistry examination rather than practicing them at an earlier period of time.\n\nThe results of the total amount of time spent, which are indicated through the elapsed time rates, suggest that a majority of all users spent little time practicing descriptive chemistry problems. Also, it was found that both the WE and NWE group followed an overall similar pattern of spending brief Website transaction times. That is, there were no significant differences between the groups suggesting that the use of worked examples did not make a difference in having users spend more or less time when practicing chemistry problems (M WE = 12.12 hours, M NWE = 13.05 hours, p = 0.85, Tables 4.3 and 4.4). The extreme variables and outliers in the analyzed results data indicate that there were a few cases where users appeared to spend either very long or very short times using the Website. This is a case where having a laboratory situation rather than an anonymous Website access approach would have given insights as to the reasons for outliers. For example, for very long times, the users were likely to be off task due to interruptions.\n\nThe total number of user interactions with the Website was analyzed. No statistically significant difference between the experimental and control group means was found with respect to average number of interactions (M WE = 259.48 interactions, M NWE = 294.05 interactions, p = 0.35, Tables 4.5 and 4.6). Serious users in both groups accessed the chemistry quiz items, eight-item quizzes, tutoring, and feedback. In addition, users in the worked-example group received worked examples for practice.\n\nAny user could access an eight-item quiz reminiscent of an actual AP descriptive chemistry examination question. Significant differences were not found between the two groups for accessing eight-item quizzes. Users from each group would receive a set of eight quiz items. Each set was analyzed based on the number of correctly answered products and reactants. In analyzing the eight-item quizzes, therefore, users were given credit for partially correctly answered questions (M WE = 3.37 eight-item correct, M NWE = 3.78 eight-item correct, p = 0.31, Tables 4.14 and 4.15).\n\nOnly users in the WE group had access to worked examples. Two worked examples were possible for each of the first three practice items for each of the 15 practice group categories. The mean values for both the worked-example-1 and workedexample-2 were considered when analyzing the worked example group user pattern in utilizing the examples when practicing chemistry problems. It was found that workedexample-1 was accessed more than twice as often as worked-example-2 (M WE-1 = 16.28 accesses; M WE-2 = 6.74 accesses; Table 4.13).\n\nThe principal part of this study was based upon the use of surprise tests. In the many previous studies of this Website, there were no differential treatments nor were all users assessed in some consistent way. The surprise tests were considered to be the principal indicators for analyzing the performance of users when practicing chemistry quiz items. Both groups received the same set of questions after the same number of interactions with the Website. As closely as possible, the twentieth, fortieth, sixtieth, eightieth, and one hundredth accesses to the site confronted the user with a surprise 3item quiz. The results are displayed in Table 4.16.\n\nBased upon previous studies, it was thought that using worked examples would lead to improved learning. Statistically significant differences were not found in any of the five sets of surprise tests, however (Table 4.17). Each surprise test included 3 items, and each of these items (15 total) was studied separately. There was also no discernable pattern of improvement of scores from the first surprise test to the fifth surprise test (Figure 5.1). Because the intrinsic difficulty of items cannot be controlled in the context of realistic practice for the AP examination, variations when only three items are included can be large. The first item of surprise-test-5 (\"excess aqueous 1.0 M sodium iodide is added to acidified 1.0 M sodium iodate\") was perceived of by users as importantly more difficult than item 2 of surprise-test-4 (\"a few drops of methanol are burned in an excess of air\"). Most experienced high school or college chemistry teachers likely would have guessed this outcome. However, item 3 of surprise-test-1 (\"aqueous sodium sulfate is added to aqueous strontium chloride\") is really quite similar to item 3 of surprise-test-5 (\"aqueous barium chloride is mixed with aqueous potassium sulfate\") but the average scores (M WE = 0.37, p= 0.44 and M NWE = 0.39, p = 0.40) are not dramatically improved.\n\nEighty interactions took place with the Website between the first and last of these \"surprise test\" items.\n\nor twice and did not further revisit the Website. Early registration (for example, at least more than two months use and interaction with the Website) might have improved users' performance.\n\nExtensive use of worked examples and tutoring components might have had added value to improve the performance. A majority of the students did not access both of the worked examples provided to them but instead accessed only one or none.\n\nLikewise, users might have referred to the tutoring components more frequently for answering the chemistry quiz items for understanding the chemistry concepts before practicing the quiz items."
}