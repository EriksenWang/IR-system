{
    "title": "Semantic Concept Mining Based on Hierarchical Event Detection for Soccer Video Indexing",
    "publication_date": "2002",
    "authors": [
        {
            "full_name": "Maheshkumar H Kolekar",
            "firstname": "Maheshkumar H",
            "lastname": "Kolekar",
            "affiliations": []
        },
        {
            "full_name": "Kannappan Palaniappan",
            "firstname": "Kannappan",
            "lastname": "Palaniappan",
            "affiliations": []
        },
        {
            "full_name": "Somnath Sengupta",
            "firstname": "Somnath",
            "lastname": "Sengupta",
            "affiliations": []
        },
        {
            "full_name": "Gunasekaran Seetharaman",
            "firstname": "Gunasekaran",
            "lastname": "Seetharaman",
            "affiliations": []
        }
    ],
    "abstract": "In this paper, we present a novel automated indexing and semantic labeling for broadcast soccer video sequences. The proposed method automatically extracts silent events from the video and classifies each event sequence into a concept by sequential association mining. The paper makes three new contributions in multimodal sports video indexing and summarization. First, we propose a novel hierarchical framework for soccer (football) video event sequence detection and classification. Unlike most existing video classification approaches, which focus on shot detection followed by shot-clustering for classification, the proposed scheme perform a top-down video scene classification which avoids shot clustering. This improves the classification accuracy and also maintains the temporal order of shots. Second, we compute the association for the events of each excitement clip using a priori mining algorithm. We propose a novel sequential association distance to classify the association of the excitement clip into semantic concepts.\nFor soccer video, we have considered goal scored by team-A, goal scored by team-B, goal saved by team-A, goal saved by team-B as semantic concepts.\nThird, the extracted excitement clips with semantic concept label helps us to summarize many hours of video to collection of soccer highlights such as goals, saves, corner kicks, etc. We show promising results, with correctly indexed soccer scenes, enabling structural and temporal analysis, such as video retrieval, highlight extraction, and video skimming.",
    "full_text": "Effective handling of videos such as browsing, retrieving [1], and editing requires semantic understanding of videos. Semantic video content analysis is quite a challenging problem due to a large variety of video content. However, there are certain similarities among certain types of videos, which can be cues to solve the problem. For example, a news video [2], [3] can be considered as a sequence of video segments which starts with anchor person followed by story units; a sports video [4], [5] as a repetitions of play and break scenes. As stated above, a video is often considered as a sequence of video segments, each of which can be considered as a unit to understand the semantic content or the story of the video. Therefore, structuring videos according to their semantic compositions, while understanding the semantic role of each video segment, is a step toward sematic understanding of the videos.\n\nIn recent years, sports video has emerged as important area of research due to its wide viewership, ease of digital archival and huge commercial potential [6], [7]. Moreover, the distribution of sports video across the Internet further increases the need for automatic video analysis, such as quick browsing, video summarization, detecting and recording interesting highlights for later review. Existing approaches of sports video analysis [8] can be broadly classified as genre-specific or genreindependent. Due to dramatically distinct broadcast styles for different sports genres, much of the prior art concerns genre specific approaches. Researchers have targeted the individual sports game such as soccer (football) [9], [10], [11], tennis [12], [13], cricket [4], basketball [5], baseball [14], [15], volleyball [16], etc. These works show that genre specific approaches typically yield successful results within the targeted domain. In comparison with the genre-specific research work, less work is observed for genre-independent studies [17], [16], [18], [19]. For a specific sports event detection task, it is not feasible to expect a general solution that will work successfully across all genres of sports video.\n\nIn the case of soccer sports video, Li et. al. [20] proposed rule based algorithm using low-level audio/video features for soccer video summarization. Lefevre et. al. [21] divided audio data into short sequences, which are classified them into three classes such as speaker, crowd, referee whistle. Babaguchi et. al. [22] proposed event detection by recognizing the textual overlays from soccer video. Wan et. al. [23] proposed dominant speech features to generate soccer highlights. Ding et. al. [24] proposed segmental Hidden Markov Model for view-based soccer analysis. Barnard et. al. proposed [25] HMM based framework to fuse audio and video features to recognize the play and break scenes in soccer video sequences. Ren and Jose [26] have proposed a HMM based framework to extract 'attack' scene from football video. Huang et. al. introduced semantic analysis of soccer video system based on Bayesian network.\n\nIn [27], authors proposed Support Vector Machine based event detection for soccer video. In [28], authors proposed Finite State Machine based annotation of soccer video. Wang et. al. [29] proposed semantic notion of offense for event detection of soccer video. Yu. et. al. [30] proposed technique to retrieve the football video clips using its global motion information. Recently, researchers [31], [32], [33] have presented the use of tracking the positions of players or ball for soccer video analysis. Wang et. al. [9] proposed an automatic approach for personalized sports music video generation.\n\nThere have been many successful works in soccer video analysis as mentioned above. But most of these works fail to respond to action-based queries, such as \"extract the goal clips out of this soccer sequence\", or \"extract the saves from this soccer video\", \"extract goals scored by player-A\", \"extract all the red card events from the collection of FIFA 2006 world cup matches\", etc. Such queries may be always needed for editing and retrieval. To facilitate fast retrieval and highlight generation, automatic indexing of video clips is essential. Successful solution of this problem has to address event based classification. The challenge is that soccer game itself has loose structure. Also, there is a lot of motion and view changes in the soccer. Hence, we propose hierarchical framework for event based classification of soccer video and we extract high level concept label based on semantic concept mining technique.\n\nOur proposed system shown in Figure 1 is composed of two components, i.e. hierarchical event detection and classification and semantic concept mining from the extracted events. First the recorded sports video is preprocessed using operations such as commercial removal, dividing game into slots. The excitement clips are extracted from these preprocessed slots. The pre-processed slots are fed to the level-1 of the hierarchical classifier. We have proposed the excitement clip extraction based on short-time audio energy, since the important activity in any sports corresponds to spectators' cheering and commentators' voice. We have observed that the clips extracted as above contain several events. The events within the extracted clips are detected using hierarchical classification tree based on low-level features of the excitement clip. The detected events E 1 , E 2 , ...., E m within the clip are arranged in their temporal order to form video event sequence. Since more number of events will be observed for important activity in the soccer sports video, we discard the clips with shorter video event sequence. The sequential association between the events is computed using a priori algorithm [34], [35]. Based on our proposed sequential association distance measure, a concept label is assigned to the clip.\n\nThese labeled clips are used for indexing and retrieval, video summarization and highlight generation purpose. The main contributions and the novelty of this paper are\n\nAudiofeature based Excitement Clip Extraction Hierarchical Event Detection and Classification E 1 E 2 ---E m Recorded Soccer Video Low-level Feature Extraction Video Event-based Clip Selection Video Event Association Compute Sequential Association Distance from known Concept-classes Semantic Concept Mining Hierarchical Event Detection and Classification Semantic Concept Labelling Mined Concept Set of Associations of Known Concept-classes ---Video Event Sequences of known concepts for training ---Preprocessing of video Video Event Sequence Video Association Classification Figure 1. Overall block diagram for semantic concept mining using low-level feature extraction and video event extraction summarized as follows: (1) We propose novel hierarchical framework for soccer video, (2) We propose novel close-up detection algorithm based on edge detection, (3) We propose novel domain specific close-up classification algorithm based on skin detection and jersey color comparison, (4) Our proposed sequential association distancebased concept mining framework generates retrievalfriendly semantic concept labels. The rest of the paper is organized as follows. Section-II presents proposed hierarchical classification tree. Section-III presents concept extraction based on sequential association rule. Section-IV presents experimental results. Section-V concludes the paper with direction for future work.\n\nMost of the research in sports video processing [36], [37] assumes a temporal decomposition of video into its structural units such as scenes, shots and frames similar to other video domain including television and films. A shot refers to a group of sequential frames often based on single set of fixed or smoothly varying camera parameters (i.e. close-up, medium or long shots, dolly, pan, zoom, etc). A scene refers to a collection of related shots. A clip refers to a collection of scenes. In soccer game, there are moments of excitement, with relatively dull periods in between. Only excitement clips qualify for event detection and indexing.\n\nIn [9] the authors propose the integration of multimodal features such as audio, video and text to detect the semantics of the events. Although the integration of multiple features improves the classification accuracy, it leads to other problems such as proper selection of features, proper fusion and synchronization of right modalities, critical choice of the weighting factor for the features and computational burden. To cope with these problems, we propose a novel hierarchical classification framework for the soccer videos as shown in Figure 2, which has the following advantages: (1) The approach avoids shot detection and clustering that are the necessary steps in most of video classification schemes, so that the\n\nSoccer Sports Video Excitement Clip Non-excitement Clip Field View Non-field View Long View ( V l ) Corner View ( V c ) close-up crowd Player's Gathering Team-A (G A ) Spectator (S) Player Team-A (P A ) Player Team-B ( P B ) Goalkeeper Team-A ( K A ) Goalkeeper Team-B ( K B ) Real-time Replay Player's Gathering Team-B ( G B ) L 1 L 2 L 3 L 4a L 4b L 5a L 5b Referee (R e ) Straight View (V s ) (R) Figure 2. Hierarchical event detection and classification tree Algorithm-1: Excitement Clip Extraction Non-field view Field view Crowd Close-up Soccer Sports Video Excitement Clips Algorithm-2: Replay Detection Real-time Replay Algorithm-3: Field View Detection Algorithm-4a : Field View Classification Algorithm-4b : Crowd Detection Algorithm-5a : Close up Classification Algorithm-5b : Crowd Classification\n\nCorner View Straight View Referee Player Team-A Player Team-B Goalkeeper Team-A Goalkeeper Team-B Players Gathering Team-B Spectator Players Gathering Team-A Figure 3. Algorithm data flow for event detection and classification to Figure 2 classification performance is improved. (2) The approach uses top-down five-level hierarchical method so that each level can use simple features to classify the videos. (3) This improves the computation speed, since the number of frames to be processed will remarkably reduce level by level. Figure 3 shows the system diagram for event detection and classification.\n\nWe have defined the events as scenes in the video with some semantic meaning (i.e. labels from a semantic hierarchy) attached to it based on the leaf nodes shown in Figure 2. Events are extracted as the leaf nodes of the level-2 to level-5 of hierarchical tree. The events are Replay, Long View, Straight View, Corner View, Spectator, Player Team-A, Player Team-B, Goalkeeper Team-A, Goalkeeper Team-B, Referee, Players Gathering Team-A, Players Gathering Team-B.\n\nThe low-level feature extraction for the event detection and labeling is discussed in the following subsections.\n\nWe have observed that during the exciting events spectator's cheer and commentator's speech becomes louder. Based on this observation, we have used shorttime audio energy feature for extracting excitement clip. We are considering the short-time as the number of audio samples corresponding to one video frames. A particular video frame is considered as an excitement frame if its audio excitement exceeds a certain threshold. We propose following steps for excitement clip detection.\n\nAlgorithm-1: Excitement Clip Extraction 1. Short-time audio energy E(n), is defined as\n\nwhere x(m) is the discrete time audio signal,\n\nw(m) is a rectangular window and V is the number of audio samples corresponding to a single video frame.\n\nAveraging using a sliding window in order to distinguish genuine audio excitement from audio noise. However, it helps for early detection of the events as well.\n\nwhere, L is the length of sliding window. The normalized values are as follows:\n\nwhere, N is the total number of video frames.\n\nExcitement frame detection based on the E 2 (n), a video frame n will be finally labeled as ψ(n) ∈ [0, 1] as defined below:\n\nwhere, P audio is the mean of E 2 (n).\n\nWe observed that the excitement clips of longer durations are important clips. If all frames in the duration of 20 seconds were all classified as excited frames, then the clip is regarded as excitement clip.\n\nReplay events often represent interesting events. Wang et. al. [38] used motion and color based features to detect replays. Pan et. al. [39] detected slow-motion replays based on logo template. As shown in Figure 4 we observed that replays are generally broadcast with flying graphics (logo-transition) indicating the start and end of the replay. The flying graphics generally last for 10 to 20 frames. Replay segment is sandwiched by two logo-transitions. Since a replay shows many different viewpoints and thus contains many shots in a relatively short period, shot frequency in a replay segment is significantly higher than the average shot frequency in the excitement clip. The color of logo is unique and the size of the logo is big enough to affect the distribution of color histogram as shown in\n\nwhere N 1 × N 2 is the image size. 6. Select the logo-transition frames using threshold P HHD = 0.5 * mean.\n\nif (HHD < P HHD ) then frame is logo-transition frame 7. Select the segment between two successive logotransitions and compute shot frequency f r . 8. The selected segment is classified as replay segment using following condition.\n\nif (f r > f c ) then selected segment belongs to replay class else selected segment belongs to real-time class\n\nWe are using a green color pixel ratio similar to [40] to classify the real-time video clips into field view and non-field view. We used 120 representative field view frames from various video (see Table III) to determine the parameters for the DGPR method. The combined 256bin histogram of the hue component of the representative images is used to determine the size of the green window, which was found to be [G min , G max ] = [58,68]. Let G peak be the peak in hue histogram with the green (a) ( c ) (d) (b) Figure 5. (a) Logo template of Olympic 2008 matches, (b) Hue-Histogram of (a), (c) Logo-template of FIFA 2006 matches, (d) Huehistogram of (c) window, G peak ∈ [G min , G max ]. The dominant green pixel ratio (DGP R) is defined as:\n\nwhere, H(G peak ) is the number of pixels with value G peak in the histogram for a given frame, and N 1 × N 2 is the total number of pixels in the image. For the field view image shown in Figure 6 (a), we observed DGP R = 32.75 % and for non-field view image in Figure 6 (c), we observed DGP R = 0.3 %. We observed DGP R values for field view images are greater than 16 %.\n\nView Detection 1. Convert the input RGB image into HSV image format. 2. Plot histogram of the hue component of the image. 3. Select the G peak window as G peak ∈ [G min , G max ] 4. Compute DGP R. 5. Classify the image using DGP R threshold P DGP R = 16% if (DGP R > P DGP R ) then frame belongs to class field view else frame belongs to class non-field view\n\nUnder the constant illumination model, the optic-flow equation of a spatiotemporal image volume I(x) centered at location x = [x, y, t] is given by Eq. 7 [41] where,\n\nand v(x) is estimated by minimizing Eq. 7 over a local 3D image patch Ω(x, y), centered at x. In order to reliably detect only the moving structures without performing expensive eigenvalue decompositions, the concept of the flux tensor is proposed [41]. Flux tensor is the temporal variations of the optical flow field within the local 3D spatiotemporal volume. Computing the second derivative of Eq. 7 with respect to t, Eq. 8 is obtained where, a(x) = [a x , a y , a t ] is the acceleration of the image brightness located at x.\n\nwhich can be written in vector notation as,\n\n) Using the same approach for deriving the classic 3D structure, minimizing Eq. 8 assuming a constant velocity model and subject to the normalization constraint ||v(x)|| = 1 leads to Eq. 10,\n\nAssuming a constant velocity model in the neighborhood Ω(x, y), results in the acceleration experienced by the brightness pattern in the neighborhood Ω(x, y) to be zero at every pixel. The 3D flux tensor J F using Eq. 10 can be written as (11) and in expanded matrix form as Eq. 12.\n\nAs seen from Eq. 12, the elements of the flux tensor incorporate information about temporal gradient changes which leads to efficient discrimination between stationary and moving image features. Thus the trace of the flux tensor matrix which can be compactly written and computed as,\n\nand can be directly used to classify moving and non-moving regions without the need for expensive eigenvalue decompositions. Motion-mask is obtained by thresholding and post-processing averaged flux tensor trace. Post-processing include morphological operations to join fragmented objects and to fill holes. In field view, players and crowd are moving objects and field is non-moving object. Hence, we used motionmask to classify the frames of the field view as long view, straight view and corner view. Our approach is summarized as follows:\n\nAlgorithm-4a: Field View Classification 1. Generate motion-mask for the input field-view frame (see second column of the Figure 7).\n\nnoisy objects from the image (see third column of Figure 7).\n\nIn the connected component image, background color is the color of object 'field'. Divide the frame into three regions 11, 12, and 2 (see first column of Figure 7). 4. Let F P 2 , F P 11 , F P 12 be the percentages of Field Pixels in the region 2, 11, 12 of the connected component image respectively. The field-view frame is classified into long view, corner view, and straight view using the thresholds P 1 , P\n\n2 , P 3 as follows: if (F P 2 > P 1 ) ∧ ((F P 11 + F P 12 ) > P 2 ) then frame belongs to class long-view else if |F P 11 -F P 12 | > P 3 frame belongs to class corner-view else frame belongs to class straight-view F. Level-4b: Close-up detection We observed that non-field view generally contains only close-up and crowd frames. The percentage of edge pixels (P EP ) is used to classify the frame as crowd or close-up, since we typically observe more edge pixels for crowd frames as shown in Figure 8. Any robust edge detector can be used. In our case, we applied Canny edge detector and use the following ratio as the closeup detection parameter: P EP = T otal number of edge pixels T otal number of pixels in the f rame ×100% (14) Our approach is summarized in Algorithm-4b. Algorithm-4b: Crowd Detection 1. Convert the input RGB image into Y C b C r model. 2. Apply Canny operator to detect the edge pixels. 3. Compute Percentage Edge Pixel (P EP ) for the image. 4. Classify the image using following condition: if (P EP > P P EP ) then frame belongs to class crowd else frame belongs to class close-up G. Level-5a:Close up Classification\n\nIn sports video, jersey colors are typically differentiate players of different teams, as well as the umpire or referee. At level-5a, the close-up images are classified as Player Team-A, Player Team-B, Goalkeeper Team-A, Goalkeeper Team-B or Referee. The location of the face of the player in the close-up frame is segmented using skin color information as shown in Figure 9 (b). The connected component technique is applied to remove noisy skin detected objects. The face position will generally occur in the block B 6 , B 7 , B 10 , B 11 depending on the size of close-up as shown in Figure 9 (a). If the number of skin pixels in the block is greater than 1 2 3 4 5 6 7 8 9 10 11 12 13 15 16 (a) (b) ( c ) (d) ( e ) Figure 9. (a) Image of goalkeeper (b) Image showing skin detection, (c) Connected component image, (d) selected block-10, (e) Hue-histogram of (d) the threshold P skin , then block is considered as skin block. The skin block (face block) locations are used to locate and identify the player's jersey color. For example, if block B 6 , B 7 , B 10 , B 11 are skin blocks, that is B 6 > P skin , B 7 > P skin , B 10 > P skin , B 11 > P skin , then the skin block binary pattern is 1111 as shown in Table I. Corresponding to these skin block locations, we check B 14 and B 15 for the jersey color of the player, which is the binary pattern 0011. Algorithm-5a: Close-up Classification 1. Convert input RGB image into Y C b C r image format. Use the following condition for detecting skin pixels. if (105 < Y < 117)∧(110 < C b < 113)∧(C r > 128), then pixel belongs to skin color else pixel does not belong to skin color 2. Apply connected component technique to remove noisy skin detected objects. 3. Divide the image into 16 blocks and compute the percentage of skin color pixels in each block. 4. Table I shows the correspondence between the location of skin block in the frame and the location of the associated jersey block. 5. Compute 256-bin hue-histogram H JB of the selected jersey block. 6. Compute the average 256-bin hue-histogram H k for each close-up class (see Figure 10). 7. Compute the histogram distance of jersey block JB for frame n from the class k using following formula.\n\n(15) The Goalkeeper Team-A, Goalkeeper Team-B, Player Team-A, Player Team-B and Referee classes have class labels k = {1, 2, 3, 4, 5} respectively.\n\nAt this level, we classify the crowd using jersey color information into Players Gathering Team-A, Players Gathering Team-B, and Spectator as shown in Figure 11 and Algorithm-5b. We observed that players Gathering generally have field background. Hence, if we set green bins of hue histogram to zero, the error due to background can be removed. Our approach is\n\nTABLE I. CORRESPONDENCE BETWEEN SKIN BLOCKS AND JERSEY BLOCKS Skin Blocks Jersey Blocks B 6 B 7 B 10 B 11 B 10 B 11 B 14 B 15 1 1 1 1\n\n0 0 1 1 0 0 1 1 0 0 1 1 1 1 0 0 1 1 0 0 1 0 1 0 0 0 1 0 0 1 0 1 0 0 0 1 1 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0 1 summarized as follows: Algorithm-5b: Crowd Classification 1. Convert input RGB image n into HSV format. 2. Compute 256-bin Hue-histogram for the input image n. 3. Zero the bin [G min , G max ] to remove green background effects. 4. Compute average 256-bin hue-histogram H k for each crowd class. 5. Compute histogram distance of image n from the class k using following formula.\n\nThe Spectator Players Gathering Team-A and Players Gathering Team-B classes have class labels k = {1, 2, 3} respectively.\n\nFor semantic concept extraction application, the objective is to capture information about how events in the extracted clip are related to one another. In comparison with the classification rule [42], [43], [44], the association-based technique doesn't need to define the models in advance. Instead, the association mining help us to explore models from video. The leafnodes of the level 2 to 5 of the classification tree of Figure 2 are arranged in their temporal order. The labels are attached to these events to form video event sequence D for particular excitement clip. The labels assigned to the events are shown in the brackets: Replay (R), Long View (V l ), Straight View (V s ), Corner View (V c ), Goalkeeper Team-A (K A ), Goalkeeper Team-B (K B ), Player Team-A (P A ), Player Team-B (P B ), Referee (R e ), Spectator (S), Players Gathering Team-A (G A ), Players Gathering Team-B (G B ). Semantic Concepts are mined from the video event sequence through a sequential association rule-base.\n\nSemantic concepts are the collection of a temporally ordered set of events. It can be expressed as C i = ∪ m j=1 E j , for i = 1, 2, ........, z where, z is the number of concepts, E 1 , E 2 , ..., E m correspond to the events and m is the number of events associated with the i th concept. C = {C i } i=1,2,...,z represents the set of all extracted concepts. We have considered four types of conceptclasses Goal Scored by team-A (Goal A ), Goal Scored by team-B (Goal B ), Goal saved by team-A (Save A ), Goal saved by team-B (Save B ), because these are the most important concepts from the spectator's point of view.\n\nIn soccer video, we will observe more number of events during the importance activity. For example, during the goal, we will observe the close-up of the player/goalkeeper who has contributed, the close-up of referee, celebration of the players by gathering, slowmotion replays, etc. Generally, the length of video event sequence (LV ES) is longer for exciting activity such as goal or save in soccer video. Hence, we are selecting the clips based on the following criteria: if (LV ES > P LV ES ) then Select the Clip As shown in Figure 12, we have selected the threshold P LV ES = 4. Hence, clip-3 has been rejected because it contains only 4 video events.\n\nFor a better understanding of video mining from the database point of view, let's assume that D is a transaction database of one consumer, and that each event in D is one transaction. The sequential correlation among the events of D would reflect the association among video events. Table II shows a typical example for computing the sequential association of the video events from the sequence D = \"P B V c P B G B SG B RS\". The terminologies used are as follows: 1: Video Event is a database item that denotes an events with semantic meaning attached to it. In our example, P B , G B , V c , R, S are video events. 2: Transactional database D typically includes a list of sequential patterns. In our example,\n\nSave A Goal B Save B Soccer Sports Video Clip Duration (sec) Excitement Clips Duration > 20 sec Extracted Events (Number) (6) (5) (4) (6) Video Event Sequence > 4 Association Support >=2 Extracted Concept Level-1 Level 2 to 5 Video Data Transformation Finding Association Concept Labelling 28 24 21 6 Clip-1 Clip-2 Clip-3 Clip-4 Clip-5 C 1 C 2 C 3 26 Figure 12. Typical example of concept extraction. Clip-4 is rejected by hierarchical classifier since it has duration smaller than the threshold. Clip-3 is rejected since it has video event sequence not greater than threshold (=4). TABLE II. VIDEO EVENT ASSOCIATION FOR D = \"P B VcP B G B SG B RS\" 1-L 2-Event 2-LEvent 3-LEvent 3-LEvent 3-LEvent Event Set Set Set (after Set (after Set Set join) pruning)\n\nconsists of L sequential events. In our example, P B , G B , S are 1-EventAssociation, P B G B is 2-EventAssociation, and P B G B S is 3-EventAssociation.\n\nThe Support of an association is the number of times particular association appears sequentially in the clip. A minimum support threshold requires to be set to extract important association and we have set this as 1. The associations having support larger than 1 are qualified for next level. In our example, P B , G B , S have support 2 and V c , R have support 1 at level-1. 5: L-EventSet is an aggregation of all L-EventAssociation with each of its member being an L-EventAssociation. In our example, 2-EventSet is {P B G B , P B S, G B P B , G B S, SP B , SG B }. 6: L-LEventSet is an aggregation of all L-EventAssociation whose support is greater than a given threshold. In our case, 2-LEventSet is {P B G B , P B S, G B S}, since these associations have support equal to 2.\n\nWe have used a priori algorithm to extract the association between the video events. It employs an iterative approach known as level-wise search, where L-LEventSet is used to explore (L+1)-EventSet. The a priori algorithm from the database field [34], [35] is described in Algorithm-6 for ready reference.\n\nAs shown in Table II, in the first level, we sequentially scan the given sequence D and find the events with their support larger than a threshold. The aggregation of these events form 1-LEventSet (see column-1 of Table-II). We use these 1-LEventSet as input to generate the candidates of 2-EventSet by using the candidate generation algorithm (see column-2). We then scan D again to calculate the support of each association in 2-EventSet. The associations with their support larger than the threshold are collected to form 2-LEventSet (see column-3) and generate the candidates of 3-EventSet. We will iteratively execute this phase until no more non-empty EventSet can be found.\n\nThe candidate generation function deletes the members in I k whose subsequences are not in L k-1 . Take the 2-LEventSet in the third column of Table II as an example. If L 2 is given as the input, we will get the 3-EventSet shown in the fourth column after the join. After pruning out sequences whose subsequences are not in L 2 , the sequences shown in the fifth column will be left. For example, the sequence P B SG B is pruned out because its subsequence SG B is not present in L 2 (column-3).\n\nJoin the events of association in L k-1 2. Insert the join results into I k Select {p.event 1 , ..., p.event k-1 , q.event k-1 } from {L k-1 .p , L k-1 .q , p = q} where {p.event 1 = q.event 1 , .., p.event k-2 = q.event k-2 } 3. Delete any member x ∈ I k such that some {(k-1)-EventAssociation} of x is not in L k-1 D. Video Association Classification 1) Sequential Association Distance: Let association of known concept as A(wc k ) = {X 1 , X 2 , .., X P } indicating a sequential association with P events, and association of concept under test as A(C T ) = {X 1 , X 2 , ..., X Q } with Q events. We propose a distance measure, described as the SEQuential Association Distance (SEQAD) between A(wc k ) and A(C T ) as given below:\n\nwhere, LCS and N CI are the length of the Longest Common Subsequence and Number of Common events respectively. Let A(wc 1 ), A(wc 2 ), A(wc 3 ), and A(wc 4 ) are the associations for the concept Goal A , Goal B , Save A , and Save B respectively. The SEQAD is used to compute the distances between association of the concept under test and the association of the known concept-class. The label of that concept-class which has minimum SEQAD is assigned to the concept under test.\n\n2) Key Association for known concepts: First, we have computed the association of 180 excitement clips whose concepts are known to us. Then we have selected five frequently occurring associations for the particular concept-class and determined key association in such a way that the length of common subsequence between the key association and any member of five frequently occurring associations is at least 2. We observed the key associations as A(ωc\n\nSEQAD{A(ωc 4 ), A(c\n\nHence, the concept derived from the association A(c T ) is Goal scored by team-B, since its association with A(ωc 2 ) gives the minimum SEQAD score.\n\nWe have tested our proposed approach using 13 hours of video containing live recordings of FIFA world cup 2006, FIFA world cup 2002, Olympic 2008, Scottish cup 2002, and Champians League 2002 matches as shown in Table III. Since commercials may also be classified as a excitement based on audio excitement, we remove the commercials from the videos before applying hierarchical classifier. We first present results for proposed hierarchical classification tree and then we will present the results of semantic concept mining.\n\nWe are extracting excitement clips at level-1 and from level-2 to level-5, we are analyzing the clips to extract the event. This extracted event sequence is going to be used for mining semantic concept for the excitement clip. We are only interested to know whether the particular event is present or absent in the particular excitement clip. Hence, we are presenting clip-based performance instead of frame-based for classifiers of level-2 to level-5. The length of the clips decreases as the level of hierarchy\n\nTABLE III. SOCCER VIDEO SEQUENCES USED FOR TESTING Video Name of Total A vs B Date ID the Match Duration FIFA-1 FIFA World 94 min Germany vs 30/06/ Cup 2006 Argentina 2006 FIFA-2 FIFA World 95 min England vs 01/07/ Cup 2006 Portugal 2006 FIFA-3 FIFA World 96 min Brazil vs 01/07/ Cup 2006 France 2006 FIFA-4 FIFA World 98 min Germany vs 25/06/ Cup 2002 Korea 2002 Olympic-1 Olympic 96 min Argentina vs 07/08/ 2008 Ivory 2008 Olympic-2 Olympic 96 min Brazil vs 07/08/ 2008 Belgium 2008 Scott-1 Scottish Cup 97 min Celtic vs 04/05/ 2002 Rangers 2002 Champ-1 Champions 99 min Real Madrid vs 15/05 League 2002 B. Leverkusen 2002 TABLE IV. PARAMETERS USED FOR EXPERIMENTATION Parameter Definition Value P audio audio energy threshold mean P HHD hue-histogram difference threshold 0.5*mean [G min , G max ] green intensity window size [58, 68] P DGP R dominant green pixel ratio threshold 16% P 1 , P 2 field pixels thresholds 65%,65% P 3 field pixels difference threshold 10% P P EP percentage of edge pixels threshold 8% P skin percentage of skin pixels 60% in the block threshold P LV ES length of video event 4 sequence threshold Support association support 1 increases, because at each level, we segment the clips into sub-clips. The parameters used for the experimentation are given in the Table IV. For measuring the performance of classifiers, we use following parameters: Recall =\n\nWe present here the results of typical soccer video clip of duration 2 minutes 25 seconds. We sampled audio at a rate of 44.1 KHz and video frames are down sampled from its original 30 frames/second to 5 frames/second. We extracted 725 video frames of this video clip. Figure 13 (c) shows the excitement clip selection based on short-time audio energy.\n\nThe overall performance of the classifier at level-1 is shown in Table V. In case of poor broadcasting quality and noisy audio, performance of audio-based excitement clip extraction decreases.\n\n2) Level-2: Replay Detection: The performance of proposed logo-based replay detection technique is shown in Table VI. Replays generally occur at the end of the excitement clips. If audio is low during the last wipe of the replay, it will not be extracted as a part of the excitement clip, and hence there will be possibility of missing replays. TABLE V. PERFORMANCE OF LEVEL-1 OF HIERARCHICAL CLASSIFIER Video Extracted Re-Preci-ID Clips N c N m N f call sion Duration (%) (%) FIFA-1 22 min 24 5 9 82.76 72.73 FIFA-2 29 min 33 6 8 84.62 80.49 FIFA-3 18 min 18 2 7 90.00 72.00 FIFA-4 16 min 10 2 4 83.33 71.43 Olympic-1 13 min 14 2 3 87.50 82.35 Olympic-2 12 min 9 1 3 90.00 75.00 Scott-1 15 min 17 1 3 94.44 85.00 Champ-1 17 min 21 3 2 87.50 91.30 TABLE VI. PERFORMANCE OF CLASSIFIERS FROM LEVEL-2 TO LEVEL-5 Le-Class Recall Precision vel N c N m N f (%) (%) 2 Replay 256 46 41 84.77 86.20 Real-time 222 34 39 86.72 85.06 3 Field-view 229 9 11 96.22 95.42 Non-field-view 291 15 16 95.10 94.79 Long-view 47 6 7 88.68 87.04 4a Straight-view 36 6 6 85.71 85.71 Corner-view 128 15 14 89.51 90.14 4b Close-up 329 59 55 84.79 85.67 Crowd 216 42 46 83.72 82.44 Player-A 132 29 25 81.99 83.97 Player-B 112 27 26 80.58 81.16 5a Goalkeeper-A 28 8 10 77.78 73.68 Goalkeeper-B 30 8 10 78.95 75.00 Referee 13 3 4 81.25 76.47 Players 77 19 18 88.21 81.05 Gathering-A 5b Players 67 14 15 82.72 81.70 Gathering-B Spectator 74 15 15 83.15 83.15 3) Level-3: Field View Detection: Table VII shows the classification performance of field view detection of the images shown in Figure 14, 15. For non-field view, we observed the values less than 0.1. Because of this discrimination, we observed above 94 % recall and precision as shown in Table VI.\n\n4) Level-4a: Field View Classification: The overall performance of the classifier at level-4a is shown in Table VI. Since we consider few previous frame for generating motion-mask, we observed some miss-classification near the shot boundaries. The detection of corner view is very important from semantic extraction point of view, since it is frequently observed in goal and save concepts of the soccer video.\n\n5) Level-4b: Close-up Detection: Table VIII shows the performance of close-up detection for the images of Figure 16. Since more number of edges are observed in crowd images, feature based on edge pixels offers very good discrimination capability. TABLE VII. PERFORMANCE OF LEVEL-3 OF HIERARCHICAL CLASSIFIER Image G peak DGP R Actual class Observed Class Fig. 14(a) 59 0.3058 field view field view Fig. 14(c) 59 0.2263 field view field view Fig. 14(e) 67 0.1728 field view field view Fig. 14(g) 61 0.3056 field view field view Fig. 15(a) 62 0.0013 non-field view non-field view Fig. 15(c) 62 0.0034 non-field view non-field view Fig. 15(e) 60 0.0021 non-field view non-field view\n\nTABLE VIII. CLASSIFICATION OF NON-FIELD VIEWS INTO CROWD AND CLOSE-UPS AT LEVEL-4B Fig. Video P EP Actual Observed 16 ID (%) Class Class (a) 14.13 crowd crowd (b) FIFA-1 12.22 crowd crowd (c) 7.38 close-up close-up (d) 6.32 close-up close-up (i) 9 crowd crowd (j) FIFA-2 10.43 crowd crowd (k) 7.62 close-up close-up (l) 6.44 close-up close-up (q) 16.51 crowd crowd (r) FIFA-3 10.87 crowd crowd (s) 6.54 close-up close-up (t) 5.17 close-up close-up 1 1 1 1 1 6 6 6 6 6 7 7 7 7 7 8 8 8 8 8 10 10 10 10 10 11 11 11 11 11 2 2 2 2 2 3 3 3 3 3 4 4 4 4 4 5 5 5 5 5 9 9 9 9 9 12 12 12 12 12 13 13 13 13 13 14 14 14 14 14 15 15 15 15 15 16 16 16 16 16 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 Figure 17. Examples of successfully classified close-up with complex background of video FIFA-2, Row-1: (a) Player Team-B (Portugal), (b) Player Team-A (England), (c) Player Team-B (Portugal), Row-2: (d) Player Team-A (England), (e) Player Team-B (Portugal), (f) Goalkeeper Team-B (Portugal) (a) (b) ( c ) (d) ( e ) ( f ) Figure 18. Examples of correctly classified crowd clips of video FIFA-2, (a) Spectator, (b) Players Gathering Team-A (England), (c) Players Gathering Team-B (Portugal), (d) Hue-histogram of (a), (e) Huehistogram of (b), (f) Hue-histogram of (c)\n\nTABLE IX. CLOSE-UP CLASSIFICATION OF FIGURE 17 Fig. JB d 1 (JB), d 2 (JB), d 3 (JB), Actual Observed 17 d 4 (JB), d 5 (JB) Class Class (a) 10 2134, 1793, 1341, 1055, 2116 P B P B (b) 10 2637, 2326, 1166, 2165, 2889 P A P A (c) 11 2556, 2174, 1789, 983, 2449 P B P B (d) 15 2273, 2031, 1170, 1969, 2587 P A P A (e) 10 2201, 1835, 1374, 1128, 2145 P B P B (f) 10 2642, 1887, 2035, 2129, 2533 K B K B TABLE X. CROWD CLASSIFICATION OF FIGURE 18 6) Level-5a: Close-up Classification: Figure 17 shows the examples of successfully classified close-ups from video F IF A -2.\n\nThe advantage of our close-up classification scheme is that it gives better results even though there is complex background. The close-up images of Figure 17 have complex background such as banner with large characters, spectator, net in the background. Our classifier has classified these close-up frames successfully as shown in Table IX. As shown in Table VI, we observed the average recall and precision as 80.11% and 78.06% respectively for close-up classification.\n\n7) Level-5b: Crowd Classification: Table X shows the classification of Figure 18 into Spectator, Players Gathering Team-A and Players Gathering Team-B. We observed the average 84.69% and 81.96% recall and precision respectively as shown in Table VI.\n\nVideo event sequence is generated for all extracted excitement clip. The clips with event sequence length less than four are filtered prior to concept mining. We compute SEQAD distance between clip under test and key associations A(ωc\n\nIn this paper, we have presented a hierarchical framework for analyzing high-level events in soccer the classified events for applications such as highlight generation and video summarization.\n\nJOURNAL OF MULTIMEDIA, VOL. 4, NO. 5, OCTOBER 2009 © 2009 ACADEMY PUBLISHER\n\n© 2009 ACADEMY PUBLISHER"
}