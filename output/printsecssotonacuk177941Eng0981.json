{
    "title": "Orthogonal-Least-Squares Forward Selection for Parsimonious Modelling from Data",
    "publication_date": "2009-08",
    "authors": [
        {
            "full_name": "Sheng Chen",
            "firstname": "Sheng",
            "lastname": "Chen",
            "affiliations": [
                {
                    "organization": "School of Electronics and Computers Sciences, University of Southampton",
                    "address": {
                        "city": "Southampton",
                        "country": "UK"
                    }
                }
            ]
        }
    ],
    "abstract": "The objective of modelling from data is not that the model simply fits the training data well. Rather, the goodness of a model is characterized by its generalization capability, interpretability and ease for knowledge extraction. All these desired properties depend crucially on the ability to construct appropriate parsimonious models by the modelling process, and a basic principle in practical nonlinear data modelling is the parsimonious principle of ensuring the smallest possible model that explains the training data. There exists a vast amount of works in the area of sparse modelling, and a widely adopted approach is based on the linear-in-the-parameters data modelling that include the radial basis function network, the neurofuzzy network and all the sparse kernel modelling techniques. A well tested strategy for parsimonious modelling from data is the orthogonal least squares (OLS) algorithm for forward selection modelling, which is capable of constructing sparse models that generalise well. This contribution continues this theme and provides a unified framework for sparse modelling from data that includes regression and classification, which belong to supervised learning, and probability density function estimation, which is an unsupervised learning problem. The OLS forward selection method based on the leave-one-out test criteria is presented within this unified data-modelling framework. Examples from regression, classification and density estimation applications are used to illustrate the effectiveness of this generic parsimonious modelling approach from data.",
    "full_text": "Data modelling is an important and recurrent theme in all the fields of engineering. Various data modelling applications can be classified into three categories, namely, regression [1-3], classification [4][5][6] and probability density function (PDF) estimation [7][8][9]. In regression, the task is to establish a model that links the observation data to their target function or desired output values. The goodness of a regression model is judged by its generalization performance, which can be conveniently determined by the test mean square error (MSE) on the data not used in training the model. Like regression, classification is also a supervised learning problem. However, the desired output is discrete valued, e.g. binary in the two-class classification problems, and the goodness of a classifier is determined by its test error probability or misclassification rate. Despite of these differences, classifier construction can be expressed in the same framework of regression modelling. The third class of data modelling, namely, PDF estimation, is very different in nature from regression and classification. The task of PDF estimation is to infer the underlying probability distribution that generates the observations. Because the true target function, the underlying PDF, is not available, this is an unsupervised learning problem and can only be carried out based on often noisy observation data. Nevertheless, this unsupervised task can be \"transformed\" into a supervised one, for example, by computing the empirical distribution function from the observation data and using it as the target function for the cumulative distribution function of the PDF estimation. This contribution adopts this unified regression framework for data modelling.\n\nThe theory and practice of linear regression modelling is well established [10][11][12], and the least squares (LS) method [13] has been a basic toolkit for data modelling. Since real-world phenomena that generate data are nonlinear to some extent, nonlinear models are often required in order to achieve adequate modelling accuracy. Over the past three decades, extensive efforts have been directed onto developing coherent and concise methods of nonlinear regression modelling . A data modelling problem generally consists of two basic components: determining the model structure and estimating or fitting the model parameters. Parameter fitting is relatively straightforward if the model structure is known a priori but this information is rarely available in practice and must be learnt. Determining the model structure is crucial in any practical data modelling problem, and a fundamental principle is that the model should be no more complex than is required to capture the underlying data generating mechanisms. This concept known as the parsimonious principle is particular relevant in nonlinear data modelling because the size of a nonlinear model can easily become explosively large. An over complicated model may simply fit to the noise in the training data, resulting overfitting. An overfitted model does not capture the underlying system structure and will perform badly on new data. In general, a huge model not only may have poor generalisation performance but also has little practical value in data analysis and system design.\n\nThere exists a vast amount of works in the area of parsimonious nonlinear regression modelling but the most popular approach is perhaps to adopt a linear-in-the-parameters nonlinear model. This is typically achieved by placing a radial basis function (RBF) or other type of kernel on each training data sample and a sparse representation is then sought which possesses excellent generalisation performance . Adopting a linear-in-the-parameters nonlinear model structure is attractive because many existing linear data modelling techniques can be applied successfully, providing that the model structure determination can be carried out effectively to guarantee a sufficiently parsimonious final model. Among the various linear-in-the-parameters nonlinear data modelling techniques, the support vector machine (SVM) method and other sparse kernel modelling methods [54][55][56][57][58][59][60][61][62][63][64][65][66][67][68] have become popular in the recent years. In particular, the SVM technique [54] is widely regarded as the state-of-the-art technique for regression and classification applications, and it has also been proposed as a promising tool for sparse kernel density estimation [69][70][71]. The formulation of SVM embodies the structural risk minimization principle, thus combining excellent generalisation properties with a sparse model representation. Despite of these attractive features and many good empirical results obtained using the SVM method, data modelling practicians have realized that the ability for the SVM method to produce sparse models has perhaps been overstated.\n\nThe orthogonal least squares (OLS) algorithm [34], de-veloped in the late 1980s for nonlinear system modelling, remains highly popular for nonlinear data modelling practicians, for the reason that the algorithm is simple and efficient, and is capable of producing parsimonious linear-in-the-parameters nonlinear models with good generalisation performance. Unlike the SVM and many other sparse kernel modelling techniques, which work on the full kernel model defined on the training data set to obtain a sparse model, the OLS method [34] adopts the forward selection to build up an adequate model by only selecting significant regressors. Since its derivation, many enhanced variants of the OLS based forward regression algorithm have been proposed [37][38][39][40][41][42][43][44][45][46][47][48][49][50][51][52][53]. In particular, the local regularisation assisted OLS algorithm [39,41], which employs the multiple regularisers to enforce the model sparsity [60], has been shown to be capable of producing very sparse regression models that generalise well. A significant improvement to the original OLS algorithm for sparse regression modelling is to enhance the algorithm with optimal experimental design criteria [39,45,47,48,50,51]. In a traditional forward regression procedure, a separate stopping criterion is required to terminate the selection procedure at an appropriate model size in order for example to avoid an over-fitted model. Typically, information based criteria, such as the AIC [72] and the minimum description length [73], were adopted to terminate the model selection process. An information based criterion can be viewed as a model structure regularisation by using a penalty term to penalise large sized models. However, the penalty term in an information based criterion does not help to determine which model term should be selected. Multiple regularisers, i.e. local regularisation [39,41,60], and optimal experimental design criteria [39,45,47,48,50,51] offer better solutions as model structure regularisation as they are directly linked to model efficiency and parameter robustness [74].\n\nThe basic criterion for most model construction procedures, including the original OLS algorithm [34,35], is the training MSE. However, the goodness of a regression model is its generalisation capability. Therefore, a better and more natural approach is using a criterion of model generalisation performance directly in the model selection procedure rather than only using it as a measure of model complexity. The evaluation of model generalisation capability is directly based on the concept of cross validation [75], and a commonly used cross validation is the delete-one or leave-oneout (LOO) cross validation [2, 76,77]. One of the most important improvements to the OLS algorithm based forward regression is the development of the OLS forward selection based on the LOO test score or MSE [40,46,47,49], which is a measure of the model generalisation performance. The use of the LOO estimate for general nonlinear-in-the-parameters models has been studied for example in [78][79][80]. However, even for the class of linear-in-the-parameters models, computation of the LOO MSE is normally expensive and the use of the LOO statistic in model selection is generally prohibitive. Owing to the orthogonal property of the OLS algorithm, the calculation of the LOO statistics beomes efficient and model selection based on the LOO test score is made computationally affordable [46]. An additional advantage of adopting the LOO test score based OLS algorithm is that the model construction process becomes truly automatic without the need for the user to specify some additional terminating criterion [46]. Our empirical modelling results for regression [40], classification [52] and kernel density estimation [43,44] have demonstrated that this OLS algorithm based on the LOO test score coupled with local regularisation compares favourably with the SVM and many other existing state-of-the-art sparse kernel modelling methods, in terms of generalisation capability and model sparsity as well as the computational complexity of model construction.\n\nThis contribution is organized as follows. Section 2 presents the regression modelling framework, which unifies all the three classes of data modelling applications, namely, regression, classification and PDF estimation. In particular, the unsupervised density learning is converted into a supervised regression one by adopting the Parzen window (PW) estimate as the target function [44]. Based on this unified data-modelling framework, the OLS forward selection algorithm using the LOO test criteria and local regularisation is detailed in Section 3. More specifically, for regression modelling, the model selection criterion is based on the LOO test MSE, while for classification applications, the LOO misclassification rate is employed for model selection. In kernel density estimation, the kernel weights must satisfy the nonnegative and unity constraints, and a combined approach is adopted to tackle this constrained regression modelling. A sparse kernel density estimate is first selected by the efficient OLS algorithm based on the LOO test score and local regularisation. The kernel weights of the final model are then updated using the multiplicative nonnegative quadratic programming (MNQP) algorithm [61,81] to meet the nonnegative and unity constraints. The MNQP algorithm additionally has a desired property of forcing some kernel weights to (near) zero values, and thus further reducing the model size [61,81]. The experimental results are included in Section 4, where empirical examples taken from regression, classification and PDF estimation applications demonstrate the effectiveness of the proposed OLS algorithm based on the LOO test criteria coupled with local regularisation within the unified data-modelling framework. The concluding remarks are summarised in Section 5.\n\nThe three classes of data modelling, namely, regression, classification and PDF estimation, can be unified under the generic regression framework of sparse kernel data modelling based on the appropriate modelling criteria, where the kernel model is interpreted in a generic sense, namely, a kernel or nonlinear basis is placed on each training data sample and the model is obtained as a linear combination of all the bases defined on the training data set. For kernel density estimation, a kernel should also meet the usual requirement of a density distribution, i.e. the area under the kernel is unity. The objective is to derive a sparse model representation with excellent generalisation capability based on a training data set.\n\nConsider the general nonlinear data generating mechanism governed by the nonlinear model\n\nwhere y  is the system output,\n\nthe system input, e is a white noise process representing for example the observation noise, and\n\ndefines the unknown system mapping. Given a set of N training data samples, , the task is to infer a kernel model, , of the form\n\nwhere denotes the model output, ŷ i  are the kernel weights and ( ) K    , is the chosen kernel function with a kernel width  . Many types of kernel function can be employed and a commonly used one is the Gaussian function of the form 2 2 2 2 || c || 2 || c || 2 2 /2 , for regression and classification, ( ) 1 , for density estimation (2 )\n\nwhere is the k-th kernel centre vector. The generic kernel model (2) is defined by placing a kernel at each of the training input samples and forming a linear combination of all the bases defined on the training data set. A sparse representation is then sought by selecting kernel model with only N s nonzero kernel weights, where\n\nAt a training data point , the kernel model (2) can be expressed as ( , )\n\nwhere k k y y k   is the modelling error at ,\n\nT , the regression model (4) over the training data set D N can be expressed in the matrix form\n\nNote that k  is the k-th column of Φ N , while denotes the k-th row of Φ N . Let an orthogonal decomposition of the regression matrix Φ N be ( )\n\nand\n\nwith orthogonal columns satisfying , if w w 0\n\nThe regression model ( 5) can alternatively be expressed as\n\nwhere the weight vector defined in the orthogonal model space satisfies the triangular system\n\n. The space spanned by the original model bases k  ,\n\n, is identical to the space spanned by the orthogonal model bases ,\n\nand the model is equivalently expressed by ˆ( )\n\nwhere is the k-th row of .\n\nA procedure that can be used to perform the orthogonalisation ( 6) is summarised in Appendix A.\n\n,1 ,\n\n]\n\nConsider the two-class classification problem with the given training data set\n\n, where is an m-dimensional pattern vector and is the class label for . The task is to construct a kernel classifier of the form\n\nwhere is the estimated class label for and\n\nLet us define the modelling error as k . Then the classification model over the training data set can be expressed in the regression model of (5) recited here again as\n\nor equivalently in the orthogonal regression model of ( 9) rewritten here again as\n\nwhere all the relevant notations are as defined in Subsection 2.1. It is clear that the kernel classifier construction can be expressed in the same kernel regression modelling framework of Subsection 2.1, and the only difference is that the target function y k in classification applications is discrete valued. In particular, for the two-class classification problem, y k is binary. The objective is again to derive a sparse kernel model that posses good generalisation capability and contains only N s significant kernels.\n\nBased on a finite data sample set\n\ndrawn from a density , where , the task is to estimate the unknown density using the kernel density estimate of the form\n\nwith the constraints 0, 1\n\nand\n\nwhere is again the kernel weight vector,\n\n the kernel width and 1 N denotes the vector of ones with dimension N. The kernel function ( )\n\n, is chosen to be the Gaussian kernel in this study. However, many other kernel functions can also be used in the density estimate (15). Following the approach of [44], this unsupervised kernel density learning is transformed into a supervised learning problem.\n\nThe well-known PW estimate [7],\n\nPar Par ˆ( ; , ) p  x  , is obtained by simply setting all the elements of Par  to . The optimal kernel width Par  is typically determined via cross validation [2,75]. The PW estimate is 1 N remarkably simple and accurate [7]. The PW estimate in fact can be derived as the maximum likelihood estimator using the divergence-based criterion [83]. The negative cross-entropy or divergence between the true density Copyright © 2009 SciRes. ENGINEERING ( ) p x and the estimate N ˆ( ; , ) p\n\nMinimising this divergence subject to the constraints ( 16) and (17) leads to n   1 N for , i.e. the 1 n N   PW estimate. A disadvantage associated with the PW estimate is its high computational cost of the point density estimate for a future data sample, as the PW estimate employs the full training data sample set in defining density estimate for subsequent observation. This high test cost has motivated the research on the sparse kernel density estimation techniques [43,44,69-71,81,82]. We may regard the PW estimate as the \"observation\" of the true density contaminated by some \"observation noise\", namely Par Par ˆ( ; , ) ( ) p p\n\nThus the generic kernel density estimation problem (15) can be viewed as the following regression problem with the PW estimate as the \"desired response\" or target function\n\nsubject to the constraints ( 16) and ( 17), where ( ) subject to the nonnegative constraint ( 16) and the unity constrain (17), where all the relevant notations have been defined in Subsection 2.1. The regression model ( 21) can of course be written equivalently in the orthogonal form of ( 9) which is recited here again as\n\nThe objective is to obtain a sparse Ns-term kernel model, satisfying the kernel weight constraints ( 16) and ( 17) and yet having a test performance comparable to that of the full-sample optimized PW estimate.\n\nAs established in the previous section, the regression, classification and PDF estimation can all be unified within the common regression modelling framework. Therefore, the OLS forward selection based on the LOO test criteria and local regularization (OLS-LOO-LR) [40] provides an efficient algorithm to construct a sparse kernel model that generalise well. For the regression and kernel density modelling, the LOO MSE criterion is an appropriate measure of model's generalisation capability for subset model selection, while for kernel classifier construction, the LOO misclassification rate offers a proper measure of classifier's generalisation performance for selecting significant kernels [52]. Sparse kernel density (SKD) construction is special as it is formulated as a constrained regression modelling, where the kernel weights must meet the nonnegative and unity constraints. A combined OLS-LOO-LR and MNQP approach is adopted for this constrained regression modelling [44], where the OLS-LOO-LR algorithm determines the sparse kernel model structure by selecting a subset of significant kernels while the MNOP algorithm [61,81] computes the kernel weights of the selected SKD estimate.\n\nThe local regularization aided least squares solution for the weight parameter vector can be obtained by minimizing the following regularised error criterion [41] g N ( , )\n\nwhere is the vector of regularisation parameters, and\n\nBecause\n\nfor , which is also given in Appendix A. The criterion ( 23) is rooted in the Bayesian learning framework. According to the Bayesian learning theory [24,39,60], the optimal g N is obtained by maximizing the posterior probability of g N , which can be shown to be\n\nwhere ( | , )\n\nis the prior with denoting the vector of hyper-parameteres and\n\nIf the Gaussian prior is chosen, i.e.\n\n2 2 1 ( | , )\n\nmaximising log( ( | ,\n\n) with respect to is equivalent to minimising the following Bayesian cost function\n\nwhere . It is obvious that the criterion ( 23) is equivalent to the criterion (28) with the relationship\n\nThe hyperparameters specify the prior distributions of . Since initially the optimal value of is unknown, should be initialised to the same small value, and this corresponds to choose a same flat distribution for each prior of g i in (27). The beauty of the Bayesian learning framework is that it learns not only the model parameters g N but also the related hyperparameters h N . This can be done by iteratively optimizing g N and h N using the evidence procedure [24,39,60]. Applying this evidence procedure results in the following iterative updating formulas for the regularization parameters [39]\n\nwhere g i for denote the current estimated parameter values, and\n\nUsually a few iterations (typically less than 10) are sufficient to find a (near) optimal . The detailed derivation of the updating Formulas ( 30) and ( 31), quoted from [39], can be found in Appendix B. The use of multiple-regularisers or local regularisation is known to be capable of providing very sparse solutions [41,60].\n\nThe LOO MSE can be computed efficiently due to the fact that the n-term model error ( )   n k and associated LOO error weighting ( )   n k  can be calculated recursively according to [40,46,47] ( )\n\nrespectively, where w k,n is the k-th element of w n . The derivation of the LOO test error (32) together with the recursive Formulas ( 34) and ( 35) is detailed in Appendix C. The subset model selection procedure is carried out as follows. At the n-th stage of the selection procedure, a model term is selected among the remaining n to N candidates if the resulting n-term model produces the smallest LOO MSE J n . The selection procedure is terminated when\n\nyielding an N s -term sparse model. It has been shown in [46] that the LOO statistic J n is at least locally convex with respect to the model size n. That is, there exists an \"optimal\" model size s N such that for s n n N J  decreases as n increases while the condition (36) holds. This property is extremely useful, as it enables the selection procedure to be automatically terminated with an N s -term model, without the need for the user to specify a separate termination criterion. The sparse regression model selection procedure based on the OLS-LOO-LR algorithm is now summarised as follows.\n\nInitialisation: Set  Step 2: Update using ( 30) and (31) with N = N I . If the pre-set maximum iteration number (e.g. 10) is reached, stop; otherwise set I + = 1 and go to Step 1. N \n\nSince the generic kernel classifier construction takes the same form of regression modelling, the OLS-LOO-LR algorithm described in the previous subsection can be applied to select a sparse kernel classifier. However, the goal of a classifier is to minimise the misclassification or error rate, and the MSE in general is not an appropriate criterion for classifier construction. Note that the class label . Define the signed decision variable\n\nThen the misclassification rate over the training data set is evaluated as\n\nwhere the indication function is defined by and the LOO misclassification rate can be computed by\n\nThis LOO misclassification rate is a measure of the classifier's generalisation capability. Moreover, the LOO signed decision variable ( , )   n k k s  can be calculated very fast owning to the orthogonal decomposition and, therefore, the LOO misclassification rate J n can be evaluated efficiently [52]. Specifically, the LOO n-term modelling error is expressed by (also see Appendix C)\n\nMultiplying the both sides of (43) with y k and applying\n\nFrom ( 44), the LOO n-term signed decision variable is given by 2 , , ( )\n\nThe recursive formula for the LOO error weighting\n\n can be represented using the following recursive formula [52] 2 , ( )\n\n( 1) , w w\n\nThe OLS-LOO-LR algorithm described in Subsection 3.1 can readily be applied to select a sparse kernel classifier with some minor modifications. These modifications are due to the fact that the selection criterion is the LOO misclassification rate (42) rather than the LOO MSE (33). Extensive empirical experience has also suggested that, for constructing sparse kernel classifier, multiple regularisers or local regularisation, which is so effective in further enforcing model sparsity in regression, becomes unnecessary. Thus, all the regularisation parameters i  , 1 i N   , can be set to a small positive constant  , and there is no need to update them using the evidence procedure. The sparse kernel classifier selection procedure based on this OLS-LOO algorithm is summarised as follows.\n\nSetting  to a small positive number, and with the following initial conditions (0) (0) 0 0 and 1 for 1 , and 1\n\nuse the procedure described in Appendix E to select a subset model with N s terms.\n\nThe selection procedure of Appendix E is essentially the same one as described in Appendix D, with only minor modifications connected with the computation of the LOO misclassification rate J n . Note that the LOO misclassification rate J n is also locally convex with respect to the classifier's size n. Thus there exists an optimal model size N s such that for s n n N J  decreases as n increases, while\n\nTherefore the selection procedure is automatically terminated with a subset classifier containing only N s significant kernels.\n\nAs shown in Subsection 2.3, the generic kernel density estimation problem can be expressed as a constrained regression modelling, and the regression modelling part itself is identical to that of regression described in Subsection 2.1. Therefore, the OLS-LOO-LR algorithm detailed in Subsection 3.1 can be used to select a sparse kernel density estimate. The only problem is that the kernel weights obtained by the OLS-LOO-LR algorithm for this sparse kernel density estimate do not necessarily meet the nonnegative constraint ( 16) and the unity constraint (17). This \"deficiency\" however can easily be corrected by using the MNQP algorithm to modify or update the kernel weights of the selected sparse model [44]. This combined OLS-LOO-LR and MNQP algorithm offers an effective means of obtaining sparse kernel density estimates with excellent generalisation capability. The detailed OLS-LOO-LR algorithm has been described in Subsection 3.1 and, therefore, only the MNQP part needs to be discussed. After the structure determination using the OLS-LOO-LR algorithm of Subsection 3.1, a sparse N s -term subset kernel model is obtained, where . Let A N s denote the subset matrix of A N , corresponding to the selected N s -term subset model. The kernel weight vector  must be recalculated using for example the MNQP algorithm [61,81]. Note that, since N s is very small, the extra computation involved is small. Formally, this task is defined as follows. Find min\n\ny  s the d-form solution for this related de-(52) where sign m ... ] T N v . Although\n\nthere exists no close optimisation problem, the solution can readily be obtained iteratively using a modified version of the MNQP algorithm [61].\n\nSince the elements of\n\ns s s N C and s N v are strictly positive, th bove e Lagrangian for the a\n\nproblem can be formed as [81] 1 2 ( ) 1\n\nwhere the superindex < t > denotes the iteratio L n index and h is the Lagrangian multiplier. Setting 1 0 and\n\nleads to the following updating equations  . During the iterative procedure, some of the kern ights may be driven to (near) zero [61,81]. The corresponding kernels can then be removed from the kernel model, leading to a further reduction in the subset model size.\n\nel we he regression, classifica-\n\neveral examples, taken from t S tion and density estimation applications, were used to demonstrate the effectiveness of the proposed unified regression modelling approach. For each of these data modelling examples, the full regression model set was formed by placing a Gaussian kernel (3) on each training data sample, and a sparse kernel model was then selected ons his example used a Gaussian kernel model to fit the using the OLS-LOO-LR algorithm. For sparse kernel density estimate, additionally, the MNQP algorithm described in Subsection 3.3 was applied to update the kernel weight vector. The appropriate value for the kernel width ρ was found empirically via cross validation. The obtained model's generalisation performance was evaluated based on a separate test data set not used for training. Comparsion with some existing sparse kernel modelling techniques was made, in terms of the model generalisation performance, model sparsity and the complexity of model construction process. of r nction modelling. l size training MSE noisy test MSE noisefree test MSE ping (thick curve), and selected kernel centres (circles). The 7-term kernel model was id fied by the proposed OLS algorithm.\n\nautomatically selected a 7-term kernel model. The modelling accuracy of the resulting 7-term kernel model is summarised in Table 1, and the corresponding model mapping generated by this 7-term kernel model is depicted in Figure 1, in comparison with the true scalar function (58).\n\nThe relevance vector machine (RVM) algorithm [60] is an existing sparse kernel modelling algorithm that is ten regarded as the state-of-the-art. It has the same excellent generalisation performance as the SVM algorithm but achieves a dramatically sparser kernel model than the SVM method. A drawback of the RVM method is a significant increase in computational complexity, compared with the SVM method. The iterative procedure for updating multiple regularisers in the RVM method converges much slower and may even suffer from numerical instability, compared with the efficient OLS-LOO-LR algorithm. The detailed comparison for these two sparse kernel modelling algorithms is given in [40]. The RVM algorithm was also applied to fit a sparse Gaussian kernel model for this example, and the algorithm produced the 15-term kernel model as listed in Table 1. The model mapping generated by the 15-term kernel model constructed using the RVM algorithm is shown in Figure 2. It can be seen that the OLS-LOO-LR algorithm and the RVM algorithm both had the same excellent generalisation performance, but the former produced a much sparser model than the latter. The OLS-LOO-LR algorithm additionally had significantly computational advantages in model construction.\n\nTable 1. Comparison of modelling accuracy for the scala fu algorithm mode OLS 7 0.038792 0.042001 0.000736 RVM 15 0 .038784 0 .041827 0.000668 (a) (b) Figure 3. Engine data set (a ut and (b) output d a model representing the rela-(59) where ) inp k u k y .\n\nThis example constructe tionship between the fuel rack position (input k u ) and the engine speed (output k y ) for a Leyland TL11 turbocharged, direct injection diesel engine operated at low engine speed. It is known that at low engine speed, the relationship between the input and output is nonlinear [84]. Detailed system description and experimental setup can be found in [84]. The data set, depicted in Figure 3, contained 410 samples. The first 210 data points were used in modelling and the last 200 points in model validation. The previous results [84] have shown that this data set can be modelled adequately as ( )\n\nTable 2. Comparison of modelling accurac algorithm model training test MSE\n\nown system to be identifi denotes the system noise, and\n\ny for the engine data set. size MSE OLS 22 0 .000453 0.000490 SVM 92 0.000447 0.000498 (a) (b) igure 4. Modelling perform for the engine data set: (a) F ance model prediction ˆk y (dashed) superimposed on system output k y (solid), nd (b) model prediction error a k k k y y   he 22-term model was constructed by the pr S algorithm. The optimal value of the . T L okernel variance for the Gausia at algorit posed O s n kernel was found empirically to be 2 1.69   . As each k\n\nx in the training data set was considered as a candid e kernel centre, there were N = 210 candidate kernel regressors in the full regression model (5).\n\nBoth the OLS-LOO-LR algorithm and the SVM hm [56] were applied to this data set, and the two sparse Gaussian kernel models obtained are compared in  4. The modelling performance of the 92-term kernel model constructed by the SVM algorithm, not shown here, are very similar to those shown in Figure 4. It can be seen that the two sparse regression modelling techniques achieved the same excellent generalisation performance but the OLS-LOO-LR method obtained a much sparser model than the SVM method. It should be emphasised that the model size is critically important for this particular example. The main purpose of identifying a model for this engine system is to use it for designing a controller. A large model will make the controller design a very complex task and, moreover, the resulting controller will be difficult to implement in the real system. It is also worth emphasising that the OLS-LOO-LR algorithm has considerably computational advantages over the SVM algorithm. Both the algorithms require to determine the kernel width ρ. However, the SVM method has two more learning parameters, namely the error-band and trade-off parameters [56], that require tuning. Therefore, the OLS-LOO-LR algorithm is easier to tune and computationally more efficient than the SVM algorithm. This was a regression benchmark the UCI repository [85]. The data set comprised 506 data points with 14 variables. The task was to predict the median house value from the remaining 13 attributes. From the data set, 456 data points were randomly selected for training and the remaining 50 data points were used to form the test set. Because a Gaussian kernel was placed at each training data sample, there were N = 456 candidate regressors in the full regression model (5). The kernel width for the OLS-LOO-LR algorithm was determined via a grid-search based cross validation. Similarly, the three learning parameters of the SVM, the kernel width, error-band and trade-off parameters, were tuned via cross validation. Average results were given over 100 repetitions, and the two sparse Gaussian kernel models obtained by the OLS-LOO-LR and SVM algorithms, respectively, are compared in Table 3.\n\nFor the particular computational plat periment, the recorded average run time for the OLS-LOO-LR algorithm when the kernel width was fixed was 200 times faster than the SVM algorithmfoot_0 when the kernel width, error-band and trade-off parameters were chosen. It can be seen from Table 3 that the OLS-LOO-LR algorithm achieved better modelling accuracy with a much sparser model than the SVM algorithm.\n\nn. MSE OLS 58.6 ± 11.3 12.9690 ± 2.6628 17.4157 ± 4.6670 SVM 243.2 ± 5.3 6.7986 ± 0.4444 23.1750 ± 9.0459 Table 4. Comparison of class cation accuracy for the first algorithm average e average test ifi 10 realizations of the breast cancer data set. The results for the SVM and RVM algorithms were quoted from [60]. model siz error rate SVM 116.7 26.9% RVM 6.3 29.9% OLS 5.8 27.4% he test MSE of the SVM algorithm was poor. This was .2. Classification Applications .2.1. Breast Cancer Data rk data set was originated in T probably because the three learning parameters, namely the kernel width, error-band and trade-off parameters,\n\nwere not tuned to the optimal values. For this regression problem of input dimension 13 and data size N ≈ 500, the grid search required by the SVM algorithm to tune the three learning parameters was expensive and the optimal values of the three learning parameters were hard to find, compared with for example the previous smaller engine data set.\n\nThis classification benchma the UCI repository [85] and the actual data set used in the experiment was obtained from [86]. The feature input space dimension was m =9. There were 100 realizations of this data set, each containing 200 training patterns and 77 test patterns. In [60], the SVM and RVM algorithms were applied to the first 10 realizations of this data set, and the results given in [60] were reproduced in Table 4.\n\nThe OLS-LOO algorithm described in Subsection 3.2 was also applied to construct Gaussian kernel classifiers for the same first 10 realisations of this data set, and the results obtained are summarised in Table 4, in comparison with those obtained by the SVM and RVM algorithms. In [86,87], seven existing state-of-the-art RBF and kernel classifier construction algorithms were compared and the performance averaged over all the 100 realizations were given. The OLS-LOO algorithm was applied to all the 100 realizations of the data set to construct sparse Gaussian kernel classifiers and the results obtained are given in\n\nTable 5, in comparison with the benchmark results quoted from [86,87]. For the first 5 .2.2. Diabetes Data ification benchmark data set in the or rate in % over the error rate model size methods studied in [86], the RBF network with 5 optimised nonlinear Gaussian units was used. The kernel Fisher discriminant was the optimal nonsparse method that placed a Gaussian kernel on every training data sample. For the SVM method with the Gaussian kernel, no average model size was given in [86] but it was certainly much larger than 50. From Table 5, it can be seen that the proposed OLS-LOO algorithm compared favourably with these benchmark RBF and kernel classifier construction algorithms, both in terms of classification accuracy and model size. 4 This was another class UCI repository [85] and the data set used in the experiment was obtained from [86]. The feature space dimension was m = 8. There were 100 realisations of the data set, each having 468 training patterns and 300 test patterns. Seven benchmark RBF and kernel classifiers were studied in [86,87], and the results given in [86] were reproduced in Table 6. For the first 5 methods studied in [86], the nonlinear RBF network with 15 optimised Gaussian units was used. For the SVM algorithm with Gaussian kernel, no average model size was given in [86]\n\nbut it could safely be assumed that it was much larger than 100. The OLS-LOO algorithm was applied to construct sparse Gaussian kernel classifiers for this data set, and the results averaged over the 100 realisations are also listed in Table 6. It can be seen that the proposed OLS-LOO method produced the best classification accuracy with the smallest classifier.\n\nable 5. Average classification test err T 100 realizations of the breast cancer data set. The first 7 results were quoted from [86]. algorithm test RBF-Network 27.64 ± 4.71 5 AdaBoost RBF-Network not a able iscriminant 30.36 ± 4.73 5 LP-Reg-AdaBoost 26.79 ± 6.08 5 QP-Reg-AdaBoost 25.91 ± 4.61 5 AdaBoost-Reg 26.51 ± 4.47 5 SVM Fisher D 26.04 ± 4.74 vail Kernel 24.77 ± 4.63 200 6. 0 OLS 25.74 ± 5.00 0 ± 2. able 6. Average classification test error rate in % over the test error rate model size T 100 realizations of the diabetes data set. The first 7 results were quoted from [86]. algorithm RBF-Network 24.29± 1.88 15 AdaBoost RBF-Network not a ble 26.47 ± 2.29 15 LP-Reg-AdaBoost 24.11 ± 1.90 15 QP-Reg-AdaBoost 25.39 ± 2.20 15 AdaBoost-Reg 23.79 ± 1.80 15 SVM Fisher Discriminant 23.53 ± 1.73 vaila Kernel 23.21 ± 1.63 468 6. 0 OLS 23.00 ± 1.70 0 ± 1. T . Average classification e in e test error rate model size able 7 test error rat % over th 100 realizations of the thyroid data set. The first 7 results were quoted from [86]. algorithm RBF-Network 4.52 ± 2.12 8 AdaBoost RBF-Network not av able Fisher Discriminant 4. 0 4.40 ± 2.18 8 LP-Reg-AdaBoost 4.59 ± 2.22 8 QP-Reg-AdaBoost 4.35 ± 2.18 8 AdaBoost-Reg 4.55 ± 2.19 8 SVM 4.80 ± 2.19 ail Kernel 4.20 ± 2.07 140 OLS 4.80 ± 2.20 6 ± 1. 4.2.3. Thyroid Data chmark data set was originated in .3. Density Estimation Applications imulation was used to test the proposed combined This classification ben the UCI repository [85] and the data set used in the experiment was obtained from [86]. The input space dimension was m = 5. There were 100 realizations of this data set, each containing 140 training patterns and 75 test patterns. Eight RBF and kernel classifiers are compared in Table 7, with the first seven methods quoted from [86,87]. It can be seen that the classification accuracy of the proposed OLS-LOO method is comparable to that of the SVM method, but the former achieved a much smaller model size than the latter.\n\nS OLS-LOO-LR and MNQP algorithm and to compare its performance with the Parzen window estimator as well as the previous sparse kernel density estimation algorithm [43]. The algorithm presented in [43], although also based on the OLS-LOO-LR regression framework, is very different from the current combined OLS-LOO-LR and MNQP algorithm. In particular, it transfers the kernels into the corresponding cumulative distribution functions and uses the empirical distribution function calculated on the training data set as the target function of the unknown cumulative distribution function. In other words, the regression framework is defined in the cumulative distribution function \"space\", not the original PDF \"space\". Converting the kernels into corresponding cumulative distribution functions can be inconvenient and may be difficult for certain types of kernels. Moreover, in the work [43], the unity constraint is met by normalising the kernel weight vector of the final selected model, which is nonoptimal, and the nonnegative constraint is ensured by adding a test to the OLS forward selection procedure. In each selection stage, a candidate that causes the resulting kernel weight vector to have negative elements, if included, will not be considered at all. This nonnegative test imposes considerable computational cost to the OLS selection procedure. The proposed combined OLS-LOO-LR and MNQP algorithm in com-\n\nCopyright © 2009 SciRes. ENGINEERING the simulation were on parison is computationally simpler.\n\nThe first and third examples in e-dimensional and six-dimensional density estimation problems, respectively, where a data set of N randomly drawn samples was used to construct kernel density estimates based on the regression model (21), and a separate test data set of N test = 10,000 samples was used to calculate the L 1 test error for the resulting estimate according to\n\nThe experiment was repeated N run different random runs. The second example was a two-class two-dimensional classification problem taken from [6]. For all the three example, the value of the kernel width  was determined via cross validation.\n\nd was the The one-dimensional density to be estimate mixture of Gaussian and Laplacian given by 2 ( 2) 1 0 . 7\n\nThe number of data points for density estimation was N = 100. The optimal kernel widths were found to be  = 0.54 and  = 1.1 empirically for the Parzen window estimate and e proposed sparse kernel density estimate, respectively. The experiment was repeated N run = 200 times. Table 8 compares the performance of these two kernel density estimates, in terms of the L 1 test error and the number of kernels required. pirical distribution function as the desired response and based on the OLS-LOO-LR algorithm only [43] was also applied to this example. Under the identical experimental conditions, the results obtained by this sparse kernel density estimator are also given in Table 8, where it can be seen that the both sparse kernel density estimators had a very similar performance, in terms of the L 1 test error and average number of kernels required.\n\nTable 8. Performance comparison for the one-dimen Gaussian and Laplacian mixture. Table 9 lists the results obtained by the three kernel density estimates, the Parzen window estimato nt sparse kernel density estimator based on the combined OLS-LOO-LR and MNQP algorithm, and the previous sparse kernel density estimator with the empirical distribution function as the desired response and based on the OLS-LOO-LR algorithm only [43], where the value of the kernel width was determined by minimizing the test error rate. It can be seen that the proposed sparse kernel density estimation method yielded the very sparse conditional density estimates and achieved the optimal Bayes classification performance. This clearly demonstrated the accuracy of the density estimates.\n\nThe estimate data set contained N = 600 samples. The ptimal kernel width was found to be ρ = 0.65 for the Pa  10. For this example, again, the two density estimates were seen to have comparable accuracies,\n\nTable 9. Performance comparison for the two-dimensional classification data set. method ˆ( | C0) p  ρ ˆ( | C1) p  ρ test error rate Parzen 125 kenels 0. 125 kernels 3 window estimate 24 0.2 8.0% proposed SKD estimate 6 kernels 0.28 5 kernels 0.28 8.0% SKD estimate of [43] 5 kernels 0.20 4 kernels 0.20 8.3% T Performance comparison for the six-dimensional ree-Gaussian mixture. able 10. th method L 1 test error kernel number Parzen window (3.51 estimate 95 ± 0.1616) 600 ± 0 × 10 -5 proposed SKD estimate (3.1134 ± 0.5335) ×10 -5 9.4 ± 1.9 SKD estimate of [43] (4.4781 ± 1.2292) ×10 -5 14.9 ± 2.1 b roposed sp ernel density ethod chieved very sparse estimates with an average number the same experimental co rk has been proposed for sparse odelling from data, which unifies the supervised re-ut the p arse k estimate m a of required kernels less than 2% of the data samples. The maximum and minimum numbers of kernels over 100 runs were 16 and 7, respectively, for the proposed sparse kernel density estimator.\n\nThis example was used to test the sparse kernel density estimation method of [43] under nditions. The results obtained by this previous sparse kernel density estimator, quoted from [43], are also given in Table 10 for comparison. It is seen from Table 10 that for this high dimensional example the proposed sparse kernel density estimator outperformed the previous sparse kernel density estimator in terms of both the test performance and the level of model sparsity.\n\nA regression framewo m gression and classification problems as well as the unsupervised probability density function learning problem in the same kernel regression model. A powerful orthogonal-least-squares algorithm has been developed for selecting very sparse kernel models that generalise well, based on the leave-one-out test criteria and coupled with local regularisation. For sparse kernel density estimation, in particular, a combined approach of the OLS-LOO-LR algorithm and multiplicative nonnegative quadratic programming has been proposed, with the OLS-LOO-LR algorithm selecting a sparse kernel density estimate while the MNQP algorithm computing the kernel weights of the selected final model to meet the constraints for den-sity estimate. Empirical datamodelling results involving all the three classes of data modelling, namely regression, classification and density estimation, have been presented to demonstrate the effectiveness of the proposed unified kernel regression modelling framework based on the OLS-LOO-LR algorithm, and the results shown have clearly confirmed that this proposed unified sparse kernel regression framework offers a truly state-of-art for data modelling applications. ements e contributions of Dr. Xia ram-Schmidt orthogonalisation procees density estimate obtained by the combined OLS-LOO-LR and MNQP algorithm, for the two-class two-dimensional classification example, where circles represent the class-1 training data and crosses the class-0 training data.\n\nThe unified regression framework developed in this contribution is based on the linear-in-the-parameters kernel model, where the full candidate kernel set is obtained by placing a kernel at each training data point and employing a fixed kernel width for all the kernel regressors. Further reseach has been conducted to develop a nonlinear-in-the-parameters regression model, where each regressor has tunable base centre vector and diagonal covariance matrix. A powerful orthogonalleast-squares assisted forward selection procedure can be developed based on the leave-one-out test criteria and local regulation. At each stage of the construction procedure, a nonlinear base is constructed by optimising the appropriate LOO test criterion to determine the base's centre vector and diagonal covariance matrix. Such sparse data modelling techniques based on tunable nonlinear base units have been proposed for regression data modelling [88] and classification application [89]. Sparse density estimation based on this novel tunable regression modelling framework is currently under investigation [90].\n\nhe author acknowledges th T Hong and Professor Chris J. Harris to the topic reported in this work.\n\nThe modified G dure [34] calculate the A N matrix row by row and orthogonalises N  as follows: At the l-th stage make the columns j  , 1 l j N    , orthogonal to the l-th column and pe tion for 1 l j N    . Specifically, denoting [0]   re at the opera\n\nThe last stage of the procedure is simply N  w\n\nwhere , 1 l l N    , are the regularisation parameters.\n\nIt can be shown th N  is [24] 1 1 log( ( | , )) log( ) log( )\n\nSetting\n\nLet t -th data sample be deleted from the t data set N , and the resulting leave-one-out training is he k raining D set used to estimate the model parameter vector. The corresponding regularised least squares solution is defined by\n\nBy definition, it can be show\n\nThe LOO test error evaluated at the k-th not used for training, denoted as , is given data sample by Let a very small positive number T be given d to automatically lar problem. With t e initial conditions as specified in (37), the l-th stage of the selection procedure is given as follows.\n\nStep 1. For l j N   :\n\n• Test -Conditioning number check. If  \n\nOne could argue that by adopting fast implementation of the SVM algorithm significant reduction in run time can be achieved."
}