{
    "title": "Smoothing densities under shape constraints",
    "publication_date": "2009-05-27",
    "authors": [
        {
            "full_name": "P L Davies",
            "firstname": "P L",
            "lastname": "Davies",
            "affiliations": [
                {
                    "organization": "Universität Duisburg-Essen",
                    "address": {}
                }
            ]
        },
        {
            "full_name": "M Meise",
            "firstname": "M",
            "lastname": "Meise",
            "affiliations": [
                {
                    "organization": "Universität Duisburg-Essen",
                    "address": {}
                }
            ]
        }
    ],
    "abstract": "In\nDavies and Kovac (2004)\nthe taut string method was proposed for calculating a density which is consistent with the data and has the minimum number of peaks. The main disadvantage of the taut string density is that it is piecewise constant. In this paper a procedure is presented which gives a smoother density by minimizing the total variation of a derivative of the density subject to the number, positions and heights of the local extreme values obtained from the taut string density.",
    "full_text": "There are several papers on the shape of densities, see for example Groeneboom (1985), Meyer (2001), Davies and Kovac (2004), Meyer and Woodroofe (2004), Dümbgen and Walther (2008), Dümbgen and Rufibach (2009) and Jongbloed and Van der Meulen (2009), but with the exceptions of Davies and Kovac (2004) and Dümbgen and Walther (2008) they are restricted either to monotone or concave or unimodal densities. Using the results of Dümbgen and Walther (2008) it is possible to derive a lower bound for the number of local extreme values of a density consistent with the data, but the method does not return a candidate density. The taut string procedure of Davies and Kovac (2004) returns a candidate density and in simulations it has proved efficacious for estimating the number, positions and heights of the local extreme values of the generating density (see Davies et al. (2008)). As an example we mention the claw density of Marron and Wand (1992) which is a mixture of six normal distributions and has five peaks. For samples of size n = 500 the taut string procedure will correctly identify all the peaks in about 80% of simulated data sets. The left panel of Figure 1 shows one such result. The right panel shows a taut string estimate of the exponential density f (x) = exp(-x) based on a sample of size n = 500. In spite of the longish tale of this density the taut string does not produce superfluous peaks. Figure 1 also shows the main problem with the taut string procedure, namely that the densities it produces are piecewise constant. In this paper a method of smoothing the densities produced by the taut string is described which respects the number, positions and heights of the local extreme values.\n\nSection 2 contains the basic idea of the proposed procedure which is to minimize the total variation of the second derivative subject to linear constraints. This leads to a standard\n\n-2 -1 0 1 2 0.0 0.1 0.2 0.3 0.4 0.5 0.6 x f(x) 0 1 2 3 4 5 0.0 0.2 0.4 0.6 0.8 x f(x)\n\nFigure 1: Two examples for the result of the taut string method. Left: Fitted density (solid) and true density (dashed) for a Claw distributed sample. Right: Fitted density (solid) and true density (dashed) for a exponentially distributed sample with parameter λ = 1. In both cases the sample size is n = 500.\n\nlinear programming problem. Section 3 gives the results for some simulated and real data sets and Section 4 contains a short discussion of the difficulties of extending the procedure to heavy-tailed distributions and to higher derivatives.\n\n2 Smoothing the density 2.1 Basic idea of the procedure Given an ordered sample x n = (x (1) , . . . , x (n) ) and a density function f we define\n\nwhere the dependence on the data x n has been suppressed. The method is based on the taut string density f n,T S which is calculated as described in Davies and Kovac (2004). The goal is to compute a smooth density f n which preserves the number, positions and heights of the local extreme values of f n,T S . The preservation of these aspects of the taut string density can be expressed by a set of linear inequalities involving f n . A smooth density satisfying these constraints can be obtained by solving a minimization problem of the form\n\nwhere R(f ) is a measure of roughness of the density f . The matrix A and the vector b describing the linear restriction will be specified below.\n\nThere are many possible choices for the measure of roughness in (1) some of which lead to non-linear programming problems which are difficult to solve. One example is the Fisher information\n\nIf we restrict attention to the total variation of a derivative of f where the partitions are restricted to the data set, then (1) leads to a linear programming problem. For the kth derivative f (k) the total variation is defined as\n\nAs pointed out in Mammen and van de Geer (1997) a function f with minimum T V (f (k) ) can always be interpolated at the data points by a spline of order k. Thus solving (1) with\n\n) for k = 1 and k = 2 results in a piecewise linear and a piecewise quadratic density respectively. Knowledge of the general structure of the solution can be exploited during the computation. Using the vector norm y = i |y i | the minimization problem can be written as a standard linear programme min\n\nwhere A k represents (2). Smoothing the taut string density f n,T S can cause the integrated distribution function F n of the smooth density f n to move away from the empirical distribution function E n of the data:\n\nTo prevent this we impose the condition\n\nwhere d ko denotes the Kolmogorov metric. The term 1.36/ √ n in ( 5) is the 0.95-quantile of the Kolmogorov metric. This requirement is usually too weak and below we describe how we iteratively decrease the width of the Kolmogorov tube to obtain a more satisfactory solution. The condition (5) is also expressible as a set of linear inequalities which we incorporate in the matrix A and the vector b of (3).\n\nIt turns out that it is easier to solve the linear programme in terms of\n\n2 , . . . , f\n\nrather than directly in terms of the density itself. This leads to a linear programme of the form min\n\nwhich is formulated precisely in the next section.\n\nAfter the piecewise constant taut string fit f n,T S has been computed, the number and location of the local extreme values of f n,T S are evaluated. We take the positions of the local extreme points to be the mid-points of the intervals of constancy which correspond to local extreme values (see upper left panel of Figure 2). We denote the locations of the local minima and maxima by e - j = x (s j ) , j = 1, . . . , n min and e + j = x (r j ) , j = 1, . . . , n max respectively. To guarantee that the solution f n,T V k of the smoothed problem has the same local extreme values we require\n\nf n,T V k (e + j ) ≥ f n,T S (e + j ) for j = 1, . . . , n max .\n\nWe use here inequality rather than equality signs as this makes the L 1 problem more stable.\n\nIn practice the overshoot is small due to the regularization. Between the extreme values we require the correct monotonicity which leads to the following inequalities\n\ndepending on the monotonicity of f n,T S between the local extremes. We note that these are only necessary conditions as they do not guarantee the correct monotonicity of the spline between the data points. We consider this problem below. Apart from the above inequalities we also have 2n inequalities which derive from the condition (5). As the final solution has to be a density this gives rise to a further n nonnegativity constraints. Finally we may also impose boundary conditions on the values of the density and its derivatives. In all we have of the order of 4n linear restrictions. This makes it impossible to solve the L 1 problem by standard methods except for relatively small samples sizes. Our method is to take a sub-sample of the original sample which includes all the points of e + j and e - j of ( 6) and (7). Since it is known that f n,T V k is a spline of order k a fit calculated only on a sub-sample can be extended to the entire data set without difficulty. It can then be checked if this solves the original problem. It turns out that a reasonable number of observations which can be treated relatively fast is about 200 but even sub-samples of size 500 can also be accommodated at the cost of an increase in computer time. Hence for small data sets all observations x (1) , . . . , x (n) can be used but for larger sample sizes, especially for n ≥ 500, a subset x(1) , . . . , x( m) of size m is chosen which is given by\n\nAfter the inclusion of the e + j and e - j the reduced sample is of size at most m = m+n min +n max . The optimization problem min T V (f (k) ) can be written in the standard form of a linear programme: min A k f which is replaced by the equivalent problem min B k c k . Using now the reduced sample we define the (m -k -1) × m matrix B k and the vector c k as follows:\n\n. . .\n\nThe definition of c k and hence the entire setup of the optimization problem uses that f (k) of the solution is piecewise constant. Based on this definition we define the m × m matrices E i , i = 1, 2, 3 to state all necessary linear side conditions:\n\n. . .\n\nTo guarantee that the resulting approximation f n,T V k is a density vector with distribution\n\nwhere u = (u 1 , . . . , u m ) and l = (l 1 , . . . , l m ) are the upper and the lower bound respectively of the 0.95%-Kolmogorov tube and hence\n\nTo keep notation simple we set u m = l m = 1 and then have\n\nFollowing ( 6) and ( 7) we require to guarantee the monotonicity\n\n. . , n min where rj and sj are the corresponding indices of r j and s j in the reduced sample. The values f n,T S (e + j ), j = 1, . . . , n max and f n,T S (e - j ), j = 1, . . . , n min are stored in the vectors f e + n,T S and f e - n,T S respectively. Additionally we have (cf. ( 8)) conditions of the type\n\nfor all i = 1, . . . , m -1 depending on the monotonicity of f n,T S of the according interval. All such inequalities are stored in the matrix M and result in the following additional conditions\n\nFinally the constrained optimization problem can be stated as\n\nx f q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q -3 -2\n\nx f q q q q q q q q qqq q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q qqq q qq q q The optimization problem (9) is solved using the simplex algorithm of Barrodale andRoberts (1978, 1980). In our limited experience interior point methods have problems with the large number of monotonicity conditions and are less stable than simplex algorithms.\n\nThe solution of (9) gives the density estimate f n,T V k on the design points x1 , . . . , xm . This is then extrapolated to the entire sample x 1 , . . . , x n . As mentioned before, we use k = 2 and hence f n,T V 2 consists of quadratic pieces. It may happen that the extended density does not satisfy the required inequalities in some intervals between two data points. In particular it may not have the correct monotonicity behaviour. In such cases the mid-point of the interval is included in the reduced sample and the entire procedure is then repeated until f n,T V 2 satisfies all inequalities.\n\nFigure 2 shows the different steps of the procedure for a simulated data set X F n with F being the Claw distribution and n = 500. The upper left panel shows f n,T S with marked extreme values, the upper right panel shows f n,T V 2 on the reduced sample. The lower left panel the version extended to the whole data set, the lower right panel shows f n,T S and f n,T V 2 together. Thus we finally have a smooth density fit f n,T V 2 with the same monotonicity properties as f n,T S such that the fitted distribution function F n,T V 2 is close to the empirical distribution.\n\nThe method described so far has problems adapting to the tails of the claw density, at least for the sample sizes used here (cf. left panel of Figure 3). This is due to the relatively wide tube width of the Kolmogorov ball which is used to force the integrated density to be close to the empirical distribution function of the data (5). Better results can be obtained by applying the Kuiper criterion as used in Davies and Kovac (2004): Squeezing the Kolmogorov tube gives a sequence of densities whose modality is nondecreasing. If at some stage the taut string distribution function is F n,T S the transformed data F n,T S (x i ), i = 1, . . . , n is tested for a uniform distribution. If the test is passed, the squeezing stops and is otherwise continued. The test for uniformity is based on the increments of Kuiper distances d κ ku up to order κ = 19 (default value) as defined by\n\nThe present version of the taut string method also allows for testing for a uniform distribution using the multiscale inference of Dümbgen and Walther (2008). This allows small concentrated peaks of lower power to be detected whereas the test based on the Kuiper metrics can only detect peaks containing O( √ n) observations. For smoothing f n,T S the order κ of the Kuiper criterion is chosen depending on the number of peaks detected by the taut string procedure: if the taut string density f n,T S has k peaks then we use the increments of the Kuiper distances up to order κ = 2k -1, see Davies and Kovac (2004). For large data sets this causes the Kolomogorov tube to become very small which may cause numerical problems when solving the linear programme. In such a case we terminate the iterations at the smallest tube width\n\n-3 -2 -1 0 1 2 3 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 x f -2 -1 0 1 2 3 4 -1.5 -1.0 -0.5 0.0 0.5 1.0 1.5\n\nx f1 q q q q q q q q q q Figure 4: Left: f n,T V 1 (solid) together with the true density function f (dashed), right:\n\nfor the sample of Fig. 3 which does not lead to numerical problems. The result of doing this is shown in the right panel of Figure 3. The fit is clearly better but is still not satisfactory because the resulting density is now concave in the tails rather than convex. We consider this problem in the following section.\n\nThe concavity/convexity behaviour of the solution can be regulated as follows. The monotonicity behaviour of the smooth density was derived from that of the taut string density. A similar reference density is required for the concavity/convexity behaviour and there are good reasons for using the density f n,T V 1 which minimizes the total variation of the first derivative k = 1 subject to the monotonicity constraints. This density is piecewise linear and is shown in the left panel of Figure 4 for the data of Figure 3: the first derivative is shown in the right panel. At each change-over of the density from convex to concave and from concave to convex there is a common straight-line section which is a local extreme value of the first derivative. The mid-point of this interval is used to delineate the intervals of convexity and concavity as shown in the right panel of Figure 4.\n\nThe concavity/convexity conditions lead to further linear inequalities of the form\n\ni+1 , i = 2, . . . , m -1.\n\nUsing a matrix C these can be expressed as and then included in the matrix E of (9) by putting\n\nFigure 5 shows the density estimate for the sample of Figure 3 which is obtained by solving the optimization problem with additional convexity conditions.\n\nFigure 6 shows the results of applying the procedure to a N (0.1) density (upper left), an exponential density (upper right), a mixture of Γ-densities (lower left) and the uniform density on [0, 1] (lower right). In all cases the sample size was n = 2000. In all cases the resulting densities have the correct monotonicity and concavity/convexity behaviour and are smooth, at least to the eye at the level of the densities themselves. The first derivatives of the densities are continuous and piecewise linear. The second derivatives are piecewise constant with, of necessity, jumps.\n\nThe results for four real data sets are shown in Figure 7. The first three data sets are data of Richardson and Green (1997). The top left panel shows the enzyme data with sample size n = 245. The top right panel shows the acidity data with sample size n = 155. The centre   left panel shows the galaxy data with sample size n = 82. The centre right panel shows the stamp data of Izenman and Sommer (1988) with sample size n = 485. Finally the bottom panel shows the density derived from the daily returns of the German DAX-index from the 4th of Jan. 1960 -28th of Dec. 2001. The sample size is n = 10515. The results are in all cases satisfactory but some comments on the stamp data are in order.\n\nThe stamp data are given in integer multiples of 0.001 and are characterized by a high degree of discreteness. For example there are 42 observations with the value 0.079 and 37 observations with the value 0.080. This degree of discreteness is incompatible with a density and any self-respecting density estimator should refuse to provide an estimator, as does the taut string. One possibility is to explicitly allow for rounding to the nearest multiple of 0.001. This would mean that the 42 observations with the value 0.079 can be taken as uniformly distributed over the interval [0.0785, 0.0795]. One possibility is to do this at random by adding 0.001(U i -0.5) to the observation x i with the U i uniformly distributed over (0, 1) If we now apply the taut string the results are not stable. Without the multiscale inference of Dümbgen and Walther (2008) the taut string returns one or two peaks and occasionally three peaks. If the multiscale inference of Dümbgen and Walther (2008) is applied then taut string returns mostly the three peaks shown in Figure 7 but it can also return 6, 7 and 8 peaks. If the discreteness is broken by placing the points equidistantly on the interval (x i -0.0005, x i + 0.0005) rather than randomly then the result is again the three peaks shown in Figure 7. We refer to Izenman and Sommer (1988) for an interesting discussion of this data set.\n\nAs with all procedures the one presented here has limits. It will not perform well for data with a large number of outlying observations such as are generated by the Cauchy distribution. Smoothing the very long tails whilst respecting the monotonicity constraints deriving from the taut string causes problems. The same applies to densities with very high and narrow peaks. Here the values of the second derivative may differ by orders of magnitude again making the problem numerically highly unstable.\n\nIn theory it is possible to include higher derivatives in the definition of smoothness. Density estimation itself is an ill-posed problem and the higher the number of derivatives, the more ill-posed becomes the problem. For relative benign data sets such as those coming from a normal distribution then third and even fourth derivatives are possible. In general however the linear programme becomes numerically instable. At the moment we use the simplex method of Barrodale andRoberts (1978, 1980) which is a general method. It may be possible to improve numerical stability by developing a tailor-made for the sort of minimization problem we consider.\n\nAlthough we have not done so, it is possible to include boundary conditions such as f 1 = f n = 0 and f\n\n(1) 1 = f (1) n\n\n= 0. These are to some extent subjective unless there are well-grounded reasons for imposing them. In general they cause no problem as long as they are consistent with the data. If not, they will introduce numerical instabilities."
}