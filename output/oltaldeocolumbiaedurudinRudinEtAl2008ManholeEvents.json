{
    "title": "Predicting Vulnerability to Serious Manhole Events In Manhattan : A Preliminary Machine Learning Approach",
    "publication_date": "2005",
    "authors": [
        {
            "full_name": "Cynthia Rudin",
            "firstname": "Cynthia",
            "lastname": "Rudin",
            "affiliations": []
        },
        {
            "full_name": "Rebecca Passonneau",
            "firstname": "Rebecca",
            "lastname": "Passonneau",
            "affiliations": []
        },
        {
            "full_name": "Axinia Radeva",
            "firstname": "Axinia",
            "lastname": "Radeva",
            "affiliations": []
        },
        {
            "full_name": "Haimonti Dutta",
            "firstname": "Haimonti",
            "lastname": "Dutta",
            "affiliations": []
        },
        {
            "full_name": "Steve Ierome",
            "firstname": "Steve",
            "lastname": "Ierome",
            "affiliations": []
        },
        {
            "full_name": "Delfina Isaac",
            "firstname": "Delfina",
            "lastname": "Isaac",
            "affiliations": []
        },
        {
            "full_name": "Delfina Isaac Consolidated",
            "firstname": "Delfina Isaac",
            "lastname": "Consolidated",
            "affiliations": []
        }
    ],
    "abstract": "Our goal is to rank the electricity service structures (manholes and service boxes) in Manhattan according to their vulnerability to serious manhole events such as manhole fires, explosions and smoking manholes. Manhole event prediction is a new application for machine learning, and we started this project with a large amount of data from several disparate sources, most of which is extremely noisy. Our data includes Con Edison ECS (Emergency Control System) trouble tickets, which are records of past events affecting the secondary (low voltage) electric distribution system such as a manhole fire, manhole explosion, smoking manhole, no-light event, low voltage event, flickering light event, side-off partial outage, or burnout. We describe our approach to dealing with this challenging dataset, involving a combination of targeted information extraction, statistics, and feature and label development for machine learning. Our efforts have resulted in a streamlined bipartite ranking model that is aimed at predicting future manhole events. This model has demonstrated promising results on a \"blind\" prediction test. The overall goal of our system is to assist Con Edison to prioritize the inspection of over 50,000 manholes and service boxes in Manhattan and to prioritize long-term follow-up repair works.",
    "full_text": "Columbia University's Center for Computational Learning Systems is involved in a research collaboration with the Consolidated Edison Company of New York to investigate whether data mining and machine learning can help to maintain the electrical grid. There are a few hundred manhole events (including fires and explosions) in Manhattan every year, often stemming from problems in the secondary (low voltage) distribution network. The number of events is small compared to the total number of manholes and service boxes in Manhattan, which is over 50,000. As we demonstrate in this work, attempting to identify the less than 1% of structures implicated in such events was like the proverbial search for a needle in a haystack. One obstacle was that the problem was not well-defined as a statistical learning task. We were provided with extremely noisy data that was not systematically categorized, and this data could not easily be transformed into features and labels for effective supervised learning. For instance, we did not have even a first pass at an operational definition of what constituted a serious event, even though this is what we were trying to predict. In fact, there were not any guarantees made that this data could even be made useful for such a task. The Consolidated Edison experts hypothesized that there was \"knowledge\" to be gained from analyzing this data, but it was not clear exactly what mechanism to use in order to find it. They hypothesized that we could predict serious events within a relatively short time frame (a longer timeframe turned out to be more useful), and that certain types of less serious events, or features of these events, would emerge as predictive. Another major obstacle was that none of us knew which data sources would be most relevant, or what difficulties we would face in attempting to consolidate the disparate sources of information into a coherent body of data. In what follows, we present our approach to the structure ranking task, in which we order the manholes and service boxes according to their vulnerability to a serious manhole event in the future. We discuss how we arrived at a relatively streamlined, intuitive model, while integrating a large number of data sources.\n\nSeveral major logistical obstacles prevent the Con Edison data from being used directly; heavy pre-processing is essential to create any type of useful and meaningful model. Manhole event prediction is a new application for machine learning, and we are working with data that has been recorded over the course of a decade. A large proportion of our data comes from trouble tickets recorded by Con Edison dispatchers that were not intended to contain a complete description of the event. The tickets include a free text \"remarks\" field of varying length and extremely heterogeneous content, which is not amenable to statistical analysis in its raw form. The tickets may contain, for instance, a description of repair work related to the event, details about parking in the area, customer contact information, reasons for departure of repair crew personnel, and other things. Many of the tickets are difficult to read, and even to find the location and degree of seriousness for each event required significant work and domain expertise. However, our efforts to convert the free text fields into useable information proved enormously beneficial. The effectiveness of the model dramatically increased when we developed a set of rules to automatically label tickets as to whether or not they represented a serious event, using the information we generated.\n\nOur evidence indicates that even with \"clean\" data, serious manhole events are not easy to predict. Serious manhole events do not necessarily have warning signs such as recent flickering lights or low voltage in the area; however, we have found evidence that a long-term history of local events is useful for prediction. In other words, for a given manhole or service box, the set of events (manhole events, past outages and past partial outages) that have occurred within a small radial distance of the manhole may be used to help predict whether there will be a manhole event in the future. We call this the \"hotspot\" theory: there are certain local areas that have historically experienced more events than others. Such areas are more likely to be vulnerable in the future, and thus many of our features are based on event history. We include a set of statistics based on mostly-raw data that is used to motivate our further efforts in developing more specialized features and labels.\n\nAside from the new application to manhole event prediction (which is exciting in its own right), there are many reasons why our approach to this real-world machine learning task is instructive. First, the formulation of the real-world problem itself poses a challenge; it was not clear what we should predict, or over what timeframe, or what features should be developed. We formulate the problem as a supervised bipartite ranking task. Second, many of the features are built from an unusual source of data, namely the Con Edison ECS trouble tickets, which are not easy to interpret as event descriptions, even by domain experts. Third, the machine learning result must yield an interpretable model in order to have physical justification; our features are designed to yield such a model. Our approach was to perform exploratory statistical analysis iteratively with machine learning and feature pruning and feature generation. The fact that were able to produce a relatively streamlined, meaningful model out of a huge amount of extremely noisy data proved testament to this general methodology.\n\nWe will first itemize the data that Con Edison has made available to us, and describe a significant component of the pre-processing we carried out in order to make the data more useful, namely identifying the locations of reported events. The statistical evidence for the hotspot theory will then be presented; the ability to locate events in time and space was a precondition for assembling this evidence. We will then discuss the features and labels we have developed for machine learning, and the bulk of the data processing we carried out in order to construct a consolidated database of usable information. We are using a pairwise-based ranking approach to combine the features, namely the RankBoost algorithm of Freund et al. [12], to design a \"targeting model,\" which is a preliminary ranked list of structures. Sample results are presented using a visualization tool that we have created using Google Earth's satellite image [10] as a backdrop.\n\nWe have been provided seven main tables of raw data for this task, originating from several sources within Con Edison. From these raw tables we have constructed four consolidated tables: one for events (Consolidated-ECS), one for structures (Consolidated-Structures), one for cables (Consolidated-Cables), and one for inspections (Consolidated-Inspections). It is a non-trivial task even to join these tables together without losing a large percentage of records: the raw tables have numerous inconsistencies in the way structure names, etc., have been (manually) recorded.\n\nThe first important processing step to perform on this data is to geocode the addresses contained in records of past events. We describe the data in Section 2.1 and the geocoding in Section 2.2.\n\nTable 1 lists the raw data sources in the left column. Each raw data table contributes to the consolidated tables as indicated in the right column. The consolidated tables contain all pre-processed data used for generating features and labels for machine learning.\n\nBrief Description Contributes to: Structures A list of all structures (manholes and service boxes) along with their geographic coordinates.\n\nThere are 51219 structures. all consolidated tables ECS Trouble ticket database, contains records of past events. Consolidated ECS ELIN Additional details regarding manhole events. Consolidated ECS ESR/ENE Additional details regarding electrical shock and energized equipment events. Consolidated ECS Inspections Date of the inspection, type of repairs made, and types of repairs recommended for each structure. Consolidated Inspections, Consolidated Structures Property Records Electrical cable information, including the service type (which is either \"main,\" connecting two structures, or \"service,\" connecting a structure to a nearby building), location of the cable, material (copper or aluminum), number of phase and neutral cables, type of cable, and the date of installation. Consolidated Cables, Consolidated Structures Vented Manhole Cover Table Indicates which structures have \"new vented\" covers, which allow gases to escape more easily. Consolidated Structures Table 1 Con Edison raw data sources. The left column lists the raw data sources, and the right column lists the consolidated table(s) to which the raw data source contributes.\n\nThe most important of the raw tables is the Con Edison Emergency Control Systems (ECS) \"tickets\" database, which is a rich resource for data mining containing approximately 1 million tickets. Each ECS ticket (or \"trouble ticket\") is a report of an event affecting the New York City electrical distribution system as recorded by a Con Edison dispatcher. The \"front\" of each ticket has a timestamp (date and time), \"trouble type\" (type of event, such as manhole fire or smoking manhole), along with several address and cross street fields that are entered manually by the dispatcher. The \"back\" of the ticket (called the ECS-Remarks) contains a mainly free-text description of Con Edison's response and repairs made, rather than a report of the event itself. We have performed extensive processing on these tickets, including geocoding and information extraction. This processing allows us, for example, to estimate a precise location of the event (Section 2.2), the structure(s) that were involved (Section 4.3), whether the ticket was linked with another ticket (Section 4.4), and whether the event was serious (Section 5.1). The geocoding of ECS tickets is the first necessary step; there is not much hope of predicting the locations of future events if we do not know the locations of past events.\n\nFigure 2.2 shows part of the front of two tickets, one for a manhole fire event (\"MHF\") and one for a manhole explosion (\"MHO\"). In order to put such events in spatiotemporal context, it is necessary to have geographic coordinates (latitude and longitude) for each event. However, the geocoding process is made substantially more difficult by misspellings and irregularities such as missing house numbers and Con Edison-specific terminology such as \"opp#250\" (for opposite house 250), \"int\" (intersection), \"s/e/c\" (southeast corner), and ranges of house numbers or streets, for instance, \"181-182 St.\"\n\nSince location information for the ECS tickets is essential for our analysis, we designed a geocoding system specialized for ECS tickets called \"Columbia Geostan.\" This system semi-automatically standardizes the address fields, allowing us to use a standard geocoding service (such as Google Earth's free service [10]) to obtain geographic coordinates for each address. In the cases where the house number is useless or missing, we use the intersection of the street and cross street; this occurs commonly, in fact the intersection is used for approximately 15% of tickets.\n\nFirst Ticket Second Ticket Ticket Number ME11005661 ME12003775 Borough M M House Number 120 N/E/C East/West/Other Street Name E.BRAODWAY GREENWHICH Street/Avenue ST Cross Street PIKE ST CEDAR ST Actual Trouble Type EDSMHF EDSMHO Received Date and Time 2001-03-13 14:30:00-05 2002-03-29 17:40:00-05\n\nTable 2 Partial Sample ECS Ticket \"fronts.\" Note the misspellings and irregularities.\n\nA detailed description of the geocoding system is omitted, but the four main stages are: cleaning the address and cross street fields using specialized regular expressions; sending two queries to Google Earth's geocoder for each ticket (one for the street address and one for intersection with the cross street); parsing the html output; and scoring each suggestion using an independent scoring criterion. The scoring criterion takes into account the match between the query we sent and Google's response, and prefers exact street addresses to intersections. The suggestion receiving the highest score is selected as the final address for that ticket.\n\nFor the tickets above, Columbia Geostan system yields the corrected addresses: '120 E Broadway' with latitude = 40.713882 and longitude = -73.992441, and 'Cedar St. & Greenwich St.' with latitude = 40.7097100 and longitude = -74.0128400. Our system retrieves geographic coordinates for approximately 97% of the relevant Manhattan ECS tickets between 1996 and 2006.\n\nPrediction of manhole events is not an easy task. However, we have found that the long-term history of events in the local geographic area is useful for prediction. Here we will use data that is as \"raw\" as possible in order to, first, give background on the problem, and also to motivate reasons for generating the more advanced processing for the ECS Remarks in Section 4. \"Referred tickets\" (which are irrelevant tickets that simply refer to other tickets, see Section 4.4) are generally excluded from our analysis since we are attempting to count distinct real-world events rather than counting distinct tickets. Aside from the geocoding and referred ticket information, all other data for this section is raw. We first define ECS manhole events and ECS burnouts, and present some distributional characteristics. We then show that there is a statistical difference between \"true precursors\" (ECS burnouts followed by an ECS manhole event) and \"false precursors\" (burnouts that are not followed by a manhole event).\n\nAn ECS manhole event is represented by an ECS ticket with one of the following ECS trouble types: manhole explosion (MHO, MHX), manhole fire (MHF), or smoking manhole (SMH). The total number of ECS manhole events that are not referred tickets in our databank is 6,670 (approximately six to nine hundred per year). An ECS burnout is represented by an ECS ticket with one of the following trouble types: flickering lights (FLT), low voltage (LV), side off (SO, SOP, SOB), underground AC/DC burnout (UAC, UDC, ACB, DCB, WBR), no lights (NL, NLA), or fire department request (FRQ). The total number of ECS burnouts that are not referred tickets is 36,301.\n\nFigure 1 shows a month-by-month histogram between years 2000-2006 of the number of ECS burnouts (light color bars) and the number of ECS manhole events (dark color bars). This shows the number of events (and the variation in the number of events) per month. Most ECS manhole events have at least one precursor: of the 6,670 ECS manhole events, 5,458 of them were within 60 meters of at least one prior ECS burnout within the last 3 years, and 3,239 of them were within 60 meters of at least one prior ECS manhole event within the last 3 years; however, most ECS manhole events have fewer than 3 ECS burnout precursors and very few (if any) ECS manhole event precursors. Features based on a 3-year history of local events cannot hope to predict the ECS manhole events with no precursors, but it is possible for them to help to predict the events that do have precursors.\n\nEven though most ECS manhole events have few precursors, the number of precursors can be used as a possible way to predict future ECS manhole events. For instance, let us consider the prediction task of distinguishing true precursors from false precursors: given an ECS burnout, can we use the past history of events in the geographic area to predict whether an ECS manhole event will follow within 60 days? Formally, a true precursor is an ECS burnout that is followed by an ECS manhole event within 60 meters and 60 days, and it is not a referred ticket. A false precursor is an ECS burnout that is not followed by any ECS manhole events within 60 meters and 60 days, and also is not a referred ticket. We will show that true precursors behave (statistically) differently than false precursors; true precursors generally have a longer history of events in the local area.\n\nLet us consider the 3 year history of prior ECS manhole events for true precursors and for false precursors. Approximately 56.67% of true precursors have some ECS manhole event history in the past 3 years and 60 meters. On the other hand, for false precursors, only about 41.95% have had an ECS manhole event within the past 3 years and 60 meters. Now let us consider the 3 year history of prior ECS burnouts for true precursors and for false precursors. Approximately 93.65% of true precursors have some ECS burnout history in the past 3 years and 60 meters. For false precursors, only 86.28% have at least one ECS burnout in the past 3 years and 60 meters. True precursors generally have a larger past event history than false precursors. Therefore, a precursor within a \"hotspot,\" i.e., a localized region of high manhole event or burnout activity, is more likely to be followed by an ECS manhole event.\n\nTo illustrate this point in more detail, Figure 2 shows histograms of prior ECS burnouts for true precursors (top) and for false precursors (bottom). These histograms are clearly different. It is striking that there is a peak at 3 rather than at 0 in the top histogram: there are many more true precursors with 3 prior ECS burnouts than with no prior ECS burnouts. A similar observation can be made for prior ECS manhole events. Thus, true precursors have a longer event history than false precursors.\n\nThe last observation we present is an analysis of the timeframe between a true precursor and the ECS manhole event that follows. We have not found a \"crucial timeframe\" to be present; if a precursor is followed by an ECS manhole event within 60 meters and 60 days, it is not easy to predict when (within those 60 days) this event is likely to occur. Figure 3 shows a histogram of the time difference between a true precursor and the ECS manhole event that follows. It is often the case that the two events are almost simultaneous (corresponding to the peak at 0), however, there is no point at which a significant drop occurs, indicating that it is about equally likely for the event to occur after a few days as after a few weeks.\n\nTo summarize, after only the first few steps of processing these data, we have shown hope for prediction by considering the task of distinguishing true precursors from false precursors. True precursors generally have a longer history of prior ECS burnouts and ECS manhole events in the geographic area. Our observations rely on the long-term; we have found that serious manhole events are quite difficult to predict if only a short-  term history is considered. The physical processes contributing to the event (such as insulation breakdown) may take place over several years, which could be a reason for this. Furthermore, we observe that there is no short-term crucial timeframe over which manhole events can be predicted. Thus it is natural to choose a longer timeframe over which prediction is more stable; in our studies below, we choose 1 year.\n\nIn what follows, we have the advantage of using data that is less \"raw\" by incorporating the information extraction described in Section 4.\n\nThe \"back\" of each ECS ticket contains the Remarks field, which details Con Edison's response to the event including any repair work performed. Some of the lines are free text entered by a dispatcher (e.g., \"MANHOLE IS SMOLKING VERY BAD\") and other lines are automatically generated. \"05/18/00 04:10 MDELBROWN DISPATCHED BY 05198.\" Since the Remarks are extremely noisy, and since it was not clear how to begin analyzing them (and since some of us found them to be unreadable prior to the several months it took to learn the technical jargon), early iterations of the model did not use the Remarks. There are important reasons, in retrospect, why omitting the Remarks could not lead to a model with nearly the predictive power of our current model.\n\nFigure 4 shows the ticket number, total number of lines in the Remarks field, and the Remarks of a short ECS ticket. For ease of reference, we have inserted line numbers\n\nTime difference in days Fig. 3 Histogram of time difference (in days) between a true precursor and the ECS manhole event that follows. There does not seem to be any obvious crucial timeframe. In other words, aside from the many ECS burnouts occurring almost simultaneously with the ECS manhole event (the peak at zero), there is no obvious short-term (e.g., one week) timeframe for the ECS manhole event to follow. It seems only slightly more likely for the ECS manhole event to occur after, for instance, 5 days following the precursor event, as it is to occur after, for instance, 40 days.\n\nin the leftmost column. Here we see several characteristics we will discuss further below: 1) a service box (SB 325681) is mentioned twice, once on line 2, again on line 7; 2) the ticket was duplicated to another ticket on line 3, then unduplicated on line 4; 3) the ticket reports a cleared burnout (CLEARED B/O).\n\nticket: ME97105931 lines: 9 remarks: 1 01/16/97 CABLE DEPT REPORTS: W/S 8 AVE 66' S/O W.116 ST 2 SB-325681 INSTALLED NEW MAIN NEUTRALS ARE ALIVE.......DR 3 01/16/97 16:26 DUPLICATED TO ME97100590 BY 66920 4 01/16/97 19:22 UNDUPLICATED FROM ME97100590 BY 66920 5 01/16/97 15:00 MDEFELIX DISPATCHED BY 66920 6 01/16/97 15:45 MDEFELIX ARRIVED BY 66920 7 FELIX REPORTS IN SB325681 CLEARED B/O HOLE STILL NEEDS FLUSH 8 THIS JOB COMP. 9 01/16/97 19:00 MDEFELIX COMPLETE BY 66920 Fig. 4 Sample ECS Remarks.\n\nA longer sample ECS ticket appears in Figure 5, which also possesses many of these features. When two domain experts were independently asked whether this ticket represents a serious event, they disagreed. As described in Section 5.1, determining which events to consider serious turned out to be difficult for domain experts to articulate in a way we could operationalize.\n\nIt is essential to use the ECS Remarks field for our analysis, as it is the only source for certain types of information. For instance, the Remarks are the only resource for determining what the name is of the \"trouble hole\" for the event (Section 4.3), whether the ticket is related to another ticket (Section 4.4), or how to judge the seriousness of an event (Section 5.1). The ticket in Figure 4 is not related to another ticket, although it briefly was (between 16:26 and 19:22 on 01/16/97; see lines 3 and 4). It mentions a specific service box (SB 325681) as the implicit trouble hole. It also contains a variant of a phrase we have identified as indicating that Con Ed repair crews performed work on the structure (\"CLEARED B/O\"), meaning that burnouts in the structure were repaired. Many tickets report problems with access to the relevant structure, such as a car blocking a manhole, and short tickets especially may fail to report useful information pertaining to symptoms, diagnosis, or repair work.\n\nFor the trouble types we investigate here, the maximum length is 552; for other trouble types and other boroughs, the range is greater. 2. Interleaving of manually/automatically entered text: 0 to 380 lines are free text (0%-69% of the entire ticket) 3. Fragmentary and telegraphic language: OPENED M/L/S TO DROP BLDG AFTER DAMAGED WAS DONE.\n\nThere is a high frequency of standardized abbreviations (e.g., B/O for burnout), abbreviations spontaneously generated by the operator (e.g., COMP for complete, as in line 8 of Figure 4), and omission of function words (prepositions, determiners, etc.) and punctuation. 4. Specialized terminology: crab, trouble hole 5. Specialized meanings for familiar terms: bridge, legs 6. Line breaks within words: AFFECTE/ D\n\n7. Large vocabulary of approximately 91K unigram types (distinct alphanumeric sequences; not counting distinct numeric sequences, e.g., structure numbers, dates) (a) Single most frequent: by (N=554,614) (b) Many singletons (N=53,647): aarival, back-feed, loadlugger, . . . 8. Numerous variants per unigram (typically misspellings); e.g., Barricaded BARRICADE\n\nEssential though the Remarks field is to our task, mining the Remarks for useful information presents serious obstacles. The ECS remarks tickets share many features known to characterize sublanguages associated with a restricted domain (e.g., scientific, occupational) [30,21,17,14]. Because the ECS tickets are created by people working under time pressure, they have the fragmentary language and telegraphic features found to be typical of trouble reports [24,18]. They contain a very wide range of categories of information, including repair work (\"CUT,CLEARED,P/O RETIED DEFECTIVE SER-VICE LEG & RESTORED FULL POWER \"), parking restrictions in the relevant area of the structure (\"-NO PARKING 08:00 TO 18:00 MON TO FRI-\"), communications with customers (\"HAVE A BAD LANGUAGE PROBLEM...WITH CARETAKER...\") and other relevant information (e.g., \"FOUND EXTENSION CORD TIED TO PIG-TAIL AT BASE OF LAMP\"). Finally, the Remarks have a very high rate of formatting idiosycracies, word variants and especially misspellings, making them an extremely noisy source of information. Figure 6 lists eight problematic characteristics.\n\nIn other language processing tasks, length (of documents, of words) provides useful classification features. We have found the length of the ECS Remarks to be useful for assessing the seriousness of an event. If the Remarks are long, it generally indicates that substantial repair work has been performed. In order to make the document length more representative of the quantity of information, we eliminate strings of punctuation symbols (often used as line separators) and other noise, and tag each line to indicate whether it consists of \"free-text\" (text entered manually by a dispatcher) such as lines 1, 2 and 7 of Figure 4, or automatically generated lines, such as lines 3 through 6 of Figure 4.\n\nIn order to judge the vulnerability of a structure, it is important to find the events in the past for which this structure was the \"trouble hole.\" The ECS Remarks is often the only place to find such information, as in line 7 of Figure 4: FELIX REPORTS IN SB325681 CLEARED B/O HOLE STILL NEEDS FLUSH We have implemented code to extract structure information from the ECS Remarks for manholes, service boxes and vaults. This is a non-trivial information extraction task. For instance, the term \"Service Box\" is represented in at least 38 different variations across ECS Remarks, including: SB, S, S/B, S.B, S?B, S.B, S.B., SBX, S/BX, SB/X, S/XB, /SBX, S\\BX, S.BX, S.BX, S?BX, S BX, SB X, S B/X, S/B/X, S.B.X, S/BXX, S/BVX, S BOX, S/BOX, S.BOX, S,BOX, S,BOX, S-BOX, XBOX, SVBX, SERV BX, SERV-BOX, SERV.BOX, SERV,BOX, SERV/BOX, SERV/BOXC, SERVICE BOX.\n\nIt is common that a wrong structure number appears within the ticket, so to eliminate this possibility we check that the structure's physical location is at most 200 meters from Columbia Geostan's location for the ticket. Our code, which is very accurate, extracts structure information from 53.77% (33,194 out of 61,730) of the ECS tickets in our database. In other words, almost half the tickets are potential noise, since they do not tell us anything about a specific structure. If the structure is mentioned more frequently than any other structure in the text of the ECS Remarks for the event, it is likely that the structure is the trouble hole for the event. We estimate that for the tickets where we have identified a trouble hole, the accuracy of our identification exceeds 87%, based on performance against a known subset.\n\nAfter the structure information is extracted from the ECS Remarks, we determine more precisely which structure(s) are the trouble hole(s) for a given ticket by computing a \"trouble hole confidence\" score. A structure receives a point towards its trouble hole confidence if it is mentioned more frequently than any other structure in the text of the ECS Remarks, or if the structure appears in another database (e.g., ELIN, ESR/ENE, the front of the ECS ticket) as a trouble hole. If the trouble hole confidence is greater than zero, this structure is considered a trouble hole for the event.\n\nMultiple ECS tickets can refer to the same event, because there can be multiple reports made to the Con Edison call center regarding the same event. Since we wish to consider distinct events (rather than distinct tickets), it is essential to understand the relationships between these tickets. We define a \"referred ticket\" as an ECS ticket that refers to another ECS ticket (which we call the \"lead ticket\"). The lead ticket contains information describing the event, while a referred ticket simply cross references the lead ticket and contains little or no information about the event. We search for synonyms for \"refer to ticket\" such as \"duplicated to ticket\" or \"see ticket.\" A ticket might reference another ticket because the events are distinct but related, often with the same keyword phrases. We do not want to exclude related tickets. The logic to identify genuine referred tickets, such as the one in Figure 7, depends on many factors such as the length of the ticket, context, and internal corrections to lines within the ticket, etc.\n\nOur code finds that 7503 of the total 61,730 tickets are referred. Of the referred tickets we've found, 88.11% (6264 out of 7109) of them have a time difference of less than 1 day between the referred ticket and corresponding lead ticket, and 97.55% (6935 out of 7109) have a time difference of less than a week from the lead ticket.\n\nticket: ME03114094 lines: 28 remarks: 09/01/03 12:15 FDNY/242 REPORTS SMOKING MANHOLE F/O 2236 7AVE. CREW REQUESTED. =--> TD 09/01/03 17:15 DUPLICATED TO ME03114093 BY 13151 - -------------------ELIN COMPLETE-------------------------REFER TO ME03114093 FOR INFO 09/01/03 17:19 REFERRED TO: MH.INCID EDSSMH FYI BY 13151 09/01/03 22:36 COMPLETED WITH JCRF SCREEN BY 13151 09/02/03 09:16 REFERRED TO: MH.INCID EDSSMH FYI BY 22556\n\nIn document repositories of all sorts, metadata identifies characteristics of the raw documents. A library catalog is an example of metadata about books, such as author, publication date and publisher. Here we use metadata to classify ECS tickets in various categories pertaining to distinct types of information content. However, the combination of a large, domain specific vocabulary with the property that most \"words\" of any length have numerous variant spellings (See Figure 6) presents an obstacle to extracting more than a few types of metadata. 1Currently, we assign the four categories of metadata shown in Table 3. For each type of metadata, we collect a set of patterns consisting of a sequence of one or more terms that need not be contiguous. The less constrained the pattern, the more likely it will apply spuriously, thus most of the patterns we have collected are relatively specific, and must apply within a single line (60 characters). Table 3 lists one or more classes of patterns that must be present for each type of metadata to be assigned to a ticket. The presence of a specific pattern can be context dependent. For example, phrases such as FOUND MH SMKG HEAVILY or STRUCTURE SMOKING LIGHTLY ON ARRIVAL are reliable indicators that a manhole or other structure was smoking. However, collecting a set of such phrases that apply across the board to all types of tickets is not possible. To take one type of example, a smoking service box is not necessarily serious, and, for longer tickets, determining what sort of structure is smoking can require careful analysis of multiple lines in the ticket. Another type of example is that a report from a customer that a structure is smoking is not necessarily reliable: in some cases, a ticket where a customer reports a smoking manhole later indicates that a crew member on site was unable to find evidence of smoke or other serious problems. A report from a fire department crew member who is on site is more likely to be reliable. To maximize the precision of our metadata, we collected distinct sets of line-based patterns, depending on the category of tickets they apply to.\n\nTwo metadata features that provide complementary information are \"cable sizes\" and \"work was performed.\" Both provide an indicator that repair work structure took place in a structure. \"Cable sizes\" refers to physical components of the structure that connect it to the network. \"Work was performed\" refers to terms used in describing repair procedures. Typically, if cables need to be cut, a permanent or temporary shunt is installed. It is frequently the case that burnouts need to be cleared in structures with a failure of some sort. Thus there is some redundancy, but as Table 4 indicates, 40% of all tickets have one or both types of metadata, and only 13% have both.\n\nSince our goal is to create a ranked list of structures according to vulnerability to serious manhole events, we chose to formulate the learning task as a supervised bipartite ranking problem. Formally, in the bipartite ranking problem, we are given examples {x i } i=1,...,m , where x i ∈ X and labels {y i } i=1,...,m , where y i ∈ {-1, 1}. We will construct a real-valued scoring function f : X → R, and we order the examples by the values of f to produce a ranked list. To construct the scoring function, we wish to minimize the misranking error, i.e., the number of times a positive example is ranked below a negative example:\n\nThus, we construct f so that more positive examples appear towards the top of the list. The misranking error is an affine transformation of the AUC (area under the Receiver Operator Characteristic curve). The data will be made as a training set and a test set; we determine whether the model is statistically predictive by constructing the model f from the training set and using it to predict the labels on the test set.\n\nThe examples {x i } i=1,...,m here represent the structures, i.e., manholes and service boxes. In our case, m = 51219, the total number of active structures in Manhattan that have valid geographic coordinates in the Structures table. The coordinates of the feature vector x i are real-valued, x i ∈ X ⊂ R n , and they represent properties (features) of the i th structure with respect to a given timeframe.\n\nOur goal is to predict future serious events based on past history and characteristics of the structure, and the features and labels are assigned according to this goal. We will describe the features and labels more precisely below. First we must define what constitutes a \"serious\" event since these are what we are trying to predict.\n\nOur goal is to predict the types of events that domain experts would classify as serious. Initially, we used the trouble type to define which events were serious; however, we eventually realized that the trouble type was somewhat misleading. For instance, we found that the trouble type \"MHO\" (manhole open / manhole explosion) is often used for events where there is simply a problem with the a manhole cover, without a serious event. We realized a more elaborate definition would rely heavily on information extraction from the Remarks. We then developed a working operational definition of a serious event, implemented as a binary function: it assigns a \"seriousness\" score of 1 if the event is serious, 0 otherwise. We also defined a \"candidate precursor\" scoring function that sorts non-serious events into those that should be considered as candidate precursor events and those that should be excluded from the model. We refined both scoring functions by eliciting direct feedback from domain experts, and by asking experts to classify tickets, as described below. The implementation of this seriousness measure for tickets provided our model with a dramatic increase in accuracy.\n\nThe main attributes used in the two scoring functions are the trouble type, various threshholds on the number of free text lines, whether the ticket appears in ELIN, whether the ticket mentions at least one structure, whether the ticket mentions cable sizes specific to the secondary system, and whether the ticket contains serious event metadata. All tickets are either filtered out before applying the scoring functions, or are assigned to the serious category, or if not serious are assigned to the class of candidate precursor events, or are excluded from the model. We elicited feedback from domain experts on the combinations of attributes used in both scoring functions. But we found that we gained most from a qualitative analysis of a human labeling task.\n\nClassifying tickets into the three categories -a serious event, a candidate precursor event, or an event to be excluded from consideration -requires two kinds of domain expertise: an understanding of the types of events that occur within the secondary grid, and expertise in reading the ECS Remarks. To elicit expert judgements on the classification of tickets into these three categories, and to assess the feasibility of our learning task, we had two domain experts classify a carefully selected set of ECS remarks. The set consisted of 175 tickets. First we identified several categories of tickets we knew were problematic for our scoring function, such as long tickets whose trouble type was not a manhole event (not MHX, MHF, MHO). Given the high skew in the data, we first identified a target size for each set with a bias towards a disproportionately large sample of serious tickets, then we randomly selected the desired number of tickets within each category. The experts had no other information about the ticket, such as the trouble type.\n\nThe results indicate the difficulty of our learning task: the two experts had only modest levels of agreement with each other. We evaluated the agreement among the two labelers using Krippendorff's Alpha [23], an interannotator agreement coefficient similar to Cohen's Kappa [5]. It measures the degree of agreement above chance, given the distribution across the three categories. The agreement among the experts was alpha=0.47, or less than halfway between random behavior (alpha=0) and perfect agreement (alpha=1.0). Both experts classified tickets as serious about equally frequently (32 versus 34 times), but there were thirteen tickets where they disagreed on the seriousness. We had them reclassify these thirteen tickets, four of which they agreed were serious. Figure 5 illustrates one such ticket. On this second pass, the agreement was alpha=0.57.\n\nWe used the human labeled data to refine our seriousness score by noting characteristics distinguishing the three categories of tickets on this sample then confirming the reliability of the characteristics in larger samples. This led to the serious metadata described in section 4.5. To illustrate the effect of seriousness metadata, there are 651 manhole events (MHX, MHF or MHO) or smoking manholes (SMH) that our seriousness score would filter out on the basis of their short length, except for the presence of seriousness metadata for some. Fully 390 of these short potentially serious tickets have been tagged with metadata indicating they most likely do refer to serious events. This illustrates how the combination of a tight length constraint and seriousness metadata allows us to include 60% of a set of relatively short tickets that would otherwise be incorrectly labeled. Similar results hold for other subsets of tickets.\n\nWe used the 175 tickets labeled by the domain experts to evaluate the accuracy and precision of our seriousness score. Our final seriousness score had an accuracy of 91% and precision of 81%. A low baseline accuracy can be computed by taking the trouble type alone as the criterion for seriousness; if we exclude tickets the humans disagreed on, and those that they agreed should be excluded from the dataset, trouble type alone has a rather low accuracy of 38%. During development, we found that improvements in the scoring function as measured against these 175 tickets tracked improvements in the machine learning performance. In sum, subject matter experts were not able to articulate criteria that we could operationalize, but the criteria we inferred from the sorting they performed for us proved to be a powerful method for eliciting knowledge we could use in refining our labeling and feature selection.\n\nOur general approach to this problem was to alternate between feature generation with statistics and feature selection with machine learning. At each step, if the new features and labels we had generated were productive, it encouraged us to compute more statistics and features along the same direction. Also at each step, it was important to prune the feature set in order to keep producing focused and meaningful models. This pruning did not yield a significant loss in accuracy since many of the features were correlated with others. The features that we describe below are the set that we are currently using, though it is important to note that many other features were generated and tested.\n\nFor training, we aimed to predict serious events in 2005: the features were derived from data before 2005, and the labels were derived from serious events in 2005. For the testing data, we used features from before 2006 to predict serious events during 2006. For future prediction, we used features derived using our entire ECS database, including tickets through the end of 2006.\n\nThroughout the course of the project, we have developed approximately one hundred features falling into three categories: features based on ECS data (specifically, based on a history of events in the nearby area), features based on cable data, and features based on inspection history. The ECS features were the most labor intensive to derive. For instance, to derive a feature such as \"number of past serious events near the structure,\" we need the results of Columbia Geostan for the coordinates of each ticket, and furthermore we need to compute whether each ticket constitutes a serious event; considering the scoring system in Section 5.1, this latter task requires most of the processing described in Section 4. The cable features are based on the cables in the structure itself and on averages within a 60 meter radius of the structure. The inspection features deal with information from the inspection reports. We have not found these to be useful for prediction, in the sense that their individual performance is about as bad as a random guess for future prediction of serious events. We believe this is due to the fact that the inspection program started during our training period in 2004, and also that this data is very noisy.\n\nWe use subsets of the features described above to produce a scoring function, or equivalently, a ranked list of structures ordered by score. Our feature selection method is based on individual feature rankingfoot_1 , where the statistic used for feature ranking is the misranking error of the feature itself, or equivalently, the AUC (area under the Receiver Operator Characteristic curve) or Mann-Whitney U-statistic. As an automatic feature selection method is not computationally feasible due to the combinatorial expense of trying all possible subsets (and due also to our need for a meaningful subset of features), we use different subsets of features with moderate to high AUC values, where trialand-error is used to assist in selecting the subsets. This ensures that the feature set is relevant, diverse, small, and meaningful. A combination of trial-and-error with the statistics of individual features has also been used in many other real-world problems, for instance, in prediction of compiler failure in hard drives [27]. The model that has been most successful, as judged by a combination of predictive power, sparsity in the number of features, and a sufficiently meaningful collection of features, contains the following features:\n\n1. Total Mentions: The number of past events (either precursor events or serious events) in which the structure was mentioned in the Remarks. The structure is not necessarily the trouble hole for the event.\n\nThe number of events (either precursor events or serious events) in which the structure was mentioned in the Remarks within the past 3 years.\n\nThe number of past events (either precursor events or serious events) in which the structure was a trouble hole (as determined by the trouble hole confidence).\n\nThe number of events (either precursor events or serious events) in which the structure was a trouble hole within the past 3 years. 5. Number of Precursor and Serious Events in the Neighborhood: The total number of precursor and serious tickets within the past 3 years and within a 60 meter radius of the structure. 6. Number of Neutral Mains: The total number of neutral main cables in the structure.\n\nAll features are normalized to the interval [0, 1]. A Mann-Whitney U-test performed on the individual features yields p-values that are are all below 10 -4 for both training and testing. In regards to the number of neutral main cables being such a useful feature, domain experts have given us several possible explanations, for instance, mains neutrals are not insulated, and thus current can arc between the neutral cables. This arcing can cause the insulation to gradually break down on the phase cables that are bundled with the neutrals. Also, if the structure is one that needs to be \"cut and racked\", i.e., cables need to be made parallel, where the phase conductors are separated from the neutrals, it is possible that the non-insulated neutral mains cables could be touching other cables on the inside of the structure, causing insulation breakdown. Another possible explanation is that there are problems with the cable data which somehow favor the number of neutrals over the number of phase cables for prediction. Our domain experts are continually working to obtain better cable data from different sources within Con Edison.\n\nOur model aims to pinpoint precisely the trouble holes for serious events. The label y i is +1 if the structure i was the trouble hole for a serious event during the time period specified for prediction. For the training set that predicts events in 2005, the label y i is +1 if the structure i was the trouble hole for a serious event during 2005.\n\nWe have described the setup in detail, so we now discuss our modeling efforts with respect to this task.\n\nThe relevant evaluation measures for this problem are related to ranking quality. Thus, it is not surprising that the best results we have obtained so far are related to optimization of (a convex upper bound on) the misranking error. These results were obtained by minimizing the objective of Freund et al.'s RankBoost algorithm [12]. The algorithm constructs a scoring function f as a linear combination of the features where the coefficients for the linear combination are chosen to minimize:\n\nso that F (f ) ≥ Number of Misranks (f ). Our matlab [26] implementation of a coordinate descent version of RankBoost (see [29]) was used to produce the coefficients for the linear combination of features f , where the linesearch at each iteration was performed using matlab's 'fminunc' optimization subroutine. \"Pseudo\" Receiver Operator Characteristic curves for the individual features and models are shown in Figures 8 and Figure 9 for both training and testing, along with AUC values for the individual features and models. RankBoost performs well as compared to an SVM classifier [6] baseline: the RankBoost model achieved an AUC value of 0.6810 compared to the SVM's AUC of 0.6675. It is encouraging that the top 5% of structures according to the testing ranked list contains 12.58% of the structures that were trouble holes for serious events in 2006.\n\nIt is important to acknowledge that the algorithmic part of machine learning is not the key reason for this model's success, rather it is the formulation of the task, in terms of the development of meaningful features and labels. It is true that a small advantage might be gained by excessive tuning of parameters within a variety of machine learning algorithms, but this advantage is always limited by the capacity of the features and labels for representing the real-world task. In fact, in our early experiments with this data (before we had done the processing described in Sections 4), the labels alone were so noisy that even the most highly-ranked structures were really not vulnerable at all. It is clear that for many real-world machine learning problems, there is no algorithmic substitute for certain types of domain knowledge.\n\nUsing a small number of meaningful features also yields a very interpretable model that can be justified by domain experts. The features with the highest weights are features 1, 4 and 6. This model thus encourages structures that have been a trouble hole recently, have been mentioned many times as having been involved in past events, and have a lot of neutral main cables. Now that we have described our approach, we include additional domain knowledge that cannot be built directly into a statistical model.\n\nThe Con Edison data sources are very diverse. Not all of the data can be shown statistically to be predictive in the same way we have demonstrated for ECS tickets in Section 3. Some of the most important pieces of domain knowledge do not have (and in some cases cannot have) statistical predictive power and must be incorporated in other ways. We do this via a manually-designed domain knowledge model, which we call the \"vulnerability score.\" It happens that most of the structures with high vulnerability score are highly ranked by the machine learning, but there are several hundred structures whose high vulnerability scores warrant an increase in priority despite low rankings.\n\nConsider the example of cable age data. We are working with a recent snapshot in time of the full cable configuration within Manhattan. From such data, it is not possible to determine whether cable installation date is a useful factor in predicting manhole events since we cannot determine the ages of the cables at the time of failure. In fact, there is a strong correlation between brand new cables and large manhole events, but this is misleading because the cables were replaced after the event. Almost half of the cable age data is missing from the current extract, specifically pre-1988 installation dates, which account for over 49% of the cable records, and our evidence suggests that the dates that are not missing are very noisy. However, it is clear from a physical standpoint that older cable ages would justify a higher inspection priority. This type of domain knowledge is important, but we need to include it separately from the machine learning. The same problem occurs with cable material: aluminum cables are believed to cause many more problems than the (much more common) copper cables. However, it is not often possible for us to determine whether a structure contained aluminum cables at the time of the event.\n\nThe vulnerability score is calculated from a formula involving the following factors: the structure was a trouble hole in the past, the structure has needed certain types of repairs upon inspection, the structure needs a \"cut and rack\" (as determined from the Inspections data or the structure upgrade metadata from the ECS Remarks), the structure contains aluminum cable, cable of a particular size type, or a large number of cables, the structure needs a main or service cable replacement (as determined upon inspection), the structure has a solid metallic cover type, or that the structure has not been inspected.\n\nA targeting model (an overall ranked list) is constructed from the machine learning and vulnerability score in such a way that there are no low-ranked high-vulnerability structures. To achieve this, a factor proportional to the vulnerability and disproportional to the machine learning rank is added to the rank. This affects mostly high vulnerability structures towards the medium and bottom of the list, and does not heavily impact the top of the list. The vulnerability score does not necessarily make the prediction more accurate. In fact, in preliminary evaluation of 2007 data using this model on data through 2006, the machine learning performance alone was equally as good as when used in combination with the vulnerability score. However, it is important to use the vulnerability score to provide sufficient justification for being inspected and to prevent structures with high vulnerability from being overlooked.\n\nWe have already shown that our approach yields models that generalize and are thus statistically predictive. Furthermore, incorporation of the vulnerability score ensures that the structures with enough physical and intuitive justification to be recommended for inspection/repair are not overlooked. We now perform case studies on specific structures near the top of the list to give real-world intuition for our results.\n\nLet us consider first a structure within midtown Manhattan, denoted by 'SB 123' (concealing the actual structure number of the specific service box). This structure was ranked within the top 50 structures out of 51219 total structures in Manhattan for prediction of a manhole event in 2006. Here we give the evidence for the structure to be higher priority, where the processed data is found in Tables 5 and 6: -This structure was mentioned in nine tickets, where it was the trouble hole for eight of them. This is a very large number of events. Three of these nine events are smoking manholes during 2004-2005, two for which it was the trouble hole (tickets ME04106107 and ME05100004). -The last ticket that the structure was involved in during 2005 was ticket ME05120387, which noted that the structure contained aluminum service cables that were in bad condition.\n\n-Two inspections were performed in 2005-2006 (two ECS tickets with trouble type \"SIP\" and two inspection reports), both inspections noting that the structure required a cut and rack, and both noting that the structure contained aluminum service cables that needed replacement. -Considering events in the surrounding neighborhood, there was a total of 46 tickets within a 60 meter radius of the structure, which is a fairly large number. -The structure contains 58 cables, which is very high for a service box (the median number of cables in a service box is 24). -The structure has a solid metallic cover, indicating that manhole events on this structure may be more serious.\n\nOur goal was to predict manhole events in 2006. In the case of SB 123, it did have a manhole event that year; in this case, the structure was the trouble hole for a smoking manhole event (SMH) in June of 2006 that was recorded in three separate tickets (ME06109887, ME06109888, and ME06109898). In fact, within one of these tickets, the following text is found: \"750AL SERVICE IN TROUBLE,\" meaning that the aluminum service cable in the structure is believed to be the source of the problem. Thus, this is a case in which we are confident to recommend the structure for future repair work. number of free-text lines in the ECS Remarks, an indicator for the presence of metadata extracted from the Remarks, Cut and Rack (if the ticket mentions a variation of \"Cut and Rack\" is it marked with an \"x\"), Trouble Hole Confidence (\"x\" appears for tickets where this structure is the trouble hole for the event; in this case, there was only one ticket, ME04110289, where the structure was mentioned but was not the trouble hole, which is a substantial ticket with many repairs described), distance in meters between the structure coordinate from the Structures Table and Columbia Geostan coordinate of the address on the ticket, and the time difference in seconds between any referred ticket and its lead ticket (here there is only one referred ticket, ME06109888).\n\nDate T1A T1B Cut&Rack Needed Main Replacement Service Replacement 2005-05-16 F F T F T 2006-06-22 F T T F F Cables To Structure From Structure #Ph #Neu Total Material Installed Type Length M 1393 SB 123 6 4 10 CU 2004-03-19 Main 36.74 M 1393 SB 123 3 0 3 CU 2004-05-02 Main 36.74 SB 122 SB 123 3 0 3 CU 2005-01-03 Main 19.31 SB 122 SB 123 3 2 5 CU 2005-02-02 Main 19.31 SB 122 SB 123 3 2 5 CU 2005-01-03 Main 19.31 SB 123 M 1393 3 2 5 CU 2004-05-02 Main 36.74 SB 123 SB 122 3 2 5 CU 2005-02-02 Main 19.31 SB 123 M 1393 3 0 3 CU 2004-05-02 Main 36.74 SB 123 SB 122 6 1 7 CU 2004-05-02 Main 19.31 SB 123 SB 122 3 0 3 CU 2004-05-02 Main 19.31 SB 123 3 1 4 AL Prior to 1987 Serv SB 123 2 0 2 CU 1994-05-26 Serv SB 123 3 0 3 CU 1994-05-26 Serv Table 6 Top: Inspections for First Case Study. \"T1A\" indicates whether a Tier 1A repair (the most serious kind of repair) was made, by T (true) or F (false), \"T1B\" indicates whether a Tier 1B repair (also a serious repair) was suggested or made, \"Cut&Rack Needed\" indicates a suggested secondary conductor upgrade, \"Main Replacement\" indicates that main cables require replacement, and \"Service Replacement\" indicates that service cables require replacement. Bottom: Cables for First Case Study. List of all cables within the structure. Main cables go between two structures, so a from structure and to structure are listed. Service cables go directly to buildings so there is only a from structure listed. Columns are: to structure, from structure, number of phase cables, number of neutral cables, total number of cables, material (copper or aluminum), date of installation, service type (main or service) and distance in meters between the from structure and to structure for main cables.\n\nConsider a structure in the Chelsea neighborhood denoted 'SB 456,' which was also ranked within the top 50 structures for the 2006 prediction model. The processed data for this structure is found in Tables 7 and 8:\n\n-This structure was involved in a large number of events, for instance, it was the trouble hole for 4 smoking manholes in the last 3 years, and 8 manhole events in its 10 year history. In fact, this structure was the trouble hole for at least one event every year. -No inspections had been performed on the structure before 2006.\n\n-Two ECS tickets state that the structure needs a cut and rack.\n\n-The structure contains 64 cables, which is very high for a service box.\n\n-The structure has a solid metallic cover, indicating that manhole events on this structure may be more serious.\n\nIn terms of prediction for events in 2006, SB 456 had two smoking manhole events in 2006, the second of which was serious; part of the ECS remarks text reads \"SB IS WIPE OUT ALL SECTIONS WILL BE CUT FOR REPLACEMENT. We can see from the cable data in Table 8 that most of the cables were replaced couple of months after this event occurred. One can see why both the machine learning and vulnerability score have selected this structure to be high on the inspection/repair priority list for 2006. x\"), Trouble Hole Confidence (\"x\" appears for tickets where this structure is the trouble hole for the event; in this case, the structure was always the trouble hole), distance in meters between structure coordinate from the Structures Table and Columbia Geostan coordinate of the address on the ticket, and the time difference in seconds between any referred ticket and its lead ticket (none of these tickets are referred tickets).\n\nWe have been given a small subset of trouble holes for serious events (ELIN events, only fires and explosions) in 2007. The tickets have not been given to us, so we cannot judge whether these events are serious using our seriousness score. There were 44 trouble holes provided for us. Out of the top 5000 structures in our ranked list, 12 of them have had events in 2007. This means that the top 10% of the structures contained the trouble hole for over 27% of the serious events we were provided. These results are more than encouraging.\n\nA method for viewing data and results is an essential tool for gaining intuition regarding the underlying geospatial trends, judging the success of our machine learning models, determining density-of-event estimates and identifying hotspots visually. We have found it extremely helpful to view the structures with respect to the surrounding buildings and geographic area. In the following, we will describe our visualization of structure rankings, events, and cables, and then we show images generated by the visualization tool corresponding to the case studies in Section 6. A preliminary version of our visualization work was presented in [9]. The visualization tool, which is made available to users via a website that connects to our server, interfaces with our database and with Google Earth locally. The architecture of the tool is illustrated in Figure 10. In order to use the tool, users first need to specify a region to be displayed using their local version of Google Earth. After the user inputs the region into the website, the server connects to our database and retrieves the information to be displayed, specifically the tickets, structures, cables, and ranks within the region. Then, the user is provided with a file containing the display that opens in Google Earth.\n\nTickets are color-coded based on their trouble type. ECS manhole events such as explosions, fire and smoking manholes are color-coded yellow while ECS burnouts such as flickering lights, AC/DC burnouts, low voltage and side-offs are color-coded purple. Users can click on a yellow or purple dot and read the Remarks for the corresponding ticket. For tickets that are geographically close together (or at the same address), the locations of the dots have been slightly jittered to make them all visible. Figure 11 (top) shows an example of an area of Manhattan and the ECS tickets representing events within that area.\n\nOur tool is also able to visualize cable data obtained from the Consolidated-Cable database. Each main cable is plotted as a line connecting the to-structure and the from-structure for the cable. We have color-coded the cables by age (green for older cables and blue for newer cables), though as we have discussed, the cable age data is not yet reliable. The structures are colored by rank from the model, red to white from most vulnerable to least vulnerable. A demonstration is shown in Figure 11 (bottom). In general, more dark red dots appear on the avenues while the (smaller) streets seem to contain less vulnerable structures. As there are more cables along the avenues, this seems natural.\n\nRemember that the ECS ticket location, which is a street address that has been geocoded (possibly with noise), can be many meters from the trouble hole for that event. In other words, a cluster of events at one intersection may not necessarily implicate the structure at that intersection.\n\nWe now continue the case studies from Section 6 using our visualization tool. Figure 12 shows the tickets, structures colored by ranks, and cables near SB 123. This structure was a trouble hole for many events reported at a nearby intersection. Figure 13 shows the tickets, structures colored by ranks, and cables near SB 456. Note that there is noise in both the structure location and geocoding of tickets, causing a misleading separation between the structure and the events for which it was a trouble hole; in fact the tickets mention that the structure is just in front of the buildings where the events occurred.\n\nThis work differs from many other contributions in applied machine learning. We started this project with a large quantity of disparate, noisy (and at the time, unintelligible) data, with no guarantee or clear indication that this data could be useful for prediction, and a murky goal of predicting \"serious\" events. It was not clear what types of features could be developed for machine learning and over what timeframe, nor was it even clear what precisely we were trying to predict. In order to construct even a very basic first-try model required extensive pre-processing and consolidation of a large quantity of data and at each stage of development, even more processing was required. It was important that our feature development was carefully guided, in the sense that exploratory analysis did not always pay off.\n\nPart of our contribution is the methodological approach we took towards this type of problem. We alternated stages of feature generation and development with feature selection and machine learning. At each stage of processing, we expanded our feature set or tuned the features and labels, determined the most important direction in order to proceed (often with the assistance of the domain experts), and updated the model. We pruned the model to be as compact as possible at each stage, which did not significantly impact the model's accuracy and allowed us to focus on how the model should be next expanded. The amount of noise in this data is tremendous. We were able to make progress mainly because the noise in the data has some structure that domain experts could sometimes interpret (not necessarily in a way that could be directly articulated).\n\nThe fact that we were able to develop a streamlined predictive model from such data is testament to this type of strategy.\n\nLet us give some examples of this general approach by describing some of the most dramatic changes to the model. At the start of the project, after computing the statistics such as those in Section 3, we designed a basic machine learning model using the trouble type to measure seriousness (at the time believing it to be an accurate measure) and using features and labels being based on tickets within a given radius around a structure. The model we obtained was very smooth, in the sense that neighboring structures had similar ranks. After finding that some of these features performed well for this task, we extracted trouble hole information from the Remarks and computed the \"trouble hole confidence\" which allowed us to assign specific trouble holes to tickets. This was aimed at developing features and labels that could pinpoint, within the local area, exactly which structures have been involved in past events, so the model would not be so smooth. We did not know how accurate or complete the structure information in the Remarks would be. Given the variability in the way events were recorded, we did not expect this information to be so useful. We were impressed at the completeness of the tickets in terms of their trouble hole information.\n\nAnother dramatic change in the model came from the seriousness score, which also influences the features and labels for machine learning. In earlier iterations, we had taken the ECS ticket trouble types at face value rather than designing our own seriousness score, not knowing how many non-serious tickets were given a serious trouble type. When we realized this, we designed a basic seriousness score depending on the length of the ticket and some information extraction from the Remarks. Simultaneously, we prepared the labeling task described in Section 5.1, and when the results were compiled, it was clear that the trouble type (at only 38% accuracy) should be used in conjunction with the contents of the Remarks to judge seriousness. This also yielded a dramatic improvement.\n\nSo far, in our description of the model, we have described only successes. Let us be true to the nature of the problem and describe failures. We have had numerous failures in our attempts to deal with cable data. The cable data has been incredibly noisy, for instance, with past versions of the data, about half of the cables could not be associated with the structures connected to them. In our first attempt to assign tickets to structures (before the trouble holes were extracted), we tried to match the address of the ticket to the address of a service cable. We thought that, for instance, if flickering lights were reported at a particular address, we could find the structure connected via service cable to the address, and that would be the trouble hole. Unfortunately, this suggestion was completely wrong and it didn't work at all. Furthermore, the match between the addresses of the service cables and the addresses of the tickets was unbelievably noisy.\n\nRelated Work: Our colleagues at the Center for Computational Learning Systems, as far as we know, are the first to use machine learning techniques for applications to the electrical grid [15,2], though there is much precedent for other maintenance techniques and statistics for use in power engineering (e.g., monitoring the health of power transformers [31,20]). Our colleagues are concerned with the problem of ranking electrical components in the primary distribution system, specifically electrical feeders, cables and joints, according to their susceptibility to failure. This application differs dramatically from ours in that feeders have electrical monitors that provide numerical information in real time, for instance, features are based on electrical load simulations and real-time telemetry data. In contrast, our data consists mostly of historical records written mainly in free-text. It is clear when a feeder fails since an outage of that feeder occurs, whereas it is not always clear when a serious manhole event has occurred, as discussed in Section 5.1.\n\nThe problem of feeder susceptibility ranking requires attention especially to the short-term timescale before the feeder fails. Thus the problem is an online ranking problem with a short-term crucial timeframe over which warning signs lead to an event. Many other problems dealing with prediction of rare events in continuous time also rely on a shorter time scale, including seizure prediction for epileptic patients, or prediction of compiler failures in hard drives (see [27]). In contrast, our task is an offline processing problem that uses a long-term history to predict events that may happen several months later.\n\nThere are other works that adopt machine learning techniques for offline prioritization problems, for instance including the prioritization of mutations that cause disease [19], and the prioritization of geographic regions for conservation [4] and species modeling [8]. Those works implement algorithms for density modeling, and prioritization can be done according to the density. In our case, prioritization is the main goal, and thus is addressed as a bipartite ranking problem. Bipartite ranking techniques allow us to characterize the relationships between structures without first estimating the density. There has been a surge of theoretical work on bipartite ranking recently (see, for instance, [12,1,29,28]). One advantage of RankBoost [12] is that there is a theoretical equivalence between RankBoost and AdaBoost [13], its counterpart for classification, meaning that one could obtain a useful classifier from RankBoost [29]. A generalization of RankBoost is also able to concentrate at the top of a ranked list [28], and it is possible that we will use this algorithm (or other algorithms designed to optimize quality measures for information retrieval) in the future.\n\nIt is not common to use domain expertise to guide the development of labels and features for machine learning, but it is not unprecedented; for instance, Castano et al. [3] have developed an algorithm for guiding the data gathering efforts of the Mars Rover towards samples that are interesting to domain experts. However, in a wide range of natural language processing tasks using machine learning or other techniques, it is frequently necessary to consult domain experts in order to understand the vocabulary of a domain and the conventions for constructing documents and other communications in the domain [22].\n\nWe have presented a targeting model for future inspection/repair priority of electricity service structures in Manhattan. As far as we know, this work includes the first machine learning model for prediction of manhole events. Case studies and blind evaluation on 2007 events show that we are able to pinpoint vulnerable structures, and thus are able to make useful recommendations. The accuracy of our model owes greatly to the use of long-term history features, and the domain expertise incorporated into the seriousness score.\n\nWe are currently expanding this work to the other New York City boroughs, and centralizing our information extraction for the ECS tickets via the GATE platform (for General Architecture for Text Engineering) [7]. GATE provides a direct link between information we extract from the tickets, such as a structure type and number, and specific spans of text in the ECS tickets, such as a sequence of characters consisting of alphabetic, numeric and unpredictable punctuation symbols. Through an extension of such techniques, we could also normalize the spelling of other spans of text consisting of misspelled or abbreviated words. Spelling normalization of the free text would allow us to apply statistical natural language processing techniques [25] to the tickets, such as clustering or text classification.\n\nWe believe it would be possible to develop an automated method to replace our current rule-based seriousness score using clustering or text classification. In addition, we plan to investigate the possibility of using text classification methods to distinguish the \"documentary history\" of structures that are hotspots from structures that are not, using word feature selection methods such as those reviewed by Forman [11].\n\nWe are also planning to incorporate updates of the data (as it becomes available) from Con Edison. Fig. 10 Architecture of the visualization tool. First, Google Earth 4.2 (beta) is used locally to generate a Keyhole Markup Language (KML) file representing a polygon. After the KML file is provided to our website, java servlets that reside on our Apache Tomcat 5.5 server connect to our PostgreSQL 8.2 database and retrieve ECS tickets, structures, and cables within the polygon. Finally, the information is displayed using Google Earth. In order to accomplish this, a .zip file is downloaded to the default directory set by the browser. The zipped directory contains images of dots needed for the display, KML files specifying the location of each event, a KML file specifying the locations of the structures, and a KML file specifying the locations of the cables.   There is noise in the structure location and in the ticket locations, leaving a separation between the tickets and the structure. According to the ticket remarks, the structure is right in front of the building where the events occurred. Also, the visualization includes main cables that connect to vaults, though since vaults are not ranked, they are not appear, leaving the cable without an endpoint.\n\nWe have begun addressing the problem of normalizing the text so as to make it possible to extend this work in a more rigorous fashion. Within the GATE[7] framework developed at the University of Sheffield, we are using standoff annotation to identify information within the tickets.\n\nA good reference on feature ranking for feature selection is Guyon and Elisseeff's tutorial[16]."
}