{
    "title": "Interactions between facial form and facial motion during the processing of identity",
    "publication_date": "1978",
    "authors": [
        {
            "full_name": "Barbara Knappmeyer",
            "firstname": "Barbara",
            "lastname": "Knappmeyer",
            "affiliations": []
        },
        {
            "full_name": "Ian M Thornton",
            "firstname": "Ian M",
            "lastname": "Thornton",
            "affiliations": []
        },
        {
            "full_name": "Heinrich H Bülthoff",
            "firstname": "Heinrich H",
            "lastname": "Bülthoff",
            "affiliations": []
        }
    ],
    "abstract": "Previous research has shown that facial motion can carry information about age, gender, emotion and, at least to some extent, identity. With respect to identity, two important issues remain unresolved: first it is unclear to what extent purely non-rigid facial motion contributes to the processing of identity. Secondly, as most previous studies have involved techniques that severely reduced available cues to facial form (e.g., blurring or pixelation), it is not really known how, if at all, information concerning facial form and facial motion interact. By combining recent computer animation techniques with psychophysical methods, we show that during the computation of identity the human face recognition system accesses and integrates both types of information: individual non-rigid facial motion and individual facial form. This has important implications for cognitive and neural models of face perception, which currently emphasize a separation between the processing of invariant aspects (facial form) and changeable aspects (facial motion) of faces.",
    "full_text": "Traditionally, researchers have used static stimuli, such as line drawings (e.g., Davies, Ellis, & Shepherd, 1978;Tanaka & Farah, 1993), photographs (e.g., Hancock, Bruce, & Burton, 2000;Perrett et al., 1998) or laserscans of human heads (e.g., Leopold, O'Toole, Vetter, & Blanz, 2001;Troje & Bülthoff, 1996) to explore the representation and processing of faces. However, human faces are dynamic rather than static objects. As we talk, as we raise our eyebrows, as we laugh, or as we nod our heads to signal agreement, our faces move and change in subtle though significant ways, varying along both, spatial and temporal, dimensions. Although artists and impersonators have long been making use of such facial motion to mimic famous people, researchers have only recently begun to employ dynamic stimuli in studies on face processing (see O 'Toole, Roark, & Abdi, 2002 for review).\n\nProbably the most obvious and intuitive functions of facial motion are the expression of emotion (e.g., Bassili, 1978;Kamachi et al., 2001) and the facilitation of communication (Campbell, de Gelder, & de Haan, 1996). But does facial motion also contribute to other aspects of face processing? Previous research has shown that facial motion can convey information about gender (Berry, 1991;Hill & Johnston, 2001), age (Berry, 1990), and, at least to some extent, identity (e.g., Bruce & Valentine, 1988;Knight & Johnston, 1997;Lander, Christie, & Bruce, 1999;Hill & Johnston, 2001;Rosenblum et al., 2002;Thornton & Kourtzi, 2002). It is this latter function -the role of facial motion during the processing of identity -that will concern us here. The ability to measure effects of facial motion in the laboratory seems to depend on a number factors. Principle among these are the type of facial motion, the degree of familiarity with the faces and the viewing conditions. In terms of type of motion, an important distinction is that between rigid and non-rigid movements. Rigid facial motion includes translations and rotations of the whole head whereas non-rigid facial motion refers to deformations of the face, for example while talking or displaying facial expressions of emotion. To date, effects of rigid motion appear to be much more robust than those involving non-rigid motion (Hill & Johnston, 2001;Pike, Kemp, Towell, & Phillips, 1997;Schiff, Banka, & De Bordes Galdi, 1986). Familiarity with faces also seems to be an important factor. While studies with very familiar or famous faces have consistently shown facilitating effects of facial motion (e.g., Bruce & Valentine, 1988;Lander et al., 1999), it is less clear whether there are any beneficial effects of facial motion for previously unfamiliar faces. Thornton and Kourtzi (2002) for example found beneficial effects of motion in a sequential matching paradigm, whereas Christie and Bruce (1998), using old-new recognition tasks, did not. The importance of familiarity may be related to the fact that it takes time and experience to pick up which facial movement are characteristic.\n\nAnother important issue concerns the viewing conditions. While studies with full quality images have often failed to show beneficial effects of facial motion (e.g., Bruce et al., 1999;Christie & Bruce, 1998;Lander et al., 1999;Knight & Johnston, 1997), many studies which have removed or degraded facial form cues in some way, have consistently shown advantages for moving over static presentation. For example, some studies have used point-light displays (Johansson, 1973) in which facial form cues only consist of a few high-contrast dots (Bruce & Valentine, 1988;Bassili, 1978;Berry, 1991;Rosenblum et al., 2002). Other studies have involved video images, which were degraded, either by photographic negation (Knight & Johnston, 1997;Lander et al., 1999) or by thresholding the displays in some way (Lander & Bruce, 2000;Lander et al., 1999). Recently, an animated average head was used to explore the effects of individual facial motion in isolation by replacing individual facial form with that of an average face (Hill & Johnston, 2001). While such attempts to maximize the impact of motion have been successful and are clearly well motivated -that is, the ability to independently assess form and motion is very appealing -the resulting stimuli are nonetheless quite unnatural. That is, except in the laboratory, we will never be given the problem of recognizing a person purely, or even mostly, from motion (e.g., from just a few high contrast dots as in the point-light displays).\n\nThe purpose of the current work is to bring together a combination of tasks and techniques that would allow us to shed new light on the role of facial motion during the processing of identity, particularly with regard to the factors just outlined. Specifically, we made use of recent advances in computer animation and motion capture techniques to completely isolate non-rigid from rigid facial motion in an attempt to better understand the formers' contribution to identity judgements. To address the issue of familiarity, we used an incidental learning task in which exposure to both the facial form and the facial motion of a target individual was equated and controlled 1 . 1 In the current paper the term \"facial form\" includes the shape (geometry) of a face and its skin texture. More specifically, the shape refers to individual shape of the inner features of a face in a neutral position, for example the thickness of the lips or the length, width and curvature of the nose. The term \"facial motion\" is used here in the sense of \"deformations over time\". These \"deformations over time\" contain both, purely dynamic information and motion-induced spatial information. Purely dynamic information, for example, might be the speed with which a person reaches the peak of a smile or the duration the person persists in the full-smile expression. Motion-induced spatial information might be, for example, an asymmetric mouth position when a person dis-\n\nThe main focus of the current work was to investigate the interaction between facial motion and facial form rather than to explore effects of facial motion in isolation. To do this we developed a testing method for presenting dynamic stimuli in which the relevance of form cues, rather than the image quality of form cues, was systematically varied. Form cues were manipulated by applying a 3D morphing technique (Blanz & Vetter, 1999) to high-quality laserscanned heads. A commercially available animation system for faces (3Dfamous Pty. Ltd.) was used to animate these heads using facial motion patterns captured from real human actors.\n\nThe power of this technique is that it enabled us to animate any face with any motion (Fig. 1) to create situations where the two cues -form and motion -were either working in concert or conflict during the processing of identity. Thus, rather than trying to isolate form and motion, we wanted to explore how these two cues might interact.\n\nIn the experiments reported below we firstly familiarized observers with animated heads each performing the same basic sequence of non-rigid facial actions (e.g., smiling, frowning, chewing etc.), but with the slight idiosyncratic differences in the facial movements natural to different human actors. After familiarization, observers were asked to judge the identity of target faces, which were produced by morphing between the forms of the individual learned faces. The motion applied to these faces was always one of the motion patterns with which the observers were familiarized during learning (Fig. 2a). We hypothesized that an observers' ability to determine the identity of a morphed target face would be biased by the specific nonrigid facial motion.\n\nSeventy-five observers (age 17 -40 years) from the Tübingen community were paid for their participation in these experiments. They were naive as to the purpose of the research and had normal or corrected to normal vision. Twenty-nine observers (16 male / 13 female) participated in Experiment 1, twenty-seven (12 male / 15 female) in Experiment 2, thirteen (5 male / 8 female) in Experiment 3 and sixteen (7 male / 9 female) took part in the family resemblance task of Experiment 4. None of the observers participated in more than one of the experiments described below. plays a twisted smile or the position of the lip-corners at the peak of a smile.\n\nAll stimuli used in following experiments were created from 3D Cyberware T M laserscans of real human heads taken from the MPI databasefoot_0 . All manipulations of the heads, such as 3D morphing, anticaricaturing, calculating an average head, applying a generic facial outline to the faces and replacement of individual skin texture were done using software developed by Blanz and Vetter (1999). An average facial outline served as a uniform aperture for all faces to prevent observers from using the cutting line from hair removal as a feature.\n\nThe faces were animated using motion patterns captured from real human actors. Six non-professional human actors (4 male / 2 female) were trained to perform the following sequence of posed facial expressions within a fixed time interval (8 -10 s): neutral, smile, frown, surprise, chew 3 times, disgust, smile, neutral. A total of seventeen blue and green foam markers were placed on each actors' face, with markers positioned on or near the eyebrows, forehead, brow furrow, mouth, chin, nose and cheeks. Actors were filmed using a Sony digital video camera. Head position was fixed to reduce rigid head movements and the actors were able to watch their faces in a monitor as they performed the facial actions.\n\nThe motion of the markers was tracked from the 25f/s video clips using commercial tracking software by famous3D Pty. Ltd. The marker on the nose was used as a reference point to remove slight remaining head translations in the image plane. Thus the resulting motion patterns did not contain any rigid head motion, had the same overall duration and differed only in the subtle idiosyncratic way in which the actors naturally moved their faces.\n\nThese motion patterns were then applied to 3D models of human faces. Specifically, we animated 3D Cyberware T M laserscans of human heads using commercial facial animation software by famous3D Pty. Ltd. To do this, we manually defined corresponding marker positions on an average face model, which was calculated from 200 laserscans from the MPI face database (Blanz & Vetter, 1999). The motion of a marker drives its corresponding \"hot spot\" directly and animates a \"region of influence\" according to a quadratic fall-off function. The colored regions on the average face depicted in Figure 1 refer to the weights that result from overlapping regions of influence. Each red spot corresponds to a \"hot spot\" for a given marker. The blue, green and yellow regions correspond to the overlapping \"regions of influence\" with blue standing for larger and yellow for smaller weights. This map of weights is referred to as \"clustering\". The clustering is somewhat arbitrary and was optimized for most natural look of the animations. Most importantly the clustering was exactly the same for each face used in the current experiments. This was achieved by automatically transferring the clustering from the average face to every other face exploiting the dense point-to-point correspondence between all faces in the database (Blanz & Vetter, 1999). Thus the resulting animated faces differed either in their form (different laserscans), in the motion pattern (different actors) that drove the animation or both, but never in the way in which the motion was applied to the faces (clustering). This animation technique allowed us to dissociate and independently vary facial motion and facial form. The animated faces were rendered into AVI format. During the experiments video clips were displayed with 12 f/s on a CRT monitor using IRIS Mediaplayer (SGI O2). Faces were presented in frontal view and covered approximately 3.6 x 4.6 degrees of visual angle.\n\nWhile a single static picture can be enough to communicate the characteristic structural information of a face, significantly more exposure appears to be required in order to convey the characteristics of complex facial movements (Christie & Bruce, 1998). In the current studies, an incidental learning procedure was used to familiarize observers with individual faces moving in idiosyncratic ways. Observers were repeatedly shown two animated faces one after the other in a looped display along with a corresponding name label. Each face was presented for 30 seconds. Half of the observers were familiarized with face A animated with actor A's motion and face B animated with actor B's motion and the other half learned face A animated with actor B's motion and face B animated with actor A's motion. This was done to counterbalance for potential differences in the distinctiveness of the motion patterns.\n\nWhile watching these animations observers were asked to fill out a questionnaire assessing personality traits of the faces. The questions were for example \"Who looks overall happier to you?\", \"Who appears more dominant?\". Observers spent approximately 30 minutes answering these questions and they were not aware that there would be a further categorization task. After this familiarization phase observers were able to accurately (100%) label the learned faces. Our intention with this training procedure was to familiarize observers with the particular faces and facial motions They were filmed with a digital camera while performing a sequence of facial actions (smile, frown, surprise, chew etc.). The movement of the markers, which had been attached to the actor's faces, was tracked and extracted from the video using tracking software by famous3D Pty. Ltd. The facial animation software from the same company was used to apply these motion patterns to any given 3D model of a human face. To do this, marker positions, their \"hot spots\" (red) and their \"regions of influence\" (blue, green, yellow) were manually defined at first on an average face model, which is based on 200 3D Cyberware T M Laserscans of human faces from the MPI face database and then automatically transferred to different faces. The motion of a marker drives its corresponding hot spot directly and animates the region of influence according to a quadratic fall-off function. The colored regions on the average face depicted here refer to the weights that result from overlapping regions of influence (decreasing weights from blue to yellow). For the animation of the mouth, the markers were used as knots for a spline which better approximates the mouth movements than translation of each single marker. The marker on the jaw drove a rotation around an axis through the ears. That is why the \"hot spots\" for these markers look different from the others. This clustering was then automatically transferred to the faces used in the current experiments by exploiting the point-to-point correspondence between all faces in the database. Thus the resulting animated faces differed either in their form (different laserscans), in the motion pattern (different actors) that drove the animation or both, but never in the way in which the motion was applied to the faces (clustering). This animation technique allowed us to dissociate and independently vary facial motion and facial form. The pictures to the right in the figure illustrate this. They show single frames taken from different faces that were animated with the same motion pattern.These faces are different from the ones used in the experiments and they are presented in different viewpoints just to illustrate that they are 3D objects. But the faces used in the experiments were always shown in full-frontal view.\n\nwithout them trying to explicitly memorize any aspect of the display.\n\nIn Experiments 1, 2 and 3, observers were shown spatial morphs that represented a gradual transition between the learned faces (shape morphs, not motion morphs) and they were asked to identify these morphs as one of the two previously learned faces. In Experiment 4 observers were presented with 50% morphs of the learned faces with new faces. They were asked to classify these morphs into two families. To test whether the incidentally learned motion patterns influenced observers' decisions in these tasks, the target faces were always animated with either one of the two learned motion patterns.\n\nThe purpose of Experiment 1 was to establish whether incidentally learned facial motion patterns would bias observers' perception of facial identity even when relevant form cues were available. Observers were first familiarized with two animated faces using the procedure described above. The faces differed in their form as well as in the way they were moving. During testing, the form cue in the target faces was systematically varied but the motion cue was held constant. This allowed us to measure the direct trade-off between facial form and facial motion. If characteristic motion influences the processing of identity, we would expect more \"face A\" responses for a face that moves like \"face A\" than for the same face moving like \"face B\". We assumed that such biasing effects might be particularly evident when form information was ambiguous (i.e. around the 50% morph).\n\nFor the incidental learning phase two pairs of head models (2 female heads / 2 male heads) were chosen from the MPI head database. Since discriminating between just two faces is a very easy task, there was the risk of ceiling effects potentially leaving little room for any motion-induced biases. To minimize this risk and to account for the fact that our recording techniques capture facial form in more detail than facial motion we decided to slightly weaken the form cue in the training faces for this initial experiment. This was achieved by morphing the faces 20% towards the average head (anti-caricaturing) (Blanz & Vetter, 1999) and by applying an average skin texture to the faces. After this transformation the two faces looked a little more similar, but were still easily distinguishable from each other after familiarization.\n\nThe two female faces were animated with facial motion captured from two female actors and the two male faces were animated with motion recorded from two male actors. Fourteen observers (9 female / 5 male) were familiarized with the two male faces labeled \"Stefan\" and \"Lester\". The remaining fifteen observers (6 female / 9 male) were familiarized with the two female faces labeled \"Susi\" and \"Lara\". For simplicity we refer only to the male labels below as there was neither a difference in the procedure nor in the results between these two groups.\n\nAt test, observers were asked to categorize single moving faces as either \"Stefan\" or \"Lester\". The animated target faces were taken from a spatial morph sequence representing a gradual transition between \"Stefan's\" and \"Lester's\" facial forms. Eleven morphs covering the whole range between the form of \"Stefan's\" face and the form of \"Lester's\" face in 10% steps were used as target faces. For example, the 50% morph contained equal form information from \"Stefan's\" and \"Lester's\" face. To examine whether the incidentally learned motion patterns would nevertheless influence the perception of identity each target face was presented 20 times, 10 times animated with \"Stefan's\" facial motion and 10 times animated with \"Lester's\" facial motion. Observers were instructed that they would see faces, whose facial form may sometimes have been modified. Thus observers were if anything cued to pay attention to the form rather than to the motion, which is conservative with respect to our hypothesis. They were asked to indicate (via key press \"S\" or \"L\") after each target video (10 s), whether the face looked more like \"Stefan\" or more like \"Lester\". Presentation order was randomized for each observer.\n\nFigure 2b shows mean proportion of \"Stefan\" responses for each morph and each motion pattern, collapsed across observers. Data from 3 out of 29 observers were excluded from the analysis according to the following criterion: neither of the two psychometrical functions (data for each motion pattern) crossed the 50% level. This was done to avoid an overestimation of the biasing effect caused by single observers who categorized the faces solely based on the motion pattern. In terms of our hypothesis this is a conservative treatment of the data. Across almost the whole range of the morph sequence, observers were significantly more likely to respond \"Stefan\" when the morphs moved like \"Stefan\" than when exactly the same morphs moved like \"Lester\". The response differences varied between 3.5 and 16.9% with the largest difference for the 60% morph and the smallest for the 0% morph, which is identical to the learned face (a) Procedure: During an incidental learning phase observers were familiarized with two moving faces (e.g., labeled \"Stefan\" and \"Lester\"), one always animated with Motion A and the other one always animated with Motion B. The motion patterns consisted of the same sequence of facial expressions performed by different human actors. At test, each face of a morph sequence between \"Stefan's\" and \"Lester's\" facial form was combined with each of the two motion patterns, e.g., \"Stefan's\" face was animated with \"Lester's\" motion and \"Lester's\" face was animated with \"Stefan's\" motion. Observers had to decide whether these moving target faces looked more like \"Stefan\" or more like \"Lester\".\n\n(b) Results: Mean distribution (collapsed across observers and face pairs) of \"Stefan\" responses as a function of morph level. The psychometric functions reveal a biasing effect of facial motion for most morph levels. That is, when faces move with \"Stefan's\" motion, observers are more likely to respond \"Stefan\" than when exactly the same faces move with \"Lester's\" motion suggesting that observers based their identity judgments not solely on cues to individual facial form, but also on cues to individual facial motion. To assess the magnitude of the biasing effect in experiment 1, points of subjective equality (PSEs), P25 and P75 were calculated for each observer and each motion pattern by fitting cumulative gauss functions. The values (percentage) denote how much form information from \"Stefan's\" face was required in the morph to elicit 25, 50 or 75% \"Stefan\" responses. One-tailed t-tests were applied to assess the magnitude of the differences. The data show that at each level of performance less form information of \"Stefan\" is contained in the morph when it is moving with \"Stefan's motion\" than when it is moving with \"Lester's\" motion.\n\n\"Lester\". This is in line with our initial hypothesis that biasing effects of motion might be particularly pronounced when the form information is ambiguous.\n\nA standard PSE (point of subjective equality) analysis was performed to assess the magnitude of shift between the two psychometrical functions. To do this, cumulative Gaussian functions were fitted to individual observer data for each motion pattern separately 3 . 3 The PSE analysis reported here was carried out using the MATLAB statistics toolbox. Re-analyses of the data using\n\nThe PSE values were extracted from the fitted data and were submitted to a 2 (face pair at training) x 2 (form-motion combination at training) x 2 (motion pattern at test) ANOVA. The ANOVA revealed a main effect of motion (F(1,22) = 10.3, p = 0.004). That is, the morph to which observers responded equally often with \"Stefan\" and \"Lester\" needed to contain software by Wichmann and Hill (2001) especially developed for the fitting of psychometrical functions yielded the same pattern of results. less form information from \"Stefan's\" face (38.8%, SE 2.3%) when it moved with \"Stefan's\" facial motion than when it moved with \"Lester's\" facial motion (53.9%, SE 3.3%). No other main effects or interactions were found. Since there was no effect of face pair or form-motion combination, the data was collapsed across these conditions.\n\nIn addition, P25 and P75 values were calculated from the fitted data. The P75 (P25) value demarks the amount of \"Stefan's\" form in the morph required to obtain 75 % (25%) \"Stefan\" responses. One-tailed ttests were applied to evaluate whether the differences (\"Lester's\" motion -\"Stefan's\" motion) were significantly larger than zero.Table 1 summarizes the results from this analysis. At all levels, significantly less form information from \"Stefan's\" face was needed when the faces were moving with \"Stefan's\" motion than when they were moving with \"Lester's\" motion. The magnitude of the differences was very similar (14 -16%) at all three levels of performance (PSE, P25, P75).\n\nEven though the faces used in this initial experiment were manipulated to look quite similar, the psychometric functions clearly show that observers were sensitive to form information. That is, the proportion of \"Stefan\" responses was close to 100%, when the face looked exactly like \"Stefan\" and close to 0%, when the face looked exactly like \"Lester\". More interestingly, the data also show that the characteristic motion associated with an individual face during learning biased observers' identity judgments. The magnitude of this motion bias was equivalent to a 14 -16% change in relative form information. Furthermore, the bias was not only present when form information was completely ambiguous (PSE), but also when relevant form cues were available (across almost the whole range of the morph sequence). Thus, rather than exclusively relying on either facial form or on facial motion, observers seem to integrate both sources of information.\n\nThe faces used for training in the previous experiment looked very similar, since we had weakened the form cue to ensure that the task was not trivial. This manipulation might have encouraged observers to pay more attention to the facial motions than under more \"natural\" conditions. The purpose of Experiment 2 was to systematically investigate whether the motion bias observed in Experiment 1 crucially depended on these form manipulations, which included 20% anticaricaturing, i.e. morphing towards the average face, and substitution of the individual skin texture with an average skin texture. Thus we replicated Experiment 1, but now the training faces were not anti-caricatured, that is they retained their original inner features (Exp. 2a) and individual skin texture was applied to the training faces (Exp. 2b). We assumed that increasing the strength of the form cue at training would weaken the motion bias.\n\nThe same male face pair from Experiment 1 was used, but now the faces were not anti-caricatured. That is, the inner features of the faces differed in their natural way. However the skin texture was still taken from the average face. Since there was no effect of face pair in the previous experiment, all fourteen observers (6 female / 8 male) were now familiarized with the same pair of male heads. The faces were animated with the same motion sequences as before. At test observers watched each morph 5 times animated with Stefan's motion and 5 times animated with Lester's motion (in Exp. 1 it was 10 times each). Unlike in the first experiment, Otherwise the procedure and the stimuli were identical to Experiment 1.\n\nData from 1 out of the 14 observers met the exclusion criterion described in Experiment 1 and was thus not included in the analysis. Figure 3a shows the mean responses (collapsed across observers) for each morph and each motion pattern. The proportion of \"Stefan\" responses was very low for the 0% morph (\"Lester's\" face) and very high for the 100% morph (\"Stefan's\" face) suggesting that form influenced observer's decision. Furthermore, across a large portion of the morph sequence observers responded more often \"Stefan\" when the morphs were moving with \"Stefan's\" motion than when they were moving with \"Lester's\" motion. This response difference was largest for the 60% morph (32.3%), smallest for the 0% morph (3.1%)\n\nThe PSE analysis revealed significant differences at all three levels of performance (PSE, P25 and P75) (see Table 2). That is the corresponding morph contained significantly less form information from \"Stefan\" when it was moving with \"Stefan's\" motion than when it was moving with \"Lester's\" motion. The magnitude of the bias ranged from 22.7 -25.1%.\n\nAs in Experiment 1 the data show a clear trade-off between facial form and facial motion. The motion bias was even larger than in the first experiment, but the data was also slightly noisier (less observers, less repetitions per data point). Since the motion bias was  3). After the task, observers were asked to discriminate the learned motion patterns applied to an average head. They performed 87% (N = 11, SE 4.6%) correct suggesting that they were able to distinguish between the motion patterns even though the bias was smaller.  Table 3: Experiment 2b: Mean PSE, P25, P75 values collapsed across observers. In contrast to Experiment 2a the training faces also differed in their skin texture. See Table 1 for a more detailed description of the data format.\n\nstill present we conclude that the anti-caricaturing (Experiment 1) was not crucial for the effect.\n\nThe procedure and the stimuli were the same as in experiment 2a except for one further manipulation: in-dividual skin texture was applied to the training faces. In addition, after having completed the whole task (training and testing phase) observers were presented with an average face which was animated with the learned motion patterns. They were asked to decide which of the two motion patterns was used for the an-imation. This task consisted of 20 trials: 10 presentations of each motion pattern randomly presented.\n\nThe data from 1 out of 13 observers met the exclusion criterion and was thus not included in the analysis. The mean proportions of \"Stefan\" responses are shown in Figure 3b. Again across the whole range of the morph sequences there was a trend to respond \"Stefan\" more often when the morphs were moving with \"Stefan's\" motion than when they were moving with \"Lester's\" motion. The difference varied from 0% (for the 0 and the 20% morph) to 14.2% (at the 50% morph).\n\nThe PSE analysis revealed significant differences at the P25 and the PSE (Table 3), but only a marginally significant trend at the P75 level. The bias was smaller than in the previous experiments (3.9 -11.4% \"Stefan\" in morph).\n\nIn the motion discrimination task, observers were on average 87% (N = 11, SE 4.6%) correct.\n\nThe data from Experiment 2b still reflect a trade-off between facial form and facial motion, although the motion bias is much smaller, in line with our prediction concerning increased form cues. The fact that motion has any impact in this experiment is impressive given that adding individual skin texture considerably increases the useful form information in the animations. Remember that form information as we use the term here includes shape and texture information. Since observers were still able to accurately (87%) distinguish between the two different motion patterns when these were applied to an average face, suggests that the increased form information did not block the extraction of motion during learning, but rather provided a much more robust cue during testing.\n\nThe previous experiments provided convergent evidence that both facial form and facial motion seem to be integrated during the processing of identity. While it has been well established that processing of facial form is tuned to upright faces, the so-called inversion effect (e.g., Yin, 1969;Thompson, 1980), it is less clear whether this is also true for the processing of facial motion. For example, using an animated average face Hill and Johnston (2001) found that even when the animated face was turned upside down, observers were still able to identify one out three facial motions which was taken from a different human actor. However, the performance was worse than for the upright presentation. Similarly, Lander et al. (1999) reported  4) suggesting that some aspects of the spatio-temporal pattern seem to be invariant to rotations in the image plane.\n\nan advantage for moving compared with multiple static displays even when faces were presented upside down. In contrast, Knight and Johnston (1997) did not find such an advantage for inverted faces. The purpose of the following experiment was to test whether the motion bias we have observed in the previous experiments would be robust against rotation in the image plane.\n\nThe stimuli at training were exactly the same as in Experiment 2a. Observers were familiarized with the faces presented in upright orientation, but now at test the faces were presented upside down, i.e. rotated 180 degrees in the image plane.\n\nData from 1 out of the 13 (5 male / 8 female) observers were not included in the analysis according to the exclusion criterion described above. The results are summarized in Figure 4. The performance at the endpoints of the morph sequence was worse than in the previous experiments, e.g., 10 -25.8% \"Stefan\" responses for the 0% morph (= \"Lester's\" facial form) and 83.3 -93.3 for the 100% morph (= \"Stefan's\" facial form). The difference in responses varied between 9.2% (for the 90% morph) and 25.8% (for the 80% morph).\n\nThe PSE analysis revealed a significant motion bias at all three levels of performance (Table 4). The magnitude of the bias ranged between 18.7 and 25.7%.  1 for a more detailed description of the data format.\n\nThe data show that even though observers were familiarized with upright moving faces the facial motion still influenced their identity decision even when the faces were presented upside down. This is quite impressive given the subtlety of the differences in the motion patterns. Some aspects of the motion patterns seem to be rather invariant across rotations in the image plane. This is consistent with Lander et al. (1999) and Hill and Johnston (2001) who also found that some useful aspects of facial motion seem to be preserved in inverted displays. However, a direct comparison with these studies has to be handled with care due to differences in task and stimuli. The magnitude of the motion bias in the current experiment is comparable with the equivalent upright condition (Exp. 2a), but it is larger at the endpoints, probably since turning the faces upside down is a non-optimal viewing condition. The fact that the overall performance is worse is consistent with the well-known inversion effect for pictures of faces (e.g., Yin, 1969;Thompson, 1980).\n\nWhile the previous experiments provide convergent evidence that facial motion influenced observer's perception of identity even when relevant form cues were present, the particular judgment task we used might have encouraged them to adopt strategies quite different from the way they would usually process facial identity. That is, since observers were required to make very fine-grained distinctions between highly similar faces within the morph sequence, they might have focused on very subtle features in the animated faces. To reduce the likelihood of a feature based strategy, we designed a new \"family resemblance\" task, which we hoped would encourage observers to rely more on their overall impression of the faces. The target faces were now created by spatially morphing 20 new individual faces with the learned facial forms (50% morphs). Observers were instructed that they would see novel faces of people who are related to one of the two learned faces and they were asked to categorize them with respect to their \"family membership\".\n\nIn the test phase, each novel face was presented with the facial motion of each of the learned faces. Based on our previous findings, we assumed that observers' responses would reflect an integration of cues from both sources of information, facial form and facial motion.\n\nThe same male face pair as in Experiments 2a was used for the current experiment. The faces were animated with facial motions from two new male actors. The sequence of facial expressions remained the same as in the previous experiments, but the overall duration was shorter (8s). Observers were familiarized with these animated faces labelled \"Stefan\" and \"Lester\" in the same way as described above.\n\nAt test, observers were now presented with 40 novel faces created by spatially morphing a novel face from the database (20 different faces per \"family\": 10 male / 10 female) with either \"Stefan\" or \"Lester\" (50% morphs). Thus, faces within a \"family\" shared some common geometry but were nonetheless considerably more distinct from each other than the morphed faces used in the previous experiments. Examples of these stimuli are shown in Figure 5a. Observers were instructed that they would see novel faces of people who were related to \"Stefan\" or \"Lester\". On each trial, their task was to categorize a single novel face as either \"a member of Stefan's family\" or \"a member of Lester's family\". Each face was presented twice, once moving with \"Lester's\" facial motion and once moving with \"Stefan's\" facial motion. Response was given via keypress (\"S\" or \"L\").\n\nFigure 5b shows the mean percentage (collapsed across observers) of correct responses for each family and each motion pattern. Responses were defined as \"correct\" when they were consistent with the form cue in the stimulus, e.g., the response \"Stefan's family\" to a face that was morphed with \"Stefan\" counted as a correct response irrespective of the motion pattern that was used to animate the face. A 2 (form motion combination at training) x 2 (form cue) x 2 (motion cue) ANOVA revealed a significant interaction between form and motion (F(1,15) = 7.6, p = 0.02). When the faces were animated with the con-(a) Experiment 4: Procedure. At training, observers were again familiarized with two animated faces (labeled \"Stefan\" and \"Lester\"). But at test they were now shown 40 new moving faces and they were asked to decide whether these faces belong to members of either \"Stefan's\" or \"Lester's family\". Each \"family\" consisted of 20 novel faces (10 male / 10 female) morphed halfway towards \"Stefan\" or \"Lester\" (50% morphs). Thus faces within one \"family\" resembled each other with respect to their form. Each face was presented twice: once animated with \"Lester's\" motion and once animated with \"Stefan's\" motion.\n\n(b) Experiment 4: Results. Mean percent correct (collapsed across observers) defined on the basis of the form cue, e.g., response \"Stefan's family\" counts as correct for a face that was morphed with \"Stefan\". Error bar represent standard errors. Performance was above chance in all conditions, suggesting that observers used the facial form cue in this task. However, observers also used the facial motion cue to make their decision, as is revealed by a strong interaction between form and motion. That is, when the motion cue was consistent with the form cue, performance was considerably more accurate than when it was inconsistent. Facial motion \"Lester\" Facial motion \"Stefan\" Facial form % Correct SE t(15) p % Correct SE t(15) p 50% \"Lester\" + 50% new 75.3 3.6 7.1 p < .001 60.3 4.6 2.3 p = .038 50% \"Stefan\" + 50% new 61.6 4.7 2.5 p = .026 77.2 3.3 8.3 p < .001 Table 5: Experiment 4: Family resemblance task. Percent correct responses (i.e. response \"Lester's family\" when facial form was morphed with \"Lester\") averaged across observers (n = 16). Standard errors, t-values to asses whether performance was above chance level (50%).\n\nsistent motion, i.e. \"Lester's family\" with \"Lester's\" motion and \"Stefan's family\" with \"Stefan's\" motion, observers were considerably more accurate, than when exactly the same faces were animated with the inconsistent motion. (Table 5) There were no other main effects or interactions. Finally, t-tests (p < 0.05) reveal that observers were consistently above chance (50%) in all conditions.\n\nUsing a more natural task, these data again indicate that cues to both, facial form and facial motion, contribute to the processing of identity. Given the definition of \"correct response\" we used in this experiment, an ideal observer who relied solely on form information would perform with 100% correct responses irrespective of the facial motion pattern that was used to animate the faces. In contrast, an ideal observer who relied exclusively on the motion cue would perform with 100% correct responses, when the facial motion is consistent with the facial form, but with 0% correct response when form and motion cue were inconsistent. Finally, an ideal observer, who integrated form and motion cues with equal weights, would perform at 100%, when form and motion cue are consistent and at chance (50%), when form and motion cue are inconsistent. The data clearly does not conform to either of the first two cases. Rather, observers seem to base their decision on some combination of form and motion, a pattern more similar to the final prediction, and one that is also in line with the results from the previous three experiments reported here. The fact that performance was above chance in all conditions, even in the inconsistent condition, may reflect a slight advantage of the form over the motion cue.\n\nIn the four experiments reported in this paper we found consistent evidence that non-rigid facial motion biased the perception of identity. Furthermore, by employing a variety of new tasks and new techniques, we have provided the strongest evidence to date that information provided by facial form and facial motion seem to be integrated during the processing of identity, rather than operating as independent cues. In the first three experiments we measured responses to morphed faces that represented a continuous transition between the forms of two learned faces. In Experiment 1, there was a consistent shift between the psychometrical functions measured for the different learned facial motions that were applied to these morphs, suggesting that facial motion biased observers' identity decisions. This shift was observable across almost the whole range of the morph sequence, even when relevant form information was available. While the learned faces in this initial experiment looked very similar, Experiment 2 replicated these findings with faces that were considerably more distinct. Although the motion bias was slightly weaker in this experiment, observers were still able to reliably distinguish between facial motions when they were presented on an average face, suggesting that the individual motion patterns had been extracted. In Experiment 3, we found that a motion bias could still be observed when target faces were rotated 180 • in the picture plane, suggesting that some aspect of the spatio-temporal pattern was rotation-invariant. Finally, in Experiment 4, a family resemblance task was used to demonstrate that the observed motion bias generalized to tasks involving a larger variety of facial forms. Again, the results suggested that observers integrated both facial form and facial motion during the processing of identity.\n\nThe finding that facial motion biased observers' identity decisions is consistent with previous research showing that such motion patterns can carry information about identity (e.g., Bruce & Valentine, 1988;Knight & Johnston, 1997;Lander et al., 1999;Rosenblum et al., 2002;Thornton & Kourtzi, 2002). Recently, however, Hill and Johnston (2001), using a very similar technique, found only weak effects of purely non-rigid facial motion compared to robust effects of rigid head motion. The stronger effects of non-rigid motion observed in the current work may reflect subtle differences in either the task or the stimuli used in these studies. For example, we used expressive, rather than speech-related, movements and we introduced an incidental learning phase to familiarize observers with specific motion patterns. Familiarity seems to be one factor that has a strong impact on the detection of motion effects (e.g., O'Toole et al., 2002 for a review).\n\nThe presence of robust non-rigid motion effects in the current work is particularly interesting as the observers had access to both facial form and facial motion cues at learning and test. That is, in contrast to previous research (e.g., Bassili, 1978;Bruce & Valentine, 1988;Knight & Johnston, 1997;Lander et al., 1999;Hill & Johnston, 2001;Rosenblum et al., 2002), which focused on reducing or eliminating the form cue to investigate effects of motion in isolation, the current study explored the interaction between facial form and facial motion. While investigating effects of isolated or enhanced motion may be very useful in order to more fully understand its potential impact on face processing, such isolated cue may rarely be used outside of the laboratory.\n\nWhile the current animation and morphing techniques may be open to similar concerns regarding ecological validity, our experimental situation was more natural in the sense that the two major sources of information -form and motion -were available in stimuli with high image quality. The fact that motion still biased observers' judgments under these conditions strongly suggests that facial movements are not redundant cues to identity, as has previously been suggested (e.g., Bruce & Valentine, 1988;Knight & Johnston, 1997). More specifically, the results from Experiment 4 suggest that facial form and facial motion might be integrated with almost equal weights in decision about identity, with only a slight advantage for facial form. Clearly, however, more research is needed to determine the exact weights and functions which are applied during the integration of these two cues.\n\nMore generally, the motion capture and animation techniques employed in the current work open the door for the systematic study of form/motion interactions across a whole range of topics, previously only been explored with static images of faces. For example, by using dynamic morphing and caricaturing methods (Blanz & Vetter, 1999;Giese & Poggio, 2000), it is possible to investigate the influence of motion on facial caricature (e.g., Rhodes, Brennan, & Carey, 1987;Giese, Knappmeyer, & Bülthoff, 2002;Hill et al., 2002) and viewpoint effects (Troje & Kersten, 1999;Watson, Hill, & Johnston, 2002). As already mentioned, the techniques and tasks described in this paper also allow researchers to disentangle rigid from nonrigid facial motion and would make it feasible to systematically study the role of motion during learning, i.e. when unfamiliar faces become familiar. While the current paper has been exclusively concerned with the contribution of facial motion to the processing of identity, similar techniques can applied to the study of other aspects of face processing, for example facial attractiveness (Knappmeyer, Thornton, Etcoff, & Bülthoff, 2002).\n\nFinally, we believe the current findings have important implications for cognitive and neural models of face perception (e.g., Bruce & Young, 1986;Haxby, Hoffman, & Gobbini, 2000). Such models have typically stressed a separation of the invariant aspects (facial form) and changeable aspects (facial motion) of faces into independent processing systems and have assigned decisions about facial identity firmly with the former system. While earlier studies have shown that either of these systems can compute identity in isolation, here we have shown that when operating together, a compromise is reached with responses reflecting input from both types of information. We suggest that such a compromise, which is consistent with a growing body of behavioural (Stone & Harper, 1999;Lorenceau & Alais, 2001;Wallis & Bülthoff, 2001;Bernstein & Cooper, 1997) and neural (Haxby et al., 2000;Decety & Grezes, 1999;Bradley, Chang, & Andersen, 1998;Kourtzi, Bülthoff, Erb, & Grodd, 2002;Oram & Perrett, 1994) evidence suggesting strong form/motion interactions during the recognition of many classes of objects, clearly needs to be reflected in our up-to-date models of face processing (O'Toole et al., 2002).\n\nhttp://faces.kyb.tuebingen.mpg.de\n\nThe authors would like to thank Fiona Newell, Chris Christou and Zoe Kourtzi for comments on an earlier draft of the manuscript; Mario Kleiner, Volker Blanz and Curzio Basso for helping to produce the stimuli and our colleagues, who allowed us to record their facial motions."
}