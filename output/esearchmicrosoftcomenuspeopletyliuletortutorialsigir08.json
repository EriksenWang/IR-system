{
    "title": "Ranking inside Search Engine Ranking model",
    "publication_date": "2008-07-20",
    "authors": [],
    "abstract": "N/A",
    "full_text": "• Precision at position n for query q:\n\n• Average precision for query q:\n\n• MAP: averaged over all queries.\n\n• For query q, given the predicted ranked list and the ground-truth ranked list, the correlation between the two lists is used as the measure.\n\n-For example, weighted Kendall's τ can be used.\n\n• Averaged over all queries.\n\n7/20/2008 Tie-Yan Liu @ Tutorial at SIGIR 2008 22 • Beyond conventional machine learning -Ordinal regression (a pointwise approach).\n\n-Preference learning (a pairwise approach).\n\n-Listwise ranking (a listwise approach).\n\nLeast\n\nOverview Conventional ML Approach Ordinal Regression: A Pointwise Approach Preference Learning: A Pairwise Approach Listwise Ranking: A Listwise Approach Conventional ML Approach • Directly apply regression or classification methods to solve the problem of ranking -Regard binary judgments or multi-valued discrete as \"nonordered\" categories, or real values. -Although ground truths are neither \"non-ordered\" categories nor real values . 7/20/2008 Tie-Yan Liu @ Tutorial at SIGIR 2008 35\n\nLeast Square Retrieval Function (N. Fuhr, TOIS 1989) • Relevance judgment for a query-document pair is represented by a vector:\n\n-For binary judgment: y=(1, 0) or (0, 1) -For multi-valued discrete: y=(1,0,0), (0,1,0), or (0,0,1)\n\n• Use polynomial function as the ranking function f(x).\n\n• Use least square error (LSE) method to learn the regression function\n\n7/20/2008 Tie-Yan Liu @ Tutorial at SIGIR 2008 36\n\nRegression Tree for Ordinal Class Prediction (S. Kramer, Fundamenta Informaticae, 2000) • Simply run a regression algorithm like S-CART as if the ordinal classes were real values. • Post-process the regression tree in order to translate real-valued predictions into discrete class labels.\n\n-Round to the nearest of the ordinal classes.\n\n-Modify the way S-CART computes the target values during tree construction (e.g., always predict integer values).\n\n7/20/2008 Tie-Yan Liu @ Tutorial at SIGIR 2008 37 Discriminative Model for IR (R. Nallapati, SIGIR 2004) • Treat relevant documents as positive examples, while irrelevant documents as negative examples. • Learning algorithms used: -Max Entropy -Support Vector Machines 7/20/2008 Tie-Yan Liu @ Tutorial at SIGIR 2008 38 Learning to Rank Algorithms : Conventional Machine Learning Approach\n\nSubset Ranking using Regression (D. Cossock and T. Zhang, COLT 2006) • Ranked based loss in terms of DCG is upper bounded by regression error.\n\n• Regression is used to learn the ranking function.\n\n7/20/2008 Tie-Yan Liu @ Tutorial at SIGIR 2008 39\n\nLearning to Rank Algorithms : Conventional Machine Learning Approach Li, C. Burges, et al. NIPS 2007) • Loss in terms of DCG is upper bounded by multi-class classification error.\n\n• Multi-class classification:\n\n• Multiple ordinal classification:\n\n• Regression:\n\n7/20/2008 Tie-Yan Liu @ Tutorial at SIGIR 2008 40\n\n• Cannot handle ground truths of pairwise preference and partial (total) orders. • Absolute and independent relevance is assumed.\n\n-Might not be true in ranking.\n\n-Relevance is relative and defined only among documents associated with the same query: an irrelevant document for a popular query might have higher term frequency than a relevant document for a rare query.\n\n• Unique properties of ranking (and particularly, ranking for IR) have not been well considered.\n\n7/20/2008 Tie-Yan Liu @ Tutorial at SIGIR 2008 41 Unique Properties of Ranking for IR • Query-level -Query determines the logical structure of the ranking problem in IR.\n\n-Relevance is defined on documents associated with the same query.\n\n-IR evaluation measures are computed at query level.\n\n• Rank-based -Learning objective is non-smooth and non-differentiable.\n\n-Top-ranked objects are more important.\n\n-Relative order is important: no need to predict accurate category, or value of f(x).\n\n7/20/2008 Tie-Yan Liu @ Tutorial at SIGIR 2008 42\n\nBridge the Gap\n\n• Go beyond conventional ML methods -Ordinal regression (a pointwise approach)\n\n• Target the ground truth of multi-valued discrete.\n\n-Preference learning (a pairwise approach)\n\n• Target the ground truth of pairwise preference.\n\n• Also compatible with that of multi-valued discrete.\n\n-Listwise ranking (a listwise approach)\n\n• Target the ground truth of partial / total order.\n\n• Also compatible with other types of ground truths.\n\n7/20/2008 Tie-Yan Liu @ Tutorial at SIGIR 2008 43 Ordinal Regression: A Pointwise Approach • Input space -Features of a single document (w.r.t. a query): • Output space -Ordered categories: 7/20/2008 Tie-Yan Liu @ Tutorial at SIGIR 2008 45\n\n) ( 1) , ( ),..., , ( ), , (\n\nOrdinal Regression vs. Regression/Classification\n\n• Regression: Real values • Classification: Non-ordered categories • Ordinal regression: Discrete values / Ordered categories • Ordinal regression can be regarded as something between regression and classification. 7/20/2008 Tie-Yan Liu @ Tutorial at SIGIR 2008 46 Pranking with Ranking (K. Krammer and Y. Singer, NIPS 2002) 7/20/2008 Tie-Yan Liu @ Tutorial at SIGIR 2008 47\n\n• Project\n\nLearning to Rank Algorithms : Ordinal Regression • Approximate the Bayes point by averaging the weights and thresholds associated with these Prank models.\n\n-Bayes point is the single hypothesis chosen from a fixed class of hypotheses that achieves the minimum probability of error. Learning to Rank Algorithms : Ordinal Regression\n\n• Sum of margins strategy -Margin defined as the sum of margins of all sub problems 7/20/2008 Tie-Yan Liu @ Tutorial at SIGIR 2008 55\n\nLearning to Rank Algorithms : Ordinal Regression\n\nCategory 1 Category 2 Category 3\n\nConstraint Ordinal Regression\n\n(W. Chu, S. Keerthi, ICML 2005) • Fix the problem of disordered threshold -Explicit constraints on thresholds -Implicit constraints on thresholds 7/20/2008 Tie-Yan Liu @ Tutorial at SIGIR 2008 56\n\nLearning to Rank Algorithms : Ordinal Regression\n\nexamples to guarantee implicit constraints on thresholds.\n\n• Only slightly different from conventional machine learning methods. • Also cannot handle ground truths of pairwise preference and partial (total) orders. • Some of the unique properties of ranking for IR have not been considered either.\n\n7/20/2008 Tie-Yan Liu @ Tutorial at SIGIR 2008 57\n\nPreference Learning: A Pairwise Approach\n\n• Input space -Document pairs: • Output space -Preference: 7/20/2008 Tie-Yan Liu @ Tutorial at SIGIR 2008 59\n\nLearning to Retrieve Information Learning to Rank Algorithms: Preference Learning\n\nLearning to Order Things Learning to Rank Algorithms: Preference Learning\n\nLearning to Order Things\n\n• From pairwise preference to total order --Proven: the optimal total order construction is NP hard.\n\n-A greedy ordering algorithm -It has been proven that the agreement for the approximation algorithm is at least half the agreement for the optimal algorithm.\n\n7/20/2008 Tie-Yan Liu @ Tutorial at SIGIR 2008 62\n\nLearning to Rank Algorithms: Preference Learning\n\n• Modeled probability:\n\n--Where\n\n• Cross entropy as the loss function\n\n• Use Neural Network as model, and gradient descent as algorithm, to optimize the cross-entropy loss. RankNet (C. Burges, T. Shaked, et al. ICML 2005) 7/20/2008 63 Tie-Yan Liu @ Tutorial at SIGIR 2008 Learning to Rank Algorithms: Preference Learning ) exp( 1 ) exp( ) ( uv uv v u uv o o d d P P     uv P ) ( ) ( v u uv x f x f o   ) 1 log( ) 1 ( log uv uv uv uv uv P P P P L      FRank (M. Tsai, T. Liu, et al. SIGIR 2007) • Similar setting to that of RankNet • New loss function: fidelity 7/20/2008 64 Tie-Yan Liu @ Tutorial at SIGIR 2008 Learning to Rank Algorithms: Preference Learning ) ) 1 ( ) 1 ( ( 1 uv uv uv uv uv P P P P L        FRank (2) RankNet (Cross entropy loss)  • Non-zero minimum loss • Unbounded  • Convex FRank (Fidelity loss)  • Zero minimum loss • Bounded within [0,1]  • Non-convex 7/20/2008 Tie-Yan Liu @ Tutorial at SIGIR 2008 65 RankBoost (Y. Freund, R. Iyer, et al. JMLR 2003) • Given: initial distribution D 1 • For t=1,…, T: -Train weak learner using distribution D t -Get weak ranker h t : X  R -Choose -Update: where • Output the final ranking: t R   7/20/2008 66 Tie-Yan Liu @ Tutorial at SIGIR 2008 Use AdaBoost to perform pairwise classification Learning to Rank Algorithms: Preference Learning\n\nFrom Binary to Multi-valued\n\n• When constructing document pairs, category information has been lost: different pairs of labels are treated identically.\n\n-Can we maintain more information about ordered category? -Can we use different learner for different kinds of pairs?\n\n• Solutions • If we have K categories, then will have K(K-1)/2 pairwise preferences between two labels.\n\n-Learn a model f uv for the pair of label r u and label r v , using any previous method (for example Ranking SVM).\n\n• Testing\n\n• Progress has been made as compared with the pointwise approach:\n\n-No longer assuming absolute relevance.\n\n-Using pairwise relationship to represent relative ranking.\n\n• However,\n\n-Cannot handle ground truth in terms of partial / total orders. -Some unique properties of ranking in IR have not been considered yet.\n\n7/20/2008 Tie-Yan Liu @ Tutorial at SIGIR 2008 71\n\nLearning to Rank Algorithms: Preference Learning\n\nCase Study: Pairwise vs. Query-level\n\n• Number of document pairs varies according to queries.\n\n-Two queries in total -Same error in terms of pairwise classification 780/790 = 98.73%.\n\n-Different errors in terms of query-level evaluation 99% vs. 50%.\n\n7/20/2008 Tie-Yan Liu @ Tutorial at SIGIR 2008 72 Distribution of Pair Numbers in Real Dataset 7/20/2008 Tie-Yan Liu @ Tutorial at SIGIR 2008 73 Web Data Learning to Rank Algorithms: Preference Learning\n\nThe more the number varies, the more pairwise is different from query-level.\n\nAs a Result, …\n\n• It is not clear how pairwise loss correlates with query-level IR evaluation measures. Learning to Rank Algorithms: Preference Learning\n\nSignificant improvement has been observed by using the query-level normalizer.\n\nTheoretical Explanation (Y. Lan, T. Liu, et al. ICML 2008) • Explanation on the empirical improvement of IRSVM over Ranking SVM:\n\nThe generalization ability of IRSVM is much better that of Ranking SVM.\n\n7/20/2008 Tie-Yan Liu @ Tutorial at SIGIR 2008 76 Learning to Rank Algorithms: Preference Learning Generalization in Learning to Rank for IR • Theories on regression and classification have been used to analyze the generalization ability of pointwise or pairwise approaches to learning to rank. -Pointwise: classification / regression -Pairwise : pairwise classification / U-statistics • However, are these expected risks really what one cares about in real applications? 7/20/2008 Tie-Yan Liu @ Tutorial at SIGIR 2008 77\n\nQuery-level Generalization in Learning to Rank for IR\n\n• Considering that IR evaluations are conducted at the query level, the generalization ability should also be analyzed at the query level.\n\n• Existing work can only give the generalization ability at document level or document pair level, which is not consistent with the evaluation in information retrieval.\n\n• New theory needs to be developed.\n\n7/20/2008 Tie-Yan Liu @ Tutorial at SIGIR 2008 78\n\nTwo-Layer Probabilistic Framework for Ranking\n\n• Query -Associate q stands for query, which is viewed as a random variable sampled from query space Q according to an unknown probability distribution P Q . -stands for the associate of the query and its ground-truth, which is viewed as a random variable sampled from space according to an unknown probability distribution D q .\n\n7/20/2008 Tie-Yan Liu @ Tutorial at SIGIR 2008 79 Associates in Different Approaches Approach w (q) g(w (q) ) Pointwise Document Relevance score Class label Pairwise Document pair Partial order … … … 7/20/2008 Tie-Yan Liu @ Tutorial at SIGIR 2008 80 Training Data 7/20/2008 Tie-Yan Liu @ Tutorial at SIGIR 2008 82 i.i.d Learning to Rank Algorithms: Preference Learning Q N P q q ~ i.i.d. ,..., 1 )} ( ),..., {( 1 1 N N ,S q ,S q • Expected Query-Level Loss • Empirical Query Level Loss • Expected Query-Level Risk • Empirical Query-Level Risk 7/20/2008 Tie-Yan Liu @ Tutorial at SIGIR 2008 83 Query-level Loss and Risk Learning to Rank Algorithms: Preference Learning\n\n• The goal of Learning to Rank should be to minimize the expected query-level risk . • However, as the distribution is unknown, one minimizes the empirical query-level risk . • The generalization in learning to rank for IR is concerned with the bound of the difference between the expected and empirical query-level risks . • When N tends to infinity, the upper bound of IRSVM will tends to zero and the convergent rate is\n\n• For Ranking SVM, as N tends to infinity, the upper bound may be a constant: O(1).\n\n• In other words, for Ranking SVM, with infinite number of training queries, there is still a gap between the query-level empirical and query-level expected risks.\n\n7/20/2008 Tie-Yan Liu @ Tutorial at SIGIR 2008 89\n\nWhy Listwise Approach?\n\n• By treating the list of documents associated with the same query as a learning instance, one can naturally obtain -The rank (position) information, -The query-level information.\n\n• And thus can embed more unique properties of ranking for IR in the learning process.\n\n7/20/2008 Tie-Yan Liu @ Tutorial at SIGIR 2008 Listwise Ranking: A Listwise Approach • Input space -Document collection w.r.t. a query • Output space -Permutation of these documents: 7/20/2008 Tie-Yan Liu @ Tutorial at SIGIR 2008 99\n\nListwise Ranking: Two Major Branches\n\n• Direct optimization of IR evaluation measures -Try to optimize rank-based IR evaluation measures, or at least something correlated to the measures. • Listwise loss minimization -Minimize a loss function defined on permutations (ranked document lists). -The loss function is designed by considering unique properties of ranking for IR. 7/20/2008 Tie-Yan Liu @ Tutorial at SIGIR 2008 100 Direct Optimization of IR Measures • It is natural to directly optimize what is used to evaluate the ranking results. • However, it is non-trivial. -Evaluation measures such as NDCG are non-smooth and non-differentiable since they depend on the ranks. (S. Robertson and H. Zaragoza, Information Retrieval 2007; J. Xu, T. Liu, et al. SIGIR 2008). -It is challenging to optimize such objective functions, since most optimization techniques in the literature were developed to handle smooth and differentiable cases. 101 7/20/2008 Tie-Yan Liu @ Tutorial at SIGIR 2008\n\n• Smoothen the objective -Optimize a smooth and differentiable upper bound of the evaluation measure (SVM-MAP) -Soften (approximate) the evaluation measure so as to make it smooth and differentiable (SoftRank)\n\n• Optimize the non-smooth objective directly\n\n-The score of a true ranking should be higher than those of incorrect rankings.\n\nTie-Yan Liu @ Tutorial at SIGIR 2008 103\n\n• For Average Precision, the true ranking is a ranking where the relevant documents are all ranked in the front, e.g.,\n\n• An incorrect ranking would be any other ranking, e.g.,\n\n• Exponential number of rankings, thus an exponential number of constraints!\n\n7/20/2008 104 Tie-Yan Liu @ Tutorial at SIGIR 2008\n\n• STEP 1: Perform optimization on only current working set of constraints.\n\n• STEP 2: Use the model learned in STEP 1 to find the most violated constraint from the exponential set of constraints.\n\n• STEP 3: If the constraint returned in STEP 2 is more violated than the most violated constraint in the working set, add it to the working set. • If the relevance at each position is fixed, Δ(y (i) ,y') will be the same. But if the documents are sorted by their scores in descending order, the second term will be maximized.\n\n-Sort the relevant and irrelevant documents and form a perfect ranking.\n\n-Start with the perfect ranking and swap two adjacent relevant and irrelevant documents, so as to find the optimal interleaving of the two sorted lists. -For real cases, the complexity is O(MlogM).\n\nTie-Yan Liu @ Tutorial at SIGIR 2008 106 Taylor, J. Guiver, et al. LR4IR 2007/WSDM 2008) • Key Idea:\n\n-Avoid sorting by treating scores as random variables\n\nEmphasize those hard queries by setting their distributions.\n\nWeak learner that can rank important queries more correctly will have larger weight α\n\nTheoretical Analysis\n\n• Training error will be continuously reduced during learning phase as long as <1 To achieve WTA=1\n\nLearning to Rank Algorithms: Direct Optimization of IR Measure\n\n• Instead of explicitly defining the loss function, directly define the gradient:\n\n• A lambda which was shown to effectively optimize NDCG.\n\n7/20/2008 Tie-Yan Liu @ Tutorial at SIGIR 2008 116 ) , ,..., , ( 1 1 n n j j y s y s s L      S: score; y: label. Learning to Rank Algorithms: Direct Optimization of IR Measure Listwise Loss Minimization\n\n• Defining listwise loss functions based on the understanding on the unique properties of ranking for IR.\n\n-Hopefully correlated with all IR evaluation measures .\n\n• Representative Algorithms -Gradient descent for optimization of the loss.\n\n• A Simple Example:\n\n• Question: which function is closer to ground truth?\n\n-Based on pointwise similarity: sim(f,g) < sim (g,h).\n\n-Based on pairwise similarity: sim(f,g) = sim(g,h) -Based on cosine similarity between score vectors? f: <3,0,1> g:<6,4,3> h:<4,6,3>\n\n• However, according to NDCG or RC, f should be closer to g! Plackett Luce Model:\n\nDefining Permutation Probability\n\n• Probability of a permutation 𝜋 is defined as  • Loss function = KL-divergence between two permutation probability distributions (φ = exp)\n\n• Never assume ground truth to be scores.\n\n• Given the scores outputted by the ranking model, compute the likelihood of a ground truth permutation, and maximize it to learn the model parameter.\n\n• The complexity is reduced from O(M⋅M!) to O(M).\n\n7/20/2008 Tie-Yan Liu @ Tutorial at SIGIR 2008 130 ListMLE Algorithm (F. Xia, T. Liu, et al. ICML 2008) • Permutation Likelihood • Likelihood Loss • Model = Neural Network • Algorithm = Stochastic Gradient Descent 7/20/2008 Tie-Yan Liu @ Tutorial at SIGIR 2008 131\n\nLearning to Rank Algorithms: Listwise Loss Minimization\n\n• In addition to Luce model, likelihood defined by Mallows model has also been used in listwise ranking.\n\n-Cranking: Combining rankings using conditional probability models on permutations (G. Lenanon and J. Lafferty, ICML 2002)\n\n• However, we will not further discuss this work due to the following reasons.\n\n-The Mallow model is more suitable for combining multiple input rankings (rank aggregation), but not for \"learning to rank\" as mentioned in this tutorial. -The testing complexity might be very high since Mallows model does not have the same properties as Luce model.\n\n7/20/2008 Tie-Yan Liu @ Tutorial at SIGIR 2008 132 Theoretical Analysis on Listwise Loss Minimization (F. Xia, T. Liu, et al. ICML 2008)\n\n• The listwise approach captures the ranking problem in a conceptually more natural way. • However, the listwise approach lacks of theoretical analysis.\n\n-Existing work focuses more on algorithm and experiments, than theoretical analysis. -While many existing theoretical results on regression and classification can be applied to the pointwise and pairwise approaches, the theoretical study on the listwise approach is not sufficient.\n\n7/20/2008 Tie-Yan Liu @ Tutorial at SIGIR 2008 133\n\nTrue Loss in Listwise Ranking\n\n• To analysis theoretical properties of listwise loss functions, the \"true\" loss of ranking is to be defined.\n\n-The true loss describes the difference between a given ranked list (permutation) and the ground truth ranked list (permutation).\n\n• Ideally, the \"true\" loss should be cost-sensitive, but for simplicity, we investigate the \"0-1\" loss. Learning to Rank Algorithms: Listwise Loss Minimization\n\n1 ) (   Surrogate Loss Minimization Framework • RankCosine, ListNet and ListMLE can all be well fitted into this framework of surrogate loss minimization. -RankCosine -ListNet -ListMLE 7/20/2008 Tie-Yan Liu @ Tutorial at SIGIR 2008 137 Learning to Rank Algorithms: Listwise Loss Minimization\n\n• Continuity, differentiability, and convexity. • Computational efficiency • Statistical consistency • Soundness • Generalization ability These properties have been well studied in classification, but not sufficiently in ranking. 7/20/2008 Tie-Yan Liu @ Tutorial at SIGIR 2008 138 Likelihood loss (ListMLE) √ √ √ O(M) 7/20/2008 Tie-Yan Liu @ Tutorial at SIGIR 2008 139 • When minimizing the expected surrogate loss R φ (f) is equivalent to minimizing the expected 0-1 loss R(h) (which solution is Bayes ranker y * ), we say the surrogate loss function is consistent. Statistical Consistency 7/20/2008 141 Tie-Yan Liu @ Tutorial at SIGIR 2008\n\nThe perfect ranking of an object is inherently determined by its own. Minimum φ is achieved when sorting f(x) results in the same permutation with a given y.\n\n-Cosine Loss is statistically consistent.\n\n-Cross entropy loss is statistically consistent.\n\n-Likelihood loss is statistically consistent.\n\n7/20/2008 Tie-Yan Liu @ Tutorial at SIGIR 2008 142 Soundness • Cosine loss is not very sound -Suppose we have two documents x 2 ≻ x 1 . 7/20/2008 Tie-Yan Liu @ Tutorial at SIGIR 2008 143 f 1 f 2 f 1 =f 2 α Correct ranking Incorrect Ranking Learning to Rank Algorithms: Listwise Loss Minimization Soundness (2) • Cross entropy loss is not very sound -Suppose we have two documents x 2 ≻ x 1 . 7/20/2008 Tie-Yan Liu @ Tutorial at SIGIR 2008 144 f 2 f 1 =f 2 f 1 Correct ranking Incorrect Ranking Learning to Rank Algorithms: Listwise Loss Minimization Soundness (3) • Likelihood loss is sounder -Suppose we have two documents x 2 ≻ x 1 . 7/20/2008 Tie-Yan Liu @ Tutorial at SIGIR 2008 145 f 2 f 1 =f 2 f 1 Correct ranking Incorrect Ranking Learning to Rank Algorithms: Listwise Loss Minimization Generalization Ability • Rademacher Average based generalization theory. 7/20/2008 Tie-Yan Liu @ Tutorial at SIGIR 2008 146 Learning to Rank Algorithms: Listwise Loss Minimization Generalization ability Rademacher Average of Rademacher Averages of Listwise Ranking Algorithms 7/20/2008 Tie-Yan Liu @ Tutorial at SIGIR 2008 147\n\nFindings on Generalization Ability\n\n• The convergence rate of the three algorithms are all .\n\n• The bound for ListMLE decreases monotonously, while the bounds for ListNet and RankCosine increase monotonously, w.r.t. the list length M.\n\n• The bound for ListMLE is always tighter than that for ListNet. When the list length M is larger than or equal to 6, the generalization bound for ListMLE is tighter than that for RankCosine.\n\n• The exponential transformation function is not the best choice in terms of generalization bound. Linear transformation function is even better in most cases.\n\n7/20/2008 Tie-Yan Liu @ Tutorial at SIGIR 2008 148 Learning to Rank Algorithms: Listwise Loss Minimization       N O 1 Discussions • Based on the above theoretical analyses, it seems that Luce model based likelihood loss (corresponding to the ListMLE algorithm) is one of the best listwise loss functions.\n\n-The effectiveness of ListMLE has also been validated by empirical study, see (F. Xia, T. Liu, et al. ICML 2008).\n\n• Unsolved problems -In real ranking problems, the true loss should be costsensitive but not 0-1. -Approximation error is also important in generalization analysis.\n\n7/20/2008 Tie-Yan Liu @ Tutorial at SIGIR 2008 149\n\nRanking Functions\n\n• As can be seen, in most previous work, attention was paid to the loss function and the learning process. • The ranking function is assumed to be f(x) (i.e. independent relevance), and mostly a linear combination of pre-defined features, for simplicity.\n\n• Is it sufficient?\n\n-Beyond independent relevance.\n\n-Beyond single ranking function.\n\n-Beyond linear combination of features. Beyond Linear Combination of Features\n\n• Learning with IR model -Existing learning to rank work assumes features to be precomputed, with a specific (or default) parameter. -Learning process only focus on the combination of the features.\n\n-Can we also learn the optimal parameters of the features from the training data?\n\n• Other work Ranking Function Discovery using GA (W. Fan, M. Gordon, et al. HICSS 2004) • Use tf, idf, document length, etc. as \"low-level\" features • Use +, -, *, /, log as operators.\n\n• Use GA to discover the best ranking function. • Use BM25F as the ranking model, and use RankNet as the learning machine to tune its parameter based on the training data.\n\n• The performance was slightly improved over the default parameters in BM25F.\n\n-Not statistically significant.\n\n-Possible explanation: the BM25 model has been \"perfectly\" manually tuned in the literature.\n\n7/20/2008 Tie-Yan Liu @ Tutorial at SIGIR 2008 161"
}