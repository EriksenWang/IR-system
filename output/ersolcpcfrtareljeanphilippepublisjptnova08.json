{
    "title": "Perception through Scattering Media for Autonomous Vehicles",
    "publication_date": "1876",
    "authors": [
        {
            "full_name": "Nicolas Hautière",
            "firstname": "Nicolas",
            "lastname": "Hautière",
            "affiliations": [
                {
                    "organization": "Division Exploitation Signalisation Eclairage, Laboratoire Central des Ponts et Chaussées",
                    "address": {
                        "city": "Paris",
                        "country": "France",
                        "postcode": "75015"
                    }
                }
            ]
        },
        {
            "full_name": "Raphaël Labayrade",
            "firstname": "Raphaël",
            "lastname": "Labayrade",
            "affiliations": [
                {
                    "organization": "Ecole Nationale des Travaux Publics de l'Etat, Direction Générale de la Construction et du Bâtiment",
                    "address": {
                        "city": "Vaulx-en-Velin",
                        "country": "France",
                        "postcode": "69120"
                    }
                }
            ]
        },
        {
            "full_name": "Clément Boussard",
            "firstname": "Clément",
            "lastname": "Boussard",
            "affiliations": [
                {
                    "organization": "Ecole Nationale Supérieure des Mines de Paris, Centre de Robotique",
                    "address": {
                        "city": "Paris",
                        "country": "France",
                        "postcode": "75006"
                    }
                }
            ]
        },
        {
            "full_name": "Jean-Philippe Tarel",
            "firstname": "Jean-Philippe",
            "lastname": "Tarel",
            "affiliations": [
                {
                    "organization": "Division Exploitation Signalisation Eclairage, Laboratoire Central des Ponts et Chaussées",
                    "address": {
                        "city": "Paris",
                        "country": "France",
                        "postcode": "75015"
                    }
                }
            ]
        },
        {
            "full_name": "Didier Aubert",
            "firstname": "Didier",
            "lastname": "Aubert",
            "affiliations": [
                {
                    "organization": "Laboratoire sur les Interactions Véhicules Infrastructure Conducteurs, unité mixte de l'INRETS et du LCPC",
                    "address": {
                        "city": "Versailles",
                        "country": "France",
                        "postcode": "78000"
                    }
                }
            ]
        }
    ],
    "abstract": "The perception of the environment is a fundamental task for autonomous robots. Unfortunately, the performances of the vision systems are drastically altered in presence of bad weather, especially fog. Indeed, due to the scattering of light by atmospheric particles, the quality of the light signal is reduced, compared to what it is in clean air. Detecting and quantifying these degradations, even identifying their causes, should make it possible to estimate the operating range of the vision systems and thus constitute a kind of self-diagnosis system. In parallel, it should be possible to adapt the operation of the sensors, to improve the quality of the signal and to dynamically adjust the operation range of the associated processings. First, we introduce some knowledge about atmospheric optics and study the behavior of existing exteroceptive sensors in scattering media. Second, we explain how existing perception systems can be used and cooperate to derive some descriptors of the visibility conditions. In particular, we show how to detect fog presence and to estimate the visibility range. Once weather conditions have been determined, they can be exploited to reduce the impact of adverse weather conditions on the operation of vision systems. We propose thus different solutions to enhance the performances of vision algorithms",
    "full_text": "under foggy weather. Finally, we tackle the problem to know if light scattering could be turned to our advantage, allowing us to develop novel perception algorithms. Experiments in real situations are presented to illustrate the developments. Limits of the systems, future challenges and trends are discussed.\n\nMany factors can alter the quality of the signal resulting from an optical sensor mounted onboard an automotive vehicle (camera, laser, etc.): the fog, the rain, the sun at grazing angle, the reflections on the pavement, the presence of stains on the windshield, the glare due to the headlights of other vehicles, the strong gradients of brightness at the entrance and exit of tunnels, etc. To be able to detect and quantify these degraded operation conditions while relying only on the signals resulting from sensors themselves is a challenge for the future driver assistances based on optical sensors.\n\nFirst, it is one of the keys to obtain a very important level of reliability of the sensor unit and associated signal processing. Indeed, whatever its intrinsic qualities, a processing will produce the awaited answers only if the input signal has a sufficient level of quality. Detecting and quantifying the degradations of this signal, even identifying the causes of these degradations, should make it possible to estimate an index of confidence on the operation of the system and thus constitute a kind of self-diagnosis system. In parallel, it can be possible to adapt the operation of the sensor, to improve the quality of the signal and/or to dynamically adjust some parameters in the processing.\n\nSecond, it is a means of carrying out new driver assistances, e.g. the automation of the fog lamps. This is also a means of gathering within the same sensor (in particular a camera) a set of functions that are already present in the vehicle (rain sensor, light level sensor) or that are going to come (automatic fog lamps, automatic demisting, automatic cleaning of the stains for example). Such a reduction of the number of sensors would make it possible to decrease the volume and the total cost of the system.\n\nThird, some of the causes of the degradation of the signal quality are also causes of road accidents (e.g. rain, fog, sun at grazing angle, etc.). Thus, by establishing a mapping function between the vision of the driver and the vision of the sensor, in particular in terms of dynamic range, resolution and sensitivity, such algorithms can make it possible to generate relevant alarms for the driver in the event of behaviors unsuited to the traffic conditions.\n\nUntil now, this approach has been followed for the computation of the visibility range, which constitutes a relevant and illustrative case study. Starting from existing works on fog modeling, two complementary methods have been developed aiming at estimating the visibility range using respectively one or two onboard cameras. Both methods are based on the definition of the meteorological visibility distance. The first technique, using a model of atmospheric scattering, detects and estimates the density of daytime fog by using a single camera. The second technique, using a generic property of the atmosphere, is able to estimate the visibility range under all meteorological conditions both in daytime and in night time by using a stereoscopic sensor or a single camera coupled with an inertial navigation system. In the future, these methods are likely to be used to provide drivers with an appropriate speed with respect to the visibility range, to enhance obstacle detection techniques or to automate the fog lamps.\n\nThis paper is organized as follows. First, it deals with the general problem of artificial perception under adverse weather conditions. Second, both approaches computing the visibility range are presented and validated thanks to actual images and video sequences grabbed under various situations on the test track of Versailles Satory. In particular, we show how to detect fog presence. We propose then different solutions to enhance the performances of vision algorithms under foggy weather. Finally, some ideas for future researches are indicated.\n\nThe literature on the interaction of light with the atmosphere has been written over more than two centuries [1,8]. Different reviews on the topic have been available in the literature [45,46] for half a century and still serve as reference for recent works in computer vision.\n\nFog is a thick cloud of microscopic water droplets suspended at ground level. When the horizontal visibility is smaller than 1 km, one speaks about fog. When the horizontal visibility is greater than 1 km, one speaks rather about haze. The fog is usually characterized by two parameters: the granulometry and the concentration of water droplets.\n\nIn a schematic way, the visible light must go through an aerosol which contains a great number of particles having a diameter of a few micrometers. The wavelength of visual light is comprised between 400 nm and 700 nm.\n\nWhen light propagating in fog, the luminous flow is attenuated by two phenomena: absorption and scattering, which leads to characterize fog by an extinction coefficient k, which is the sum of the absorption and scattering coefficients. In fact for visible light, absorption is negligible in this type of aerosol. The main phenomena in the light attenuation is thus scattering which deviates light rays from their initial direction. Finally, fog is characterized by an extinction coefficient k which is equal to the scattering coefficient.\n\nIf φ 0 is the luminous flow emitted by a given light source, the transmitted flow φ along the distance d through fog is given by Beer-Lambert's Law:\n\nwhere the product kd represents the optical thickness of the scattering media.\n\nIn the preceding paragraph, we have defined the physical characteristics of fog and the resulting phenomena, i.e. the visible light scattering. Based on [20], we propose in this paragraph to describe the different visual effects of this phenomena. We focus mainly on daytime fog situations.\n\nDuring daytime, the sky is the main source of luminous energy. It generates a uniform illuminance throughout the environment, which depends on time as well as geographical and meteorological conditions. Part of this energy is re-emitted toward the camera, generating an intrinsic luminance L 0 . In the presence of fog, part of this energy is scattered by fog along the distance d between a scene element and the camera, causing the transmitted luminance L t to be attenuated by a factor equal to e -kd :\n\nFog alters the visual signal by scattering the light emitted by every light source. Part of this energy is scattered back towards the camera off-axis, adding a halo of scattered light around the transmitted signal. This effect was shown to be equivalent to a convolution. Therefore, by analogy with an optical filter, a slab of fog can be characterized with its modulation transfer function (MTF), equal to the module of the Fourier transform (FT) of its point spread function (PSF). Halo effect is negligible under daytime conditions, contrary to nighttime situations like in Fig. 1b.\n\nVeil from Atmosphere In daytime fog, the droplets in the air between the observer and the elements of the road environment also contribute to the apparent luminance by scattering toward the eye some of the energy it receives from the sky (single scattering) and from other droplets (multiple scattering). The resulting luminance L a known as the atmospheric veiling luminance or airlight increases exponentially with distance (cf. Fig. 1a):\n\nwhere L f denotes the luminance of the fog at the horizon.\n\nWhen the observer is driving in fog, the low-beam headlamps of his vehicle ought to be turned on. Fog droplets in front of the vehicle interact with this luminous flux, scattering a part of it back into the eyes of the driver in a nonuniform luminance distribution, known as the backscattered veiling luminance. It was found to be at least two orders of magnitude smaller than the atmospheric veiling luminance in daytime. Yet, it contributes to the loss of visibility in nighttime conditions and therefore should be taken into account (cf. Fig. 1c).\n\nIn 1924, Koschmieder [45] established a simple relationship between the apparent luminance L of an object at a distance d, and its intrinsic luminance L 0 :\n\nwhere L f denotes the luminance of background sky. Based on these results, Duntley [45] derived a law for the atmospheric attenuation of contrasts:\n\nwhere C designates the apparent contrast at distance d and C 0 the intrinsic contrast of the object against the sky.\n\nThe CIE [16] adopted a contrast threshold of 5% to define V met , the \"meteorological\" visibility distance, defined as the greatest distance at which a black object (C 0 = -1) of suitable dimensions can be recognized by day against the horizon sky:\n\nAn extended model of fog taken into account all the different effects has been proposed in [20]. However, in this chapter, we focus on daytime situations, where despite its limitations, Koschmieder's model has been proven to be very useful.\n\nThe operation range of exteroceptive sensors depends on the weather conditions. A study has been done in [36] to evaluate the operation range of infrastructure based sensors with respect to weather conditions. The results can be extrapolated for in-vehicle sensors. Thus, according to the curves plotted in Fig. 2 and partially extracted from [5], the output signal of optical sensors running in the visible or near infrared light range is degraded by adverse weather conditions. Consequently, signal processings techniques relying on optical sensors to detect obstacles or the lane markings are less efficient under adverse weather conditions.\n\n10 GHz 100 GHz 1 THz 10 THz 100 THz 1000 THz Vi RADAR 1000 100 10 1 0.1 0.01 Frequency [Hz] Fog V=50m IR 24GHz 77GHz Attenuation (dB/km) 10 GHz 100 GHz 1 THz 10 THz 100 THz 1000 THz Vi RADAR 1000 100 10 1 0.1 0.01 Frequency [Hz]\n\nAttenuation (dB/km) Figure 2: Curve, partially issued from [5], depicting the atmospheric attenuation due to dense fog (V=50 m) according to the frequency of the signal.\n\nThus we may use optical sensors to characterize fog and in particular to recover the value of the atmospheric extinction coefficient k. In this paragraph, we review the different approaches which have been published.\n\nThis lexicographical term serves to designate two main types of instruments for both detecting fog and measuring the extinction coefficient k; they are transmissometers and scatterometers [42].\n\nThe basic principle behind this category of instrument consists of measuring average transmissivity of the atmosphere along a given path (see Fig. 3). Transmissometers are composed of both a projector comprising a source emitting a luminous flux φ 0 within the visible domain and a receiver set located at an invariable distance d that measures the luminous flux φ received. By using Beer-Lambert's Law, the extinction coefficient of the fog k, used for calculating the meteorological visibility distance (6), is given by:\n\nThe transmissometers are reliable. Their sensitivity is related to the length of the measurement base d. This length, which extends over several meters or even several tens of meters, provides these devices with a high level of accuracy, given the lack of homogeneity often encountered in fog. Transmissometers however are costly to implement and the optical block alignment frequently proves to be a complex procedure.\n\nSome of these devices were developed for road applications, primarily for conducting measurements under conditions of thick fog. They enable quantifying the light diffused within a sufficiently wide and well-defined solid angle. In order to carry out such measurements, a light beam is concentrated on a small volume of air (see Fig. 4). The proportion of light being diffused toward the receiver would then be:\n\nwith I the intensity diffused in the direction of the receiver, A a constant dependent on power and source optics, I 0 the source intensity, V the diffusing volume, f (θ) the value of the diffusion function in the θ direction, k the extinction coefficient and d the length of the optical path between emitter and receiver. Generally speaking, the optical path d is small and the transmission factor e -kd is assimilated to 1 and f (θ) is proportional to k, with (8) thereby becoming: where A' designates a constant that depends on device characteristics. We can state that a scatterometer, to its advantage, is significantly less expensive than a transmissometer and that no optical block alignment is required. On the other hand, the small size of the diffusing volume makes measurements highly sensitive to nonhomogeneities in the fog. Furthermore, the sensor accuracy decreases with the meteorological visibility and is not acceptable for visibilities below 50m.\n\nConsequently, neither a transmissometer or a scatterometer may be easily placed onboard a moving vehicle. Indeed, the measurement path is too short and the aligment of optical blocks is difficult for a transmissometer. A scatterometer would be too sensitive to the turbulence caused by the motion of the vehicle. However, it would be interesting to make some trials.\n\nThis laser based sensor can be used for estimating fog density by measuring the signal backscattered by fog droplets [11]. The power per pulse from range r received by a LIDAR is given by the simplified LIDAR equation [17]:\n\nwhere P T denotes the peak transmitted power per pulse, c the speed of light, τ the pulse duration, β the backscatter cross section per unit volume, A e the effective receiver aperture and k(x) the atmospheric extinction coefficient. Under strictly homogeneous conditions, β and k are not dependant of range. Hence, (10) becomes: where A \" denotes a constant that depends on device characteristics. The slope of the decaying waveform when differentiating (10) with respect to range becomes:\n\nThis differential equation is the basic principle on which the LIDAR measurement of the extinction coefficient in a homogeneous scattering medium is done (cf. Fig. 5).\n\nFor automotive applications, the LIDAR seems to be the best suited active sensor for estimating the meteorological visibility, since it does not need any external receiver, contrary to infrastructure based visibilitymeters, and can be used for other safety applications, e.g. obstacles detection [51] or lane recognition [49]. Consequently, it has been used for adjusting the power of headlights [7,54] or for adjusting the headway in Automatic Cruise Control (ACC) [13] according to the prevailing meteorological conditions. However, it has been shown that the dynamic adaptation of the emitting power of a LIDAR with respect to visibility conditions is not always perfect [18].\n\nIf a camera is used, there is no need to align the optical units as it is the case with the transmissometer, and an image is obtained which is representative of the environment, unlike with a scatterometer. Finally, in the case of a classical camera, the spectra taken into account is in the visible domain. Consequently, its image is degraded by the presence of fog. Most approaches make use of a fixed camera placed on the roadway which simplifies the task as a reference image is always available [12,37].\n\nSystems that entail use of an onboard camera are encountered less frequently. Pomerleau [53] estimates visibility by means of measuring a contrast attenuation per meter on the road markings at various distances in front of the vehicle. However, this approach based on the RALPH System [52] only indicates a relative visibility distance and requires the detection of road markings to run. The principle of the technique is schematized in Fig. 6.\n\nYahiaoui [61] estimates the quality of images by comparing the MTF of the current image with a contrast sensitivity function [44]. However, it only returns a potential visibility distance. So, these methods estimate what could be the maximum visibility distance in the scene.\n\nIn the two next sections, we present our camera based approaches. The first is able to estimate the extinction coefficient of the atmosphere using a single camera. The second is able to estimate the actual visibility range using either a camera coupled with an Inertial Navigation System (INS) or with a binocular camera.\n\nIn this section, a method to compute the extinction coefficient k and thus the meteorological visibility distance using a single camera behind the vehicle windshield is recalled from [32]. This system was patented [41].\n\nIn the image plane, the position of a pixel is given by its (u,v) coordinates. The coordinates of the optical center projection in the image are designated by (u 0 ,v 0 ). In Fig. 7, H denotes the height of the camera, θ the angle between the optical axis of the camera and the horizontal, and v h the horizon line. The intrinsic parameters of the camera are its focal length f l , and the horizontal size t pu and vertical size t pv of a pixel. We have also made use herein of α u = f l tpu and α v = f l tpv , and have typically considered:\n\nThe road is assumed flat, which makes it possible to associate a distance d with each line v of the image:\n\nModeling of the camera within its environment; it is located at a height of H in the (S,X,Y ,Z) coordinate system relative to the scene. Its intrinsic parameters are its focal length f and pixel size t. θ is the angle between the optical axis of the camera and the horizontal. Within the image coordinate system, (u,v) designates the position of a pixel, (u 0 ,v 0 ) is the position of the optical center C and v h is the vertical position of the horizon line.\n\nLet us denote f the camera response function, which models the mapping from scene luminance to image intensity by the imaging system, including optic as well as electronic parts [22]. With the notations of Section 2, the intensity I of a pixel is the result of f applied to the sum of the airlight L a and the direct transmission L t , i.e:\n\nIn this work, we assume that the conversion process between incident energy on the CCD sensor and the intensity in the image is linear. This is generally the case for short exposure times, because it prevents CCD array to be saturated. Furthermore, short exposure times (1 to 4 ms) are used on in-vehicle cameras to reduce the motion blur. This assumption can thus be considered as valid and ( 14) becomes:\n\nwhere R is the intrinsic intensity of the pixel, i.e. the intensity corresponding to the intrinsic luminance value of the corresponding scene point and A ∞ is the background sky intensity.\n\nFollowing a variable change from d to v based on ( 13), ( 15) then becomes:\n\nBy twice taking the derivative of I with respect to v, one obtains the following:\n\nwhere\n\nThe only useful solution is given in (18):\n\nwhere v i denotes the position of the inflection point of I(v). In this manner, the parameter k of Koschmieder's law is obtained once v i is known. Finally, thanks to v i , v h and k values, the values of the other parameters of ( 15) are deduced through use of I i and dI dv v=v i , which are respectively the values of the function I and of its first derivative at v = v i :\n\nwhere R is the mean intrinsic intensity of the road surface.\n\nTo estimate the parameters of ( 15), the median intensity on each line of a vertical band is estimated and an inflection point is detected. So as to be in accordance with Koschmieder's law assumptions, this band should only take into account a homogeneous area and the sky. Thus, a region within the image that displays minimal line-to-line gradient variations when browsed from bottom to top is identified thanks to a region growing process, illustrated in Fig. 8a. A vertical band is then selected in the detected area. Finally, taking the median intensity of each segment, yields the vertical image intensity Ĩ from which the inflection point is computed. The small white triangle on the image top left indicates that the system is operative. Conversely, a small black triangle indicates that the system is temporarily inoperative. The black vertical lines represent the limits of the vertical band analyzed. In this example, V met ≈ 60m.\n\nTo obtain the values of the parameters of ( 15), the position of the horizon line must be estimated. It can be estimated by means of the pitching of the vehicle when an inertial sensor is available, but is generally estimated by an additional image processing. Generally, this type of processing seeks to intersect the vanishing lines in the image. However, under foggy weather, the vanishing lines are only visible close to the vehicle. It is thus necessary to extrapolate the position of the horizon line through the fog. Consequently, this kind of process is prone to a significant standard deviation and, for the moment, we use the a priori sensor calibration.\n\nHaving now the vertical positions of both the inflection point and the horizon line, the parameters of ( 15) can be recovered and the position of the image line representative of the meteorological visibility distance is deduced. Fig. 8b illustrates the process.\n\nGiven that we have a theoretical model of the vertical variation of the intensity in the image, the confidence index on the measurement is linked with the difference between the theoretical model I and its estimation Ĩ. In other words, the more noisy Ĩ is, the less reliable the measurement is. Let us quantify this difference. As the theoretical curve is decreasing, we first smooth Ĩ until it becomes decreasing. We denote this curve Ī. We then compute the difference E between the derivative of Ĩ and the derivative of Ī, which can be expressed as following:\n\nThe maximum error E max which can be made on the computation of the derivative is equal to:\n\nwhere G max denotes the maximum authorized value of the vertical gradient in the region growing process and n the lines number of the measurement bandwidth. A normalized confidence index C can be formulated:\n\nThe proposed index has been validated using synthetic data. Two examples of index confidence computations are given in Fig. 9. The estimation of V met is correct if the position v i of the inflection point as well as the position v h of the horizon line are correct. Let us study the influence of an estimation error δ on the difference of these two positions. The error S between the estimated meteorological visibility distance Ṽmet and the actual meteorological visibility distance V met is expressed with respect to δ value by:\n\nThe curves in Fig. 10 give the committed error for values of δ ranging from -4 to +4. We can conclude that underestimate the difference of positions is more penalizing that overestimate it. To have stable measurements, we thus prefer set the horizon line above its real position.\n\nFigs. 11 and 12 show some results of meteorological visibility distance computations in two daytime fog video sequences.\n\nTo run properly, the previous method needs a homogeneous area and the sky. In case of a low dense fog or a strong masking of the road, the method is not operative anymore. Indeed, the region growing is unable to cross the image from bottom to top. Figs. 13abc show such situations.\n\nTo limit the problem, when it is possible, we propose to add a measurement of the contrast attenuation between the road and the lane markings [23], like it was proposed in [53]. However, in our case, instead of directly detecting the markings, we prefer using the results of the previous region growing. In this aim, we assume that the markings are on the border of the area detected by the region growing algorithm and we search the pixels whose intensity is greater than the median intensity I m of the considered image line. Then, on each line, the median intensity I M of the lane markings is computed. Some examples are shown in Fig. 14. In fact, this approach could be used under every meteorological conditions. However, under beautiful weather, the shadows prevent the method to run properly contrary to foggy weather where there is no shadows.\n\nThereafter, contrary to [53] who estimates a contrast attenuation factor per meter, we prefer to estimate the meteorological visibility distance so as to be coherent between both methods. Thanks to (4), we know the theoretical variations of I m (road intensity) and I M (markings intensity) with respect to the distance. By considering two distances d 1 and d 2 , the extinction coefficient k can be expressed by:\n\nWe deduce the value of the meteorological visibility distance as:\n\nlog\n\nBoth methods are complementary. Whereas the first method does not need the presence of road markings, the second one does not need the presence of the sky in the image. It is thus possible to draw advantage from both methods to build a better one.   I M1 I M2 d 1 d 2 I M1 I M2 d 1 d 2 I M1 I M2 d 1 d 2 (a) I m2 I m1 I m2 I m1 I m2 I m1 (b) Figure 14: Estimation of the meteorological visibility distance based on the lane markings contrast attenuation.\n\nThe previous method leads to good results in daytime foggy weather. In order to extend the range of covered meteorological situations, we developed a different approach, which consists in estimating what we call the mobilized visibility distance V mob [25].\n\nFor the CIE, the meteorological visibility distance is the greatest distance at which a black object of a suitable dimension can be seen in the sky on the horizon. We have decided to build a method which is close to this definition. In this aim, we propose to study the distance to the most distant object having enough contrast with respect to its background. On Fig. 15, we represent a simplified road with dash road marking. On Fig. 15a, we suppose that the most distant visible object is the extremity of the last road marking (it could have been the border of the road too). On Fig. 15b, the vehicle has moved and a new road marking is now visible. We call this distance to the most distant visible object, which depends on the road scene, the mobilized visibility distance V mob . This distance has to be compared to the mobilizable visibility distance V max . This is the greatest distance at which a picture element on the road surface would be visible.\n\nConsequently, we have the following relationship:\n\nIn this section, we are going to establish the link between the mobilizable visibility distance and the meteorological visibility distance. The mobilized visibility distance is the distance to the most distant object W considered as visible. We denote L b 0 and L w 0 , the intrinsic luminances and L b et L w the luminances at the distance d of the road B and the object W . Koschmieder's law gives us the theoretical variations of this values according to the distance d. Let's express the contrast C BW of W with respect to B like Weber does, see (36):\n\nWe deduce the expression of d according to the photometric parameters, the contrast C BW and the fog density k:\n\nFigure 15: Examples of mobilized and mobilizable visibility distances. The mobilized visibility distance V mob is the distance to the most distant visible object existing on the road surface. The mobilizable visibility distance V max is the greatest distance at which a potential object on the road surface would be visible.\n\nThat is to say the distance where an object W is perceived with a contrast of C BW . Thanks to (6), we can express this value according to the meteorological visibility distance\n\nLike CIE does, we can choose a threshold CBW below which the object is considered as being not visible. Like for the computation of the meteorological visibility distance, we assume that the road intrinsic luminance is equal to zero. Then, we define the mobilizable visibility distance V max valid for every threshold contrast:\n\nThe energy received by the object W is not entirely reflected towards the camera. Consequently, we have the following relationship:\n\nWe deduce the value of V max :\n\nThen, we easily obtain the value CBW so that V max = V met :\n\nSo, by choosing a contrast threshold CBW of 5 %, the mobilizable visibility distance is close to the meteorological visibility distance V met for a black object.\n\nActually, the road is never pure black and the sky rarely pure white. The mobilizable visibility distance represents a maximum of visibility distance which is rarely reachable, since it is the greatest distance at which the clearest object is visible on a black road. On the other hand, the mobilized visibility distance, which only takes into account the gray objects encountered in the image is the distance that we are able to estimate directly as explained in the following.\n\nIn section 5.1.1, we have introduced the concepts of mobilized and mobilizable visibility distances. Whereas the first one depends on the road scene, the second one only depends on the meteorological conditions. Then, in section 5.1.2, we established the link between the meteorological visibility distance defined by the CIE and the mobilizable visibility distance previously defined. In particular, we calculated the contrast threshold so that both distances are the same, that is to say 5 %. Consequently, we propose to estimate the mobilized visibility distance by estimating the distance to the most distant object on the road surface having a contrast above 5 %. This method is decomposed in two tasks. The first one consists in computing the contrasts in the image and selecting the ones above 5 %. The second one is the depth computation of the detected picture elements and the selection of the most distant one.\n\nDifferent definitions of the contrast exist. One of the most famous is Michelson's contrast [60]. It has been introduced to quantify the visibility of sinusoidal gratings:\n\nwhere L min and L max are the minimal and maximum luminance values of the image. The use of sinusoidal gratings and of this contrast definition has met a great success in psychophysics. In particular, it has been used to study the human eye by building contrast sensivity functions (CSF). Weber [60] defined the contrast as being a relative luminance variation ∆L with respect to the background L. This has been used to measure the visibility of targets:\n\nThis contrast definition is sometimes called psychophysical contrast and it is used in the definition of the meteorological visibility distance. These definitions are good estimators of contrast for the stimuli previously mentioned: sinusoïdal gratings for Michelson, uniform targets for Weber. However, they are not well adapted when the stimulus becomes more complex. Moreover, none of these definitions are adapted to estimate the contrast in natural images. This is mainly due to the fact that the contrast perception is local. This is on these local methods that we focused our attention.\n\nThe LIP model [35] has introduced a definition of contrast well suited to digital images. In this definition, the contrast between two pixels x and y of an image f is given by:\n\nwhere △ -denotes LIP substraction. Naturally, this definition of contrast is consistent with the definition of contrast used in visual perception (36).\n\nThen, the contrast associated to a border F which separates two adjacent regions follows:\n\nwhere △ × and △ + denote LIP multiplication and addition.\n\nTo implement this definition of contrast between two adjacent regions, Köhler's segmentation method has been used [38]. Let f be a gray level image. A couple of pixels (x,y) is said to be separated by the threshold s if two conditions are met. First, y ∈ V 4 (x). Secondly, the condition (39) is respected:\n\nLet F (s) be the set of all couples (x, y) separated by s. With these definitions, for every value of s belonging to [0,255], F (s) is built. For every couple belonging to F (s), the contrast C x,y (s) is computed:\n\nThe mean contrast (41) associated to F (s) is then performed:\n\nThe best threshold s 0 verifies the following condition:\n\n0 50 100 150 200 250 300 0 10 20 30 40 50 60 Threshold Contrast (a) 0 50 100 150 200 50 100 150 200 250 Position Intensity (b) 0 50 100 150 200 50 100 150 200 250 Position Intensity (d) 0 50 100 150 200 250 300 0 10 20 30 40 50 Threshold Contrast 1 (c) It is the threshold which has the best mean contrast along the associated border F (s 0 ). Instead of using this method to binarize images, we use it to measure the contrast locally. The evaluated contrast equals 2C(s 0 ) along the associated border F (s 0 ). Finally, if 2C(s 0 ) > 5%, F (s 0 ) is considered to be a visible edge. Details about the implementation of this method can be found in [26].\n\nThe method derived from Köhler is robust to noise. We assume that the noise of the camera is gaussian.\n\nLet consider two gaussian distributions of means L 1 and L 2 and standard deviations σ 1 and σ 2 . We can show that, as long as both distributions do not intersect, the optimal threshold s 0 found by Köhler's technique is a gaussian distribution with mean L 1 +L 2 2 and standard deviation [26]. Consequently, the method is robust to noise, because in average the returned threshold is the one without noise at the same distance of both distributions. This property is still verified when using the local formula of LIP contrast. deviation σ = 1 and σ = 15 respectively. The optimal threshold found by Köhler's technique, which is represented by the horizontal dashed line, is the same for both distributions. It is the one, which gives the maximum contrast (cf. Figs. 16b and 16d). On the opposite, if both distributions are intersected, i.e. if max(3σ 1 , 3σ 2 ) > L 2 -L 1 2 , Köhler's technique is not so efficient anymore.\n\nSome examples of computations of local contrast above 5% are given in Fig. 17 using images grabbed under different meteorological conditions.\n\nIf just a single camera is used, we are unable to gain access to image depth, like it is depicted in Fig. 18. This problem is generally overcomed by adopting the hypothesis of a flat world, which makes it possible to associate a distance with each line of the image. However, the depth on vertical objects is uncorrect and is unknown without another assumption. In a first\n\nFigure 18: Principle of non-determination of depth using a single camera.\n\napproach, we can detect picture elements belonging to the road surface. Techniques that search the road surface are numerous. A first family of methods finds the road surface by a segmentation process. Color segmentation [6,33], texture segmentation [2] are the main approaches. A second family of methods finds the road surface by detection of its edges [3,14,58]. Conversely, we can detect the objects above the road surface. Some techniques are based on optic flow computation [21]. However, it is time consuming and the main hypothesis is not always verified (spatio-temporal luminous flow preserved). Some methods rely on template matching [4] or local symmetry [10] but are necessarily not generic. In addition, techniques like depth from scattering [48], depth from focus/defocus [19], shape from shading [62] are not adapted to our objectives.\n\nWe have developed a generic monocular approach. Knowing precisely the relative motion of the vehicle between two instants, the road plane in successive images is aligned, like in [55]. By perspective projection, the objects belonging to the road surface are correctly aligned, whereas the vertical objects are deformed. This allows estimating the mobilized visibility distance. The principle of this approach is presented in section 5.4.\n\nIf we use stereovision, we are not limited to the flat world hypothesis and we are able to gain access to the depth of nearly every pixels in the image [15]. However, because of real-time constraints, most approaches compute a sparse disparity map. We present our approach in section 5.5 which is based on \"v disparity\" concept [40]. Another approach based on \"v-disparity\" can be found in [57].\n\nCamera Model The different camera parameters were described in section 4.1. The transformation between the vehicle frame (with origin at the center of gravity of the vehicle) and the camera frame, is represented by a vectorial translation 19) and a rotation around the axes Y of angle β. We denote T the translation matrix and R the rotation matrix. The coordinate change between the image frame and the camera frame can be expressed using a projective matrix M proj [34]: At last, we obtain the transformation matrix T r from the vehicle frame to the image frame:\n\nIf P is a point with homogeneous coordinates (X, Y, Z, 1) in the vehicle frame, its homogeneous coordinates in the image frame become:\n\nWe can now compute the coordinates (u, v) of the projection of P in the image frame:\n\nFlat World Assumption If we consider I 1 and I 2 images taken at time t 1 and t 2 , the knowledge of the vehicle dynamics allows us, thanks to (46), to obtain an estimation of the image I 2 from the image I 1 . Let Ĩ12 be this estimated image and P a point whose projection in the image frame belongs to it. Let us assume that this point belongs to the road plane, meaning that if (X 2 , Y 2 , Z 2 ) are the coordinates of this point in the vehicle frame, then Z 2 = 0. So the expression of X 2 and Y 2 is deduced from (46):\n\nVehicle Motion The motion M of the camera between two instants can be represented by a rotation and a translation. These transformations are in fact the same as the vehicle gets between two instants. Let φ be the yaw angle, ψ be the pitch angle and θ the roll angle (Fig. 19), then the rotation matrix Rot is given by:\n\nThe translation can be decomposed following the axes X, Y, Z and is denoted T rans = (T x , T y , T z ) T . The rotation-translation can be rewritten in terms of homogeneous coordinates with the following system:\n\nCreation of an Aligned Image From the knowledge of the coordinates of a point P in (47) and of the vehicle dynamics given by M , we can express the coordinates of the point P in the camera frame at time t 1 :\n\nwhere M is the vehicle rotation/translation matrix. We obtain the coordinates (u 12 , v 12 ) of P in the image frame of I 1 :\n\nAn example of aligned image computation for a large displacement (≈ 4m) is shown in Fig. 20.\n\nImage Matching We have to match both images I 1 and Ĩ12 . It means that we have to find local correspondences between two neighborhoods from each image. These correspondences are computed via the ZNCC correlation metrics (a comparison of different existing metrics is carried out in [50]): i\n\nwhere Ī1 (x) and Ī12 (x + ∆) are the means of pixel intensities for the window centered at x and ∆ is the considered shift.\n\nThe more the correlation score is close to 1, the more we can consider these two neighborhoods as identical. Since the road plane is aligned in both images I 1 and Ĩ12 , no scanning is normally necessary to match image features using ZN CC correlation. However, working on a single pair of neighborhoods limits our study. Indeed, some matching errors can occur and a pixel belonging to the road can be incorrectly aligned in the image Ĩ12 . That's why we defined a search window (cf. Fig. 21a). The correlation neighborhood in image I 1 is centered on an edge pixel. The correlation neighborhood in the image Ĩ12 is centered successively around a pixel varying in a search neighborhood centered on the pixel (u 1 , v 1 ) in image I 1 .\n\nWe defined two kinds of search neighborhoods: squared neighborhoods for objects belonging to the road plane and deformed neighborhoods for vertical objects (cf. Fig. 21, like it is done in spatial stereovision [59]. Indeed, one can notice that objects not belonging to the road plane are deformed towards the top and the borders of the image after image alignment. Finally, the idea is to compute for each pixel of gradient a disparity with a normal and a deformed searching neighborhood and to keep the disparity giving the best ZN CC correlation score. An example of result is given in Fig. 22 using actual images of fog. The majority of pixels belonging to the road plane is successfully recognized, contrary to the pixels belonging to the vertical sign. Having now the pixels belonging to the road plane, we can associate a depth with each line of the image assuming a flat world using (13). In this process, we do not necessary need to find the pixels belonging to the vertical objects. However, they are used to filter erroneous pixels associated to the road plane or to a vertical object according the majority of pixels found in a neighborhood. Details about the method implementation as well as the role that plays the vehicle dynamics in the method are given in [9].\n\nThe previous method is able to compute a depth map of the road surface using a single camera coupled an inertial navigation system. However, we have to assume that the road is flat. The mobilized visibility distance is thus the range to the most distant visible object on the road plane. In this section, we use spatial stereovision and we are thus able to compute the road profile. The mobilized visibility distance will thus be the range to the most distant visible object on the road surface. From a theoretical point of view, the method should be more precise since the road is never a perfect plane. However, it requires a stereovision sensor with a rectified epipolar geometry which is also a rather strong constraint.\n\nThe stereovision algorithm uses the \"v-disparity\" transform, in which the detection of straight lines is equivalent to the detection of planes in the scene. In this aim, we represent the v coordinate of a pixel towards the disparity ∆ (performing accumulation from the disparity map along scanning lines) and detect straight lines and curves in this \"v-disparity\" image (denoted by I v ∆ ) [40]. This algorithm assumes the road scene is composed of set of planes: obstacles are modelized as vertical planes, whereas the road is supposed to be an horizontal plane (when it is planar), or a set of oblique planes (when it is not planar), as shown in Fig. 23.\n\nAccording to the modeling of the stereo sensor given on Fig. 23, the plane of equation Z = d, corresponding to a vertical object, is projected along the straight line of (53) in\n\nThe plane of equation Y = 0, corresponding to the road surface, is projected along the straight line of (54) in\n\nThe different camera parameters were described in section 4. are h which denotes the height of the cameras above the ground and b which is the distance between the cameras (i.e. the stereoscopic base). Details can be found in [40].\n\nThe algorithm performs a robust extraction of these planes from which it deduces many useful information about the road and the obstacles located on its surface. Fig. 28 illustrates the outline of the process. From two stereo images, a disparity map I ∆ is computed (ZNCC criteria is used to this purpose along edges). Then an accumulative projection of this disparity map is performed to build the \"v-disparity\" image I v∆ . For the image line i, the abscissa u M of a point M in I v∆ corresponds to the disparity ∆ M and its grey level i M to the number of points with the same disparity ∆ M on the line i : i M = P ∈I ∆ δ v P ,i δ ∆ P, ∆ M where δ i,j denotes the Kronecker delta. From this \"v-disparity\" image, a robust extraction of straight lines is performed through a Hough transform. This extraction of straight lines is equivalent to the extraction of the planes of interest taken into account in the modeling of the road scene (see Fig. 24c).\n\nSparse approach In order to quickly compute the \"v-disparity\" image, a sparse and rough disparity map has been built. This disparity map may contain numerous false matches, which prevents us to use it as a depth map of the environment. Thanks to the global surfaces extracted from the \"v-disparity\" image, false matches can be removed. In this aim, we check wether a pixel of the disparity map belongs to any global surface extracted using the same matching process. If it the case, the same disparity value is mapped to the pixel and leads to Fig. 24d. Details of this process can be found in [39]. Finally, this enhanced disparity map can be used as a depth map of the vehicle environment, since the depth D of a pixel of disparity ∆ is expressed by: D = b(α cos θ -(jv 0 ) sin θ) ∆\n\nQuasi-dense approach However, such a disparity map is sparse, i.e. the disparity is known only on vertical edges pixels. But under degraded weather conditions, it poses a problem, particularly on the top of vertical objects, such as vehicles where numerous false matches still exist.\n\nTo cope with this situation, a complementary approach has been proposed. It is based on a disparity propagation method.\n\nWe thus propagate the seeds, which are the set of the matched pixels in the first pass of the v-disparity algorithm, like in [43], except that for each matching pair candidate, we check if it belongs to one the profils of the v-disparity image. Thanks to this approach, the disparity map is quasi-dense especially on the horizontal edges (see Figs. 24ef). Compared to dense disparity matching approaches, the computation of these disparity maps is rather low cost in terms of computation time and allows to precisely locate some bounding boxes around vertical objects using \"u-disparity\" approach [29], like in Fig. 25.\n\nTo estimate the visibility distance, we have now to combine a disparity map of the road surface obtained using one of the previously described approaches with a contrast map. Because most distant objects on the road plane are on the horizon line, the scanning starts from this location. Within each neighborhood where a point of disparity is known the contrast is computed. The process stops when a contrast above 5% is met. The visibility distance is then the depth of the picture element with a contrast above 5%. This generic process has been patented [27].\n\nDisparity map Scanning Contrast computation C>5% Yes No Visibility distance Camera + INS Binocular camera Disparity map Scanning Contrast computation C>5% Yes No Visibility distance Camera + INS Binocular camera\n\nAn example of final result is given in Fig. 27. In Fig. 27a, the result of local contrasts above 5% is presented for a daytime foggy weather image. In Fig. 27b, the final result of the algorithm is shown. In particular, the disparity point, on which the visibility distance is computed, is represented with a black cross inside the white window. Finally, Figs. 28 and 29 show some results of mobilized visibility distance computations using stereovision in two fog video sequences (daytime and twilight fog) [28].\n\nOnce visibility distance has been determined, it can be used to adjust some parameters in other image processing algorithms. This section is devoted to such an application focusing on contrast restoration.\n\nAccording to (5), the contrast of images is drastically degraded and varies across the scene under daytime foggy weather. Consequently, advanced driver assistances relying on artificial vision and pattern analysis are no longer able to run properly. To mitigate this problem, we proposed to restore the contrast by inverting Koschmieder's Law to recover the value of the intrinsic intensity R at each point of the scene, such as:\n\nThen, according to the strategy used to approximate the depth distribution d of the road scene, different applications can be constructed.\n\nRoad departure prevention is usually based on a lane marking detector relying on a single camera mounted behind the windshield of the vehicle. In these conditions, most systems assume a flat road. This assumption can also be used to restore the contrast of the road surface. Thus, using ( 18) and ( 56), one obtains:\n\nDetails of the method are given in [24]. To illustrate the algorithm, the lane markings have been extracted in a foggy image with and without contrast restoration using the method described in [56] with the same setting (see Fig. 30). One can see that the operation range of the lane marking detector is enhanced thanks to the contrast restoration process.\n\nTo restore the contrast of the road scene and not only the contrast of the road surface, another approach is proposed. In this context, the depth distribution in the scene can be roughly modeled in two parts. A first part models the road surface which can be approximated by a plane like in the previous paragraph. A second part models the objects above the road surface. According to classical perspective geometrical representations, the depth of a scene point can be expressed as a function of the euclidian distance in the image plane between the corresponding pixel and the vanishing point (u h , v h ) [47]. Consequently, the depth d of a pixel with (u, v) coordinates can be inferred as:\n\nFigure 30: (a) Original foggy image used for lane markings detection using the method described in [56]; (b) image with restored contrast used for lane markings detection using the same parameters as the first image, i.e. a gradient threshold of 10 (blue: pixels likely to be lane markings, green: fitted lane marking, red: level of confidence on the fitting).\n\nwhere κ > l models the relative importance of the flat world against the vertical world. To correctly restore the contrast, according to the scene model given in the previous paragraph, the remaining task consists in finding the optimal values of κ and c. To do it, one solution is to solve the following equation using Powell's method:\n\nwhere c is a parameter which defined a clipping plane at d = λ c-v h which is used to limit the depth modeling errors near the horizon line and Q is a norm of the local normalized correlation between the original image and the restored image. Indeed, the normalized correlation score between the original and the restored versions of a neighborhood should remain high. A decreasing normalized correlation means that the content of the original and restored neighborhoods differ. More details about this method are given in [31]. Sample results are given in Fig. 31.\n\nInitialized with a small initial value of κ in (58), e.g. κ = 1.1λ, the principle of the simultaneous contrast restoration and obstacle detection algorithm is to progressively increase the value of κ and to detect the distorted areas. As soon as vertical objects are encountered, a local contrast distortion can be noticed. In this case, the vertical object causing the distortion is detected by \"u-v disparity\" stereovision approach, detailed in paragraph 5.5. The increase of κ can then be restarted until the desired final value, e.g. κ = 10λ, is reached. The algorithm is detailed in [30] and illustrated in Fig. 32 In this way, scattering is turned to our advantage allowing us to detect vertical objects, which can then be confirmed by another approach (stereovision in this case). Based on this principle, we plan to develop such novel algorithms aiming at recovering the third dimension of scenes using a single image by exploiting the scattering of the light by atmospheric particles.\n\nIn the previous sections, an approach to deal with adverse visibility conditions in daytime fog was presented: modeling, methods, experimental validation and applications. Admit-tedly there is still some work ahead to finalize and validate the applications, in a near future we would like to tackle other adverse meteorological or lighting conditions: night-time fog (Fig. 33c), rain (Fig. 33a), sun at grazing angle (Fig. 33b), snow fall, entrance or exit of tunnels, even though existing works on these subjects are quite rare. Consequently, to be able to develop new methods and applications, a lot of work is needed to model, simulate or reproduce these adverse conditions. However, as mentioned in the introduction, it is one of the keys to obtain a very important level of reliability of the sensor unit inside the vehicles and associated signal processing. Then, new systems for the vehicle using a camera could be developed. It would lead to increase the number of supported functions by the camera (rain sensor, fog sensor, etc.) and thus to reduce the cost of its installation in future vehicles. In this way, the deployment of such onerous systems could be facilitated.\n\nIn this chapter, methods have been presented to deal with daytime fog conditions in the future driver assistances using optical sensors. First of all, a daytime fog modeling has been recalled and new visibility distances have been proposed fitting well with these driving conditions. Two complementary methods have been presented aiming at estimating the visibility range using respectively one or two onboard cameras. Both methods are based on the definition of the meteorological visibility distance. The first technique, using a model of atmospheric scattering, detects and estimates the density of daytime fog by using a single camera. The second technique, using a generic property of the atmosphere, is able to estimate the visibility range under all meteorological conditions both in daytime and in night time by using a stereoscopic sensor or a single camera coupled with an inertial navigation system. In the future, these methods are likely to be used to provide drivers with an appropriate speed with respect to the visibility range, or to automate the fog lamps. Once the weather conditions have been determined, they can have been exploited to reduce the impact of adverse weather conditions on the operation of vision systems. We have thus proposed different solutions to enhance the performances of vision algorithms under foggy weather. Finally, we have tackled the problem to know if light scattering could be turned to our advantage, allowing us to develop novel perception algorithms. Methods are validated using actual fog images grabbed under various situations on the test track of Versailles Satory in France. In the future, we would like to apply this approach to other adverse conditions like night-time fog, rain, low angled sun, etc."
}