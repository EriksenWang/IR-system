{
    "title": "Towards Understanding the Limits of Gait Recognition",
    "publication_date": "1977",
    "authors": [
        {
            "full_name": "Zongyi Liu",
            "firstname": "Zongyi",
            "lastname": "Liu",
            "affiliations": [
                {
                    "organization": "Computer Vision and Image Informatics Laboratory Computer Science and Engineering, University of South Florida Tampa",
                    "address": {
                        "postcode": "33647"
                    }
                }
            ]
        },
        {
            "full_name": "Laura Malave",
            "firstname": "Laura",
            "lastname": "Malave",
            "affiliations": [
                {
                    "organization": "Computer Vision and Image Informatics Laboratory Computer Science and Engineering, University of South Florida Tampa",
                    "address": {
                        "postcode": "33647"
                    }
                }
            ]
        },
        {
            "full_name": "Adebola Osuntugun",
            "firstname": "Adebola",
            "lastname": "Osuntugun",
            "affiliations": [
                {
                    "organization": "Computer Vision and Image Informatics Laboratory Computer Science and Engineering, University of South Florida Tampa",
                    "address": {
                        "postcode": "33647"
                    }
                }
            ]
        },
        {
            "full_name": "Preksha Sudhakar",
            "firstname": "Preksha",
            "lastname": "Sudhakar",
            "affiliations": [
                {
                    "organization": "Computer Vision and Image Informatics Laboratory Computer Science and Engineering, University of South Florida Tampa",
                    "address": {
                        "postcode": "33647"
                    }
                }
            ]
        },
        {
            "full_name": "Sudeep Sarkar",
            "firstname": "Sudeep",
            "lastname": "Sarkar",
            "affiliations": [
                {
                    "organization": "Computer Vision and Image Informatics Laboratory Computer Science and Engineering, University of South Florida Tampa",
                    "address": {
                        "postcode": "33647"
                    }
                }
            ]
        }
    ],
    "abstract": "Most state of the art video-based gait recognition algorithms start from binary silhouettes. These silhouettes, defined as foreground regions, are usually detected by background subtraction methods, which results in holes or missed parts due to similarity of foreground and background color, and boundary errors due to video compression artifacts. Errors in low-level representation make it hard to understand the effect of certain conditions, such as surface and time, on gait recognition. In this paper, we present a part-level, manual silhouette database consisting of 71 subjects, over one gait cycle, with differences in surface, shoe-type, carrying condition, and time. We have a total of about 11,000 manual silhouette frames. The purpose of this manual silhouette database is twofold. First, this is a resource that we make available at\nwww.GaitChallenge.org\nfor use by the gait community to test and design better silhouette detection algorithms. These silhouettes can also be used to learn gait dynamics. Second, using the baseline gait recognition algorithm, which was specified along with the HumanID Gait Challenge problem, we show that performance from manual silhouettes is similar and only sometimes better than that from automated silhouettes detected by statistical background subtraction. Low performances when comparing sequences with differences in walking surfaces and time-variation are not fully explained by silhouette quality. We also study the recognition power in each body part and show that recognition based on just the legs is equal to that from the whole silhouette. There is also significant recognition power in the head and torso shape.",
    "full_text": "In the 1970's Cutting and Kozlowski, 1 using point light display based experiments patterned after those of Johansson's, 2 demonstrated the ability to recognize friends from gait. In computer vision, while gait analysis has been a research topic for a while, it is only recently that identification from gait has received attention and has become an active area of computer vision. [3][4][5][6][7][8][9][10][11][12][13][14][15] For biometics research, gait is usually refered to in its broadest sense to include both body shape and dynamics, i.e. any information that can be extracted from the video of a walking person to robustly identify the person under various condition variations. It is one biometric source that can be acquired at a distance, making it important for some early warning or monitoring applications that needs to perform recognition when the subject is far away. While one cannot expect it to be a perfect biometric source, it important to understand its scope. For some application scenarios, less than perfect recognition is better than nothing. However, before deploying gait base recognition systems, it is important to study the associated scientific questions such as: With what confidence level can we identify persons from their gait? Is gait good for verification or is it better for identification scenarios? What are the limitations of recognition from gait? What factors effect gait recognition and to what extent? Answers to these questions are not known.\n\nTo help answer these kinds of questions, the HumanID Gait Challenge Problem was formulated. 16,17 It consists of (i) a large gait database with 1870 sequences from 122 subjects, spanning 5 covariates: across view (about 30 degrees), across shoe-type, across briefcase, across surface types (grass/concrete), and across time; (ii) a set of twelve experiments to investigate the effect of five factors on performance. The five factors are studied both individually and in combinations. The results from the twelve experiments provide an ordering of the difficulty of the experiments. And, it consists of (iii) a baseline algorithm based on the Tanimoto distance between silhouettes, extracted by simple background differencing, to provide the needed performance benchmark. Some sample frames are shown in Figure 1.\n\nSend correspondences to Sudeep Sarkar, E-mail: sarkar@csee.usf.edu, Telephone: 1 813 974 2113 Table 1. Summary the top rank recognition for experiments A (viewpoint variation between probe and gallery), B (shoe-type variation), D (surface variation), and K (Time variation) in the Gait Challenge dataset. The numbers for the first two columns are as read from graphs in the cited papers.\n\nFusion (UMD) 19 DTW (UMD) 20 HMM (UMD) 21 Body Shape (CMU) 22 HMM (MIT) 23 Body (CAS) 24 Baseline (USF)\n\n16 A (view) 52% 78% 99% 87% 88% 70% 79% B (shoe) 40% 65% 89% 81% 75% 59% 66% D (surface) 20% 29% 36% 21% 25% 34% 29% K (time) 3% # subjects in gallery 71 71 71 71 71 71 71\n\nFor some of the key gait challenge experiments, Table 1 lists summary performances that have been so far reported in the literature by various gait recognition strategies, such those based on Hidden Markov Models (HMMs), Dynamic Time Warping (DTW), body shape matching, and shape moments. The listed performance numbers are the correct identification rates at the top most rank. This is a standard performance metric used in biometrics 18 for the identification scenario, where one is interested to find a match to a given probe from whole the gallery set, i.e. one-to-many match. (For the verification scenario, where one is interested in matching one probe to one gallery (one-to-one match), the performance is specified in terms of standard false alarm and detection rates. In general, identification is considered to be a harder problem than verification.) We see that performance of the baseline gait recognition algorithm is quite effective and competitive with other algorithms. Another observation of particular interest, is the significant effect of change in surface type on the identification rate; this effect is consistent across different types of gait recognition algorithms. For the baseline algorithm, we also see the significant effect of time variation of about six months. This effect of time on gait recognition has been documented by others, but on different data sets taken indoors. When the difference in time between gallery (the prestored template) and probe (the input data) is in the order of minutes, the identification performance ranges from 91% to 95%, 9,13,15 whereas the performances drop to 30% to 45% when the differences are in the order of months and days 11,13,19 for similar sized datasets.\n\nSince all the algorithms rely on silhouette as the low-level representation of choice, it is reasonable to suspect the flaky low-level processes for the low performances across surfaces and time. These experiments involve either a change in the background or substantial change in illumination, which very likely impact silhouette quality. Also, the high recognition rates that we see for the other covariates, such as view or shoe type, might be due to error correlation in the silhouettes, such as those due to shadows or holes. The error correlations is expected to be high since they involve data collected over a short time period of each other. To unravel these factors, we present results with part-level manual silhouettes. One conclusion of this work is that the quality of automated silhouettes, detected using standard techniques, involving background difference with Mahanalobis distance in the color space, does not seem to be the limiting factor in gait recognition. We arrive at this conclusion based on results with manually specified silhouettes. Second conclusion is that the recognition from legs is almost equal to that from the whole silhouette. The recognition from torso shape is also significant.\n\nFor the sake of gaining insight into the relationship between recognition and silhouette quality, we manually created silhouettes over one gait cycle for 71 subjects under 4 different conditions, exhibiting the impact of cross shoe-type, surface, and time, respectively. This cycle was chosen to begin at the right heel strike phase of the walking cycle through to the next right heel strike, thus including one complete walking cycle. We attempted to pick this gait cycle from the same 3D location in each sequence, whenever possible. In addition, we tried to exclude the portion that included the calibration box with high contrast (see Figure 1), which frequently leads to high background subtraction errors.\n\nWe not only mark a pixel as being from the background or subject, but provided more detailed specifications in terms of body parts too. We explicitly labeled the head, torso, left arm, right arm, left upper leg, left lower leg, right upper leg, and right lower leg using different colors. Figure 2 shows some examples of part-level manual silhouettes corresponding to the original color images in the left column. Quality control checks looked for miscolored parts and backgrounds, randomly colored isolated pixels, errors on the boundary of the body, and missed body parts. Some of the difficulties encountered during the creating process include low-image quality due to varying overall intensity, occlusion of feet in the grass sequences, similarity of dark skin tones of some subjects with the background, frequent occlusion of the right arm, and the presence of dark or baggy clothing, which made it hard to delineate various body parts. However, despite these difficulties we were able to create pretty consistent quality silhouettes, as judged visually by another subject, across the subjects.\n\nFor recognition, the manual silhouettes are first height scaled so that they are all 128 pixels tall, which is around the average original resolution. Figure 3 shows images from the same person before and after scaling. In addition, the silhouettes are also centered to the middle of the frames so that frames can be compared by simple projection. Note that these simple operations, in essence, helps us in arriving at Kendall's notion of pre-shape by removing translation and scaling. Since all the subjects are upright and the camera is stationary, there is no need for rotation normalization before silhouette shape matching.\n\nWe compare sequences, each with multiple but not necessarily equal gait cycles, using the baseline gait recognition algorithm that was specified along with the Gait Challenge problem. 17 The similarity computation is based on spatio-temporal correlation. Let the probe, consisting of M frames, and the gallery, consisting of N frames, be denoted by\n\nFirst, the probe (input) sequence is partitioned into subsequences,  each roughly over one gait period, N Gait . Gait periodicity is estimated based on periodic variation of the count the number of foreground pixels in the lower part of the silhouette in each frame over time. This number will reach a maximum when the two legs are farthest apart (full stride stance) and drop to a minimum when the legs overlap (heels together stance).\n\nSecond, each of these probe subsequences, S Pk = {S P (k), ••• , S P (k + N Gait )}, is cross correlated with the given gallery sequence, S G .\n\nwhere, the similarity between two image frames, S(S P (i), S G ( j)), is defined to be the Tanimoto similarity between the silhouettes, i.e. the ratio of the number of common pixels to the number of pixels in their union. The overall similarity measure is chosen to be the median value of the maximum correlation of the gallery sequence with each of these probe subsequences. The strategy for breaking up the probe sequence into subsequences allows us to address the case when we have segmentation errors in some contiguous sets of frames due to some background subtraction artifact or due to localized motion in the background.\n\nThe above strategy is effective for gait recognition when compared with performances of more complicated strategies, as we can see in Table 1. Note that both body shape and dynamics contribute to the similarity measure.\n\nThe baseline similarity computation strategy outlined above will not work, as is, for computing similarity from manual silhouettes, since they are defined over only one gait cycle. The correlation part is not needed. We simply linearly timewarp the sequences, but with the same frame-to-frame Tanimoto similarity measure.\n\nLet the two silhouette sequences be denoted by S 1 = {S 1 (1), ••• , S 1 (M)} and S 2 = {S 2 (1), ••• , S 2 (N)}. Without loss of generality, let M ≤ N. Then the similarity between the two sequences is\n\nwhere, the similarity between two image frames, S(S 1 (i), S 2 ( j)), is defined to be the Tanimoto similarity between the silhouettes, i.e. the ratio of the number of common pixels to the number of pixels in their union. The overall similarity measure is the warped distance between the two sequences. Figure 4 illustrates this warping. Note that since the starting and ending stances of the two sequences are guaranteed to match, the chosen warping strategy is reasonable.\n\nWe compare the recognition from manual silhouettes with recognition based on automated silhouettes, both over one gait cycle and multiple gait cycles. The comparison over one gait cycle is, of course, the fairer of the two. We match each probe sequence to the gallery sequences, thus obtaining a similarity matrix with size that is the number of probe sequences by the gallery size. Following the pattern of the FERET evaluations, 18 we measure performance for both identification and verification scenarios using cumulative match characteristics (CMCs) and receiver operating characteristics (ROCs), respectively. In the identification scenario, the task is to identify a given probe to be one of the given gallery images.\n\nTo quantify performance, we sort the gallery images based on computed similarities with the given probe. In terms of the similarity matrix, this would correspond to sorting the rows of the similarity matrix. If the correct gallery image corresponding to the given probe occurs within rank k in this sorted set, then we have a successful identification at rank k. A cumulative match characteristic plots these identification rates (P I ) against the rank k. In the verification scenario, a  system either rejects or accepts if a person is who they claim to be. Operationally, a person presents (1). a new signature, the probe, and (2). an identity claim. The system then compares the probe with the stored gallery sequence that corresponds to the claimed identity. The claim is accepted if the match between the probe and gallery is above an operating threshold, otherwise it is rejected. For a given operating threshold, there is a verification rate (or detection rate) and a false accept rate. Changing the operating threshold can change the verification and false accept rates. The complete set of verification and false accept rates is plotted on a receiver operating characteristic (ROC).\n\n0 1 2 3 4 5 0 10 20 30 40 70 80 90 100 False Alarm Rate Verification Rate Automated, Multi-cycle Automated, Single-cycle Manual, Single-cycle 0 1 2 3 4 5 0 10 20 30 40 50 60 70 80 90 100 False Alarm Rate Verification Rate Automated, Multi-cycle Automated, Single-cycle Manual, Single-cycle (a) (b) 0 1 2 3 4 5 0 10 20 30 40 50 60 70 80 90 100 False Alarm Rate Verification Rate Automated, Multi-cycle Automated, Single-cycle Manual, Single-cycle 0 1 2 3 4 5 0 10 20 30 40 50 60 70 80 90 100 False Alarm Rate Verification Rate Automated, Multi-cycle Automated, Single-cycle Manual Single-cycle First, we consider performance based on the whole silhouette. Figures 5 and 6 plot the Cumulative Match Curves (CMCs) for the first 5 rank and the Receiver Operating Characteristics (ROCs) up to the 5% false alarm rate, respectively. The results are summarized in Table 2. We see that the identification rate at rank 1 of the automated multi-cycle silhouettes is 81%, 39%, 78%, and 15% for cross shoe-type, surface, briefcase, time, respectively. This rates are, of course, higher than that obtained with just single cycle gait sequences. For the automated single cycle silhouettes, the number is 54%, 24%, 37%, and 9%, respectively. And for the manual single cycle silhouettes, the number is 49%, 20%, 12%, and 12%, respectively. The non-parametric Mcnemar's test confirms that the differences of the identification rates are statistically significant, except for the observed differences for the experiment across time. Comparing the single cycles performances, we see that the recognition with manual silhouettes outperforms the manual silhouette performance for all the experiments except for the time difference experiment. The biggest improvement is for the experiment comparing sequences with briefcase with sequences without briefcase. The removal of the briefcases in the manual silhouettes possibly contributed to higher recognition rates. Another observation worth noting is that performances when comparing sequences with differences in walking surface condition and time is still low with manual silhouettes, suggesting that silhouette quality does not fully explain the low performances.\n\nSecond, we consider recognition from parts. What are the relative recognition rates based on silhouette portions from individual body parts, i.e. legs, hands, torso, or their combination? The manual silhouettes, which are specified at part-level, readily facilitates such as study. Figure 7 shows the CMCs upto rank 5 and Table 3 summarizes the top  rank identification rates and the verification rate for a 1% false alarm rate, when considering different body portions for the different experiments. We see that the recognition from just the legs match the recognition from the full body. The recognition from the combination of legs and hands, which convey the dynamic component of gait, somewhat outperforms the recognition from full body silhouettes. Also notice that significant recognition power also exists in the head and torso portions, which basically conveys body shape.\n\nWe presented a large manual silhouettes database for gait recognition and modeling research. The database contains part-level silhouettes, over one gait cycle, for about 70 subjects, spanning variation in surface, shoe, carrying condition, and time. This database could be used by others for a variety of purposes, such as designing better silhouette detection algorithms, learning gait kinematic models, and studying human gait variations. It is marker-less, normal, non-threadmill, gait data that is essentially uncorrupted by placement of markers and other intrusive or restrictive devices. It has been noted by many researchers that the use of markers or threadmill changes gait.\n\nGait recognition experiments with the manual silhouettes suggest several interesting conclusions. First, recognition with manual silhouettes improved over automated silhouettes. However, while the improvement was the largest for comparing sequences with and without briefcase, it was not large for surface and time variation experiments. This suggests that automated silhouette quality is only partly to explain the low performance on these experiments. Second, recognition from just silhouette portions from the legs is almost equal to that from the whole body. The head and torso portions, which capture the static body shape, also has recognition power. This seems to suggest that both gait dynamics and body shape contribute towards recognition from gait video."
}