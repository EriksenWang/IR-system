{
    "title": "Real-Time Prosody-Driven Synthesis of Body Language",
    "publication_date": "2002",
    "authors": [
        {
            "full_name": "Sergey Levine",
            "firstname": "Sergey",
            "lastname": "Levine",
            "affiliations": []
        },
        {
            "full_name": "Christian Theobalt",
            "firstname": "Christian",
            "lastname": "Theobalt",
            "affiliations": []
        },
        {
            "full_name": "Vladlen Koltun",
            "firstname": "Vladlen",
            "lastname": "Koltun",
            "affiliations": []
        },
        {
            "full_name": "Stanford University",
            "firstname": "Stanford",
            "lastname": "University",
            "affiliations": []
        }
    ],
    "abstract": "Human communication involves not only speech, but also a wide variety of gestures and body motions. Interactions in virtual environments often lack this multi-modal aspect of communication. We present a method for automatically synthesizing body language animations directly from the participants' speech signals, without the need for additional input. Our system generates appropriate body language animations by selecting segments from motion capture data of real people in conversation. The synthesis can be performed progressively, with no advance knowledge of the utterance, making the system suitable for animating characters from live human speech. The selection is driven by a hidden Markov model and uses prosody-based features extracted from speech. The training phase is fully automatic and does not require hand-labeling of input data, and the synthesis phase is efficient enough to run in real time on live microphone input. User studies confirm that our method is able to produce realistic and compelling body language.",
    "full_text": "Interactions between human characters are often the most interesting aspects of networked virtual environments. Modern realtime graphics technology can endow these characters with a photorealistic appearance, but is still unable to generate the vast variety of motions that real human beings exhibit. Gestures and speech coexist in time and are tightly intertwined [McNeill 1992], but current * e-mail:{svlevine,theobalt,vladlen}@cs.stanford.edu input devices are far too cumbersome to allow body language to be conveyed as intuitively and seamlessly as it would be in person. Current virtual worlds frequently employ keyboard or mouse commands to allow participants to utilize a small library of pre-recorded gestures, but this mode of communication is unnatural for extemporaneous body language. Given these limitations on direct input, body language for human characters must be synthesized automatically in order to produce consistent and believable results.\n\nWe present a data-driven method that automatically generates body language animation from the prosody of the participant's speech signal. The system is trained on motion capture data of real people in conversation, with simultaneously recorded audio. Our main contribution is a method for modeling the gesture formation process that is appropriate for progressive real-time synthesis, as well as an efficient algorithm that uses this model to produce an animation from a live speech signal, such as a microphone.\n\nTo generate the animation, we select appropriate gesture subunits from the motion capture training data based on prosody cues in the speech signal. Prosody cues are known to correspond well to emotional state [Adolphs 2002;Schr√∂der 2004] and emphasis [Terken 1991]. Gesture has also been observed to reflect emotional state [Wallbot 1998;Montepare et al. 1999] and highlight emphasized phrases [McNeill 1992]. The selection is performed by a specialized hidden Markov model (HMM), which ensures that the gesture subunits transition smoothly and are appropriate for the tone of the current utterance. In order to synthesize gestures in real time, the HMM predicts the next gesture and corrects mispredictions. By using coherent gesture subunits with appropriate transitions enforced by the HMM, we ensure a smooth and realistic animation. The use of prosody for driving the selection of motions also ensures that the synthesized animation matches the timing and tone of the speech.\n\nWe also present four user studies that compare synthesized animations for novel utterances with the original motion capture sequences corresponding to each utterance. The results of the studies, presented in Section 10, confirm that our method produces compelling body language and generalizes to different speakers.\n\nBy animating human characters in real time with no additional input, our system can seamlessly produce plausible body language for human-controlled characters, thus improving the immersiveness of interactive virtual environments and the fluidity of virtual conversations. To the best of our knowledge, this is the first proposed audio-driven method for body language synthesis that can generate animations from live speech.\n\nAlthough no system has been proposed that both synthesizes fullbody gestures and operates in real time on live voice input, a number of methods have been devised that synthesize either full-body or facial animations from a variety of inputs. Such methods often aim to animate Embodied Conversational Agents (ECAs), which operate on a pre-defined script or behavior tree [Cassell 2000], and therefore allow for concurrent planning of synthesized speech and gesture to ensure co-occurrence. Often these methods rely on the content author to specify gestures as part of the input, using a concise annotation scheme [Hartmann et al. 2002;Kopp and Wachsmuth 2004]. New, more complete annotation schemes for gestures are still being proposed [Kipp et al. 2007], and there is no clear consensus on how gestures should be specified. Some higherlevel methods also combine behavioral planning with gesture and speech synthesis [Cassell et al. 1994;Perlin and Goldberg 1996], with gestures specified as part of scripted behavior. However, all of these methods rely on an annotation scheme that concisely and completely specifies the desired gestures. Stone et al. [2004] avoid the need for annotation of the input text with a data-driven method, which re-arranges pre-recorded motion capture data to form the desired utterance. However, this method is limited to synthesizing utterances made up of pre-recorded phrases, and does require the hand-annotation of all training data. We also employ a data-driven method, but our system is able to automatically retarget appropriate pre-recorded motion to an arbitrary utterance.\n\nSeveral methods have been proposed that operate on arbitrary input, such as text. Cassell et al. [2001] propose an automatic rule-based gesture generation system for ECAs using natural language processing, while Neff et al. [2008] use a probabilistic synthesis method trained on hand-annotated video. However, both of these methods rely on concurrent generation of speech and gesture from text. Text does not capture the emotional dimension that is so important to body language, and neither text communication, nor speech synthesized from text can produce as strong an impact as real conversation [Jensen et al. 2000].\n\nAnimation directly from voice has been explored for synthesizing facial expressions and lip movements, generally as a datadriven approach using some form of probabilistic model. Bregler et al. [1997] propose a video-based method that reorders frames in a video sequence to correspond to a stream of phonemes extracted from speech. This method is further extended by Brand [1999] by retargeting the animation onto a new model and adopting a sophisticated hidden Markov model for synthesis. Hidden Markov models are now commonly used to model the relationship between speech and facial expression [Li and Shum 2006;Xue et al. 2006]. Other automatic methods have proposed synthesis of facial expressions using more sophisticated morphing of video sequences [Ezzat et al. 2002], physical simulation of muscles [Sifakis et al. 2006], or by using hybrid rule-based and data-driven methods [Beskow 2003].\n\nAlthough speech-based synthesis of facial expressions is quite common, it does not generally utilize vocal prosody. Since facial expressions are dominated by mouth movement, many speechbased systems use techniques similar to phoneme extraction to select appropriate mouth shapes. However, a number of methods have been proposed that use speech prosody to model expressive human motion beyond lip movements. Albrecht et al. [2002] use prosody features to drive a rule-based facial expression animation system, while more recent systems apply a data-driven approach to generate head motion from pitch [Chuang and Bregler 2005] and facial expressions from vocal intensity [Ju and Lee 2008]. Incorporating a more sophisticated model, Sargin et al. [2007] use prosody features to directly drive head orientation with a HMM. Although these methods only animate head orientation from prosody, Morency et al. [2007] suggest that prosody may also be useful for predicting gestural displays.\n\nOur system selects appropriate motions using a prosody-driven HMM reminiscent of the above techniques, but with each output segment corresponding to an entire gesture subunit, such as a \"stroke\" or a \"hold.\" This effectively reassembles the training motion segments into a new animation. Current techniques for assembling motion capture into new animations generally utilize some form of graph traversal to select the most appropriate transitions [Arikan and Forsyth 2002;Kovar et al. 2002;Lee et al. 2002]. Our system captures a similar notion in the structure of the HMM, with high-probability hidden-state transitions corresponding to transitions that occur frequently in the training data. While facial animation synthesis HMMs have previously used remapping of observations [Brand 1999], or conditioned the output animations on the input audio [Li and Shum 2006], we map animation states directly to the hidden states of our model. This allows us to design a simpler system that is able to deal with coherent gesture subunits without requiring them to directly coincide in time with audio segments.\n\nWhile the methods described above are able to synthesize plausible body language or facial expression for ECAs, none of them can generate full-body animations from live speech. Animating human-controlled characters requires real-time speeds and a predictive model that handles arbitrary speech without the need for manual annotation or knowledge of the entire utterance. Such a model constitutes the main contribution of this work.\n\nThe most widely-used taxonomy for gestures was proposed by Mc-Neill [1992], though he later suggested that a more continuous classification would be more appropriate [2005]. McNeill's original taxonomy consists of four gesture types: iconics, metaphorics, deictics, and beats. Iconics present images of concrete objects or actions, metaphorics represent abstract ideas, deictics serve to locate entities in space, and beats are simple, repetitive motions meant to emphasize key phrases [McNeill 1992].\n\nMcNeill noted that \"beats tend to have the same form regardless of content,\" and that they are generally used to highlight emphasized words. He also observed that beats constitute half of all gestures, and nearly two-thirds of gestures accompanying nonnarrative speech [McNeill 1992]. Since prosody correlates well to emphasis [Terken 1991], it should correspond to the emphasized words that beats highlight, making it particularly useful for synthesizing this type of gesture.\n\nIn addition, there is evidence that prosody carries much of the emotive content of speech [Adolphs 2002;Schr√∂der 2004], and that emotional state is often reflected in body language [Wallbot 1998;Montepare et al. 1999]. Therefore, we expect a prosodydriven system to produce accurately timed and appropriate beats, as well as more complex abstract or emotion-related gestures with the appropriate emotive content. The accompanying video presents examples of synthesized animations for emphasized and emotional utterances.\n\nWe capture prosody using the three features that it is most commonly associated with: pitch, intensity, and duration. Pitch and intensity have previously been used to drive expressive faces [Chuang and Bregler 2005;Ju and Lee 2008], duration has a natural correspondence to the rate of speech, and all of these aspects of prosody are informative for determining emotional state [Scherer et al. 1991]. While semantic meaning also corresponds strongly to gesture, we do not attempt to interpret the utterance. This approach has some limitations, as discussed in Section 11, but is more appropriate for online, real-time synthesis. Extracting semantic meaning from speech to synthesize gesture without knowledge of the full utterance is difficult because it requires a predictive speech interpreter, while prosody can be obtained efficiently without looking ahead in the utterance.\n\nMotion Capture Speech Segmentation Segmentation Clustering Feature Extraction Combined Motion/Speech State Stream HMM Parameter Estimation\n\nDuring the training phase, our system processes a corpus of motion capture and audio data to build a probabilistic model that correlates gesture and prosody. The motion capture data is processed by extracting gesture subunits, such as \"strokes\" and \"holds.\" These subunits are then clustered to identify recurrences of the same motion, as described in Section 5. The training speech signal is processed by extracting syllables and computing a set of prosody-based features on each syllable, as described in Section 6. Finally, the parameters of a hidden Markov model, which is described in Section 7, are estimated directly from the two resulting state-streams. This process is summarized in Figure 2. The HMM can then be used to automatically synthesize appropriate body language for a live novel utterance in real time, using the algorithm described in Section 8. The correlation between speech and body language is inherently a many-to-many mapping: there is no unique animation that is most appropriate for a given utterance [Brand 1999]. This ambiguity makes validation of synthesis techniques difficult. Since the only way to judge the quality of a synthesized body language animation is by subjective evaluation, we conducted a survey to validate our method. We present the results of this study in Section 10.\n\nThe final training set for our system consists of 30 minutes of motion capture data with accompanying speech, divided into six scenes. An additional 15 minute training set from a different speaker was also used in the user study. Each scene in each set was excerpted from an extemporaneous conversation, ensuring that the body language was representative of real human interaction. Typical conversation topics were travel, movies, and politics. The actors were asked to avoid prominent iconic gestures that might depend strongly on semantics, but no other instructions were given.\n\nThe motion capture data is mapped onto a 14-joint skeleton and segmented into gesture unit phases, henceforth referred to as gesture subunits. These segments are then clustered to identify recurrences of similar motions.\n\nCurrent motion segmentation methods identify broad categories of motion within a corpus of motion capture data, such as walking and sitting [Barbiƒç et al. 2004;M√ºller et al. 2005], while much of the existing work on data-driven gesture animation segments training data manually [Stone et al. 2004;Neff et al. 2008]. Perceptually distinct gesture subunits are not as dissimilar as these broad categories, while manual annotation does not scale gracefully to large amounts of data. Therefore, we propose a new method for segmentation inspired by the current understanding of gesture structure.\n\nGestures consist of pre-stroke hold, stroke, post-stroke hold, and  retraction phases [McNeill 1992;Kendon 2004]. Such phases have previously been used to segment hand gestures [Majkowska et al. 2006]. From this we deduce that a gesture unit consists of alternating periods of fast and slow motion. Therefore, we place segment boundaries when we detect a shift from slow motion into fast motion, or vice versa, using an algorithm inspired by Fod et al. [2002]. For each frame, we compute z = 14 j=1 uj||œâj|| 2 , the weighted sum of the squared magnitudes of the angular velocities of the joints, denoted by œâj for joint j. The weights uj correspond to the estimated perceptual importance of joints, with high-influence joints such as the pelvis, abdomen, and hips weighted higher, and low-influence joints such as the hands weighted lower. To avoid creating small segments due to noise, we only create segments that exceed either a minimum length or a minimum limit on the integral of z over the segment. To avoid unnaturally long still segments during long pauses, we place an upper bound on segment length. The motions of the head, arms, and lower body do not always coincide, so we segment these three body sections separately and henceforth treat them as separate animation streams. The joints that constitute each of the sections are illustrated in Figure 3. Our final training set contains 2542 segments for the head, 2388 for the arms, and 1799 for the lower body.\n\nOnce the motion data has been segmented, we cluster the segments to identify recurring gesture subunits. Our clustering algorithm is based on Ward hierarchical clustering [Ward 1963], though a variety of methods may be appropriate. Segments are compared according to a three-part distance function that takes into account length, starting pose, and a fixed-size \"summary\" of the dynamics of the segment. The \"summary\" holds some measure of velocity and maximum and average displacement along each of the axes for a few key body parts in each section, as in Figure 3. The head section uses the rotation of the neck about each of the axes. The arm section uses the positions of the hands, which often determine the perceptual similarity of two gestures. The lower body uses the positions of the feet relative to the pelvis, which capture motions such as the shifting of weight. For the arms and lower body, velocity and maximum and average displacement each constitute a six-dimensional vector (three axes per joint), while for the head this vector has three dimensions. Each vector is rescaled to a logarithmic scale, since larger gestures can tolerate greater variation while remaining perceptually similar. Using six-dimensional vectors serves to de-emphasize the unused hand in one-handed gestures: if instead a three-dimensional vector for each hand was rescaled, small motions in the unused hand would be weighted too heavily on the logarithmic scale relative to the more important large motion of the active hand. To compare two summaries, we use the sum of the distances between the three vectors. The full training set was clustered into 20 gesture subunits for the head, 45 for the arms, and 6 for the lower body.\n\nTo obtain prosody features, we continuously extract pitch and intensity curves from the audio stream. After segmentation, these curves are used to compute a concise prosody descriptor for each audio segment, describing the inflection, intensity, and duration of the syllable. Pitch is extracted using the autocorrelation method, as described in [Boersma 1993], and intensity is extracted by squaring waveform values and filtering them with a Gaussian analysis window. For both tasks, we use components of the Praat speech analysis tool [Boersma 2001].\n\nGesture strokes have been observed to consistently end at or before, but never after, the prosodic stress peak of the accompanying syllable. This is referred to as one of the gesture \"synchrony rules\" by McNeill [1992]. Therefore, we segment the audio signal by syllables, and compute feature values on each syllable. For efficient segmentation, we used a simple algorithm inspired by Maeran et al. [1997], which identifies peaks separated by valleys in the intensity curve, under the assumption that distinct syllables will have distinct intensity peaks. In order to segment the signal progressively in real time, we identify a new syllable as soon as the potential peak of the next syllable is discovered. Because of this, syllable observations are issued at or before the peak of the next syllable, which allows us to more easily follow the synchrony rule.\n\nIn order to train the system on a small corpus of training data, we limit the size of the observation state space by using a small set of discrete features, rather than the more standard mixture of Gaussians approach. As described in Section 3, we utilize pitch, intensity, and the duration of syllables. Pitch is described using two binary features indicating the presence or absence of significant upward or downward inflection, corresponding to one standard deviation above the mean. Intensity is described using a trinary feature indicating silence, standard intensity, or high intensity, corresponding to half a standard deviation above the mean. Length is also described using a trinary feature, with \"long\" segments one deviation above the mean and \"short\" segments half a deviation below. The means and deviations of the training sequence are established prior to feature extraction. During synthesis, the means and deviations are estimated incrementally from the available audio data as additional syllables are observed.\n\nPreviously proposed HMM-driven animation methods generally use temporally aligned animation and speech segments. The input audio is remapped directly to animation output [Brand 1999], or is coupled to the output in a more complex manner, such as conditioning output and transition probabilities on the input state [Li and Shum 2006]. The HMM itself is trained with some variation of the EM algorithm [Li and Shum 2006;Xue et al. 2006].\n\nSince our input and output states are not temporally aligned, and since each of our animation segments corresponds to a meaningful unit, such as a \"stroke\" or \"hold,\" we take a different approach and map animation segments directly to the hidden states of our model, rather than inferring hidden structure with the EM algorithm. This allows us greater control in designing a specialized system for dealing with the lack of temporal alignment even when using a small amount of training data, though at the expense of being unable to infer additional hidden structure beyond that provided by the clustering of gesture subunits.\n\nThe process of gesture subunit formation consists of the selection of appropriate motions that correspond well to speech and line up in time to form a continuous animation stream. As discussed in Section 6, gesture strokes terminate at or before syllable peaks, and syllable observations arrive when the peak of the next syllable is encountered. In our training data, less than of motion segments did not contain a syllable observation, so we may assume that at least one syllable will be observed in every motion, though we will often observe more. Therefore, syllable observations provide a natural discretization for continuous gesture subunit formation, allowing us to model this process with a discrete-time HMM.\n\nUnder this discretization, we assume that gesture formation is a Markov process in which Ht, the animation state at syllable observation Vt, depends only on Vt and the previous animation state Ht-1. As discussed in Section 6, Vt contains a small set of discrete prosody features, and may be expressed as the index of a discrete observation state, which we will denote vj. We define the animation state Ht = {At, œÑt(i)}, where At = mi is the index of the motion cluster corresponding to the desired motion, and œÑt(i) is the fraction of that motion which has elapsed up to the observation Vt. This allows motions to span multiple syllables.\n\nTo prevent adjacent motions from overlapping, we could use a temporal scaling factor to ensure that the current motion terminates precisely when the next motion begins. However, during online synthesis, we have no knowledge of the length of future syllables, and therefore would not be able to accurately predict the scaling factor. Instead, we interrupt a motion and begin a new one when it becomes unlikely that another syllable will be observed within that motion, thus ensuring that motions terminate at syllable observations and do not overlap. While this reduces the quality of the animation, it allows the gestures strokes to be synchronized to syllable peaks without time warping. In practice, segments were interrupted on average 77% the way to completion on 10 minutes of novel utterances from typical conversations, indicating that most segments run almost to completion, and the discontinuity resulting from the synchronizing interruption is minor.\n\nIn order to compactly express the parameters of the gesture model, we first note that œÑ evolves in a very structured manner. If observation Vt corresponds to the continuation of the current motion mi, then At = At-1 = mi and œÑt(i) = œÑt-1(i) + œÑt(i), where œÑt(i) the fraction of the motion mi which has elapsed since the previous observation. If instead the current motion terminates at observation Vt and is succeeded by At = mj, then œÑt(j) = 0. Let œÑ t (i) = œÑt-1(i) + œÑt(i), then œÑt is completely determined by œÑ t and At, except in the case when motion mi follows itself, and At = At-1 = mi but œÑt(i) = 0. To disambiguate this case, we introduce a start state si and a continuation state ci for each motion cluster mi, so that At may take on either si or ci (we will denote an arbitrary animation state as aj). This approach is similar to the \"BIO notation\" (beginning, inside, outside) used in semantic role labeling [Ramshaw and Marcus 1995]. Under this new formulation, œÑt(i) = œÑ t (i) when At = ci and At-1 = ci or si. Otherwise, œÑt(i) = 0. Therefore, œÑt is completely determined by œÑ t and At.\n\nThe transition probabilities for At are a function of At-1 = mi and œÑ t (i). While this distribution may be learned from the training data, we can simplify it considerably. Although the precise length of a motion may vary slightly for syllable alignment, we assume that the transitions out of that motion do not depend this length. Therefore, we can define Tc i as the vector of transition probabilities out of a motion mi: Tc i ,s j = P (At = sj|At-1 = ci, At = ci).\n\nWe can also assume that the continuation of mi depends only on œÑ t (i), since, if At = ci, At-1 = ci or si, providing little additional information. Intuitively, this seems reasonable, since encountering another observation within a motion becomes less probable as the motion progresses. Therefore, we can construct the probabilities of transitions from ci as a linear combination of Tc i and ec i , the guaranteed transition into ci, according to some function fi of œÑ t (i):\n\nWe now define the vector Ts i as the distribution of transitions out of si, and construct the full transition matrix T = [Ts 1 , Tc 1 , ..., Ts n , Tc n ], where n is the total number of motion clusters. The parameters of the HMM are then completely specified by the matrix T , a matrix of observation probabilities O given by Oi,j = P (vi|aj), and the interpolation functions f1, f2, ..., fn.\n\nWe estimate the matrices directly from the training data by counting the frequency of transitions and observations. When pairing observations with animation states, we must consider the animation state that is active at the end of the next observation, as shown in Figure 4. When a motion mi on syllable Vt, we must the next animation segment. However, if we associate Vt with the current segment, the observed evidence would indicate that ci is the most probable state, which is not the case. Instead, we would like to predict the next state, which is precisely the state that is active at the end of the next syllable Vt+1.\n\nThe value of an interpolation function, fi(œÑt(i)), gives the probability that another syllable will terminate within motion mi after the current one, which terminated at point œÑt(i). Since motions are more likely to terminate as œÑt(i) increases, we may assume that fi is monotonically decreasing. From empirical observation of the distribution of œÑ values for the final syllables in each motion segment, we concluded that a logistic curve would be well suited for approximating fi. Therefore, the interpolation functions are estimated by fitting a logistic curve to the œÑ values for final syllables within each training motion cluster by means of gradient descent.\n\nThe model assembled in the training phase allows our system to animate a character in real time from a novel live speech stream. Since the model is trained on animation segments that follow audio segments, it is able to predict the most appropriate animation as soon as a new syllable is detected. As noted earlier, gesture strokes terminate at or before the intensity peak of the accompanying sylla- [McNeill 1992]. The syllable segmentation algorithm detects a syllable in continuous speech when it encounters the syllable peak of the next syllable, so new gestures begin at syllable peaks. Consequently, the previous gesture ends at or before a syllable peak.\n\nPrevious Update Next Update Syllable Observation Vt A t-1 Œ± t-1 , œÑt-1 Forward Direct Probabilities Probabilities Update: Equations 2,4 Update: Equation 5 Œ≥t Œ±t, œÑt Œ±t √ó Œ≥t At Stochastic Sampling At is the t th animation state, Vt is the t th observation, Œ±t is the forward probability vector, œÑt is the vector of expected œÑ values, and Œ≥t is the direct probability vector.\n\nTo synthesize the most probable and coherent animation, we compute a vector of forward probabilities along with direct probabilities based on the previously selected hidden state. The forward probabilities carry context from previous observations, while the direct probabilities indicate likely transitions given only the last displayed animation and current observation. Together, these two vectors may be combined to obtain a prediction that is both probable given the speech context and coherent given the previously displayed state, resulting in animation that is both smooth and plausible. The synthesis process is illustrated in Figure 5.\n\nGiven the parameters of a hidden Markov model and a sequence of syllable-observations V2, ..., Vt}, the most probable hidden state at time t can be efficiently determined from forward probabilities [Rabiner 1989]. Since transition probabilities depend on the value œÑt(i) at the current observation, we must take it into account when updating forward probabilities. To this end, we maintain both a vector of forward probabilities Œ±t, and a vector of the expected values E(œÑt(i)|At = ci) at the t th observation.\n\nAs before, we define a vector œÑ t (i) = œÑt-1(i) + œÑt(i). Using this vector, computing the transition probabilities between any two states at observation Vt is analogous to Equation 1:\n\nWith this formula for the transition between any two hidden states, we can use the usual update rule for Œ±t:\n\nwhere Œ∑ is the normalization value. Once the forward probabilities are computed, œÑt must also be updated. This is done according to the following update rule:\n\n\"Motorcycles become kind of a fully immersive experience, where the sound of it, the vibration, the seating position, it all matters.\" Since only si and ci can transition into ci, P (At = ci, At-1) = 0 if At-1 = ci and At-1 = si. If At-1 = si, this is the first event during this motion, so the previous value œÑt-1(i) must have been zero. Therefore, E(œÑt(i\n\nTherefore, we can reduce Equation 3 to:\n\nOnce we compute the forward probabilities and œÑt, we could obtain the most probable state from Œ±t(i). However, choosing the next state based solely on forward probabilities would produce an erratic animation stream, since the most probable state at a given observation need not follow smoothly from the most probable state at the previous observation.\n\nSince we have already displayed the previous animation At-1, we can estimate current state from only the previous state and current observation to ensure a high-probability transition, and thus a coherent animation stream. Given œÑ t (i) = œÑt-1(i) + œÑt(i) for currently active motion i, we compute transition probabilities just as we did in Section 7.2. If the previous state is the start state si, transition probabilities are given directly as (At|At-1 = si) = Ts i ,A t . If the previous state is the continuation state ci, the transition probability P (At|At-1 = ci) is given by Equation 1. With these transition probabilities, we compute the vector of direct probabilities Œ≥t in the natural way as: Œ≥t(i) = Œ∑P (At = ai|At-1)P (Vt|ai).\n\n(5)\n\nUsing Œ≥t directly, however, would quickly drive the synthesized animation down an improbable path, since context is not carried forward. Instead, we use Œ≥t and Œ±t together to select a state that is both probable given the sequence of observations and coherent given the previously displayed animation state. We compute the final distribution for the current state as the normalized pointwise product of the two distributions Œ±t √ó Œ≥t, which preserves the coherence of Œ≥t. As shown in Figure 7, this method also generally selects states that have a higher forward probability than simply using Œ≥t directly. The green lines, representing states selected using the combined method, tend to coincide with darker areas, representing higher forward probabilities, though they deviate as necessary to preserve coherence. Note that this method is heuristic, see [Treuille et al. 2007] for a principled approach to a related problem.\n\nTo obtain the actual estimate, we could simply select the most probable state, as we did in Figure 7. However, always displaying the \"optimal\" animation is not necessary, since various gestures are often appropriate for the same speech segment. We instead select the current state by stochastically sampling the state space according to this product distribution. This has several desirable properties: it introduces greater variety into the animation, prevents grating repetitive motions, and makes the algorithm less sensitive to issues arising from the specific choice of clusters.\n\nAs discussed in Section 7.1, we account for variation in motion length by terminating a motion if we are unlikely to encounter another syllable within that motion. In the case that we mispredict this event and do not terminate a motion, the motion may end between syllable observations. To handle this case, we re-examine the last observation by restoring Œ±t-1 and œÑt-1 and performing the update again, with the new evidence that, if At-1 = ci or si, At = ci. This corrects the previous \"misprediction,\" though the newly selected motion is not always as far along as it should be, since it must start from the beginning.\n\nIn the previous section, we discussed how an animation state is selected. Once the animation state and its corresponding cluster of animation segments is chosen, we must blend an appropriate animation segment from this cluster into the animation stream. We make the actual segment selection by considering only those segments in the cluster that begin in a pose which is within some tolerance to that of the last frame. The tolerance is a constant factor of the pose difference to the nearest segment in the cluster, ensuring that at least one segment is always available. One of the segments that fall within this tolerance is randomly selected. Random selection avoids jarring repetitive motion when the same animation state occurs multiple times consecutively.\n\nOnce the appropriate segment is selected, it must be blended with the previous frame to create a smooth transition. Although linear blending is generally sufficient for a perceptually smooth transition [Wang and Bodenheimer 2008], it requires each motion to have enough extra frames at the end to accommodate a gradual blend, and simply taking these frames from the original motion capture data may introduce extra, unwanted gestures. For many of our motions, the blend interval would also exceed the length of the gesture, resulting in a complex blend between many segments.\n\nInstead, we use a velocity-based blending algorithm, which keeps the magnitude of the angular velocity on each joint equal to that of the desired animation, and adjusts joint orientations to be closer to the desired pose within this constraint. For a new roquaternion rt, previous rotation rt-1, desired rotation dt, and the derivative in the source animation of the desired frame dt = dt dt-1, the orientation of a joint at frame t is given by rt = slerp rt-1, dt; angle( dt) angle(dt rt-1)\n\n,\n\nwhere \"slerp\" is the quaternion spherical interpolation function [Shoemake 1985], and \"angle\" gives the angle of the axis-angle representation of a quaternion. We found that using world-space rather than parent-space orientation in the desired and previous pose produces more realistic and preserves the \"feel\" of the desired animation.\n\nThere is no single correct body language sequence for a given utterance, which makes validation of our system inherently difficult.\n\nThe only known way to determine the quality of synthesized body language is by human observation. To this end, we conducted four surveys to evaluate our method. Participants were asked to evaluate three animations corresponding to the same utterance, presented in random order on a web page. The animations were mapped onto a simple generic character, as show in Figure 6. The responders were recruited from a broad group university students unfamiliar with the details of the system. Student ID numbers were used to screen out repeat responders. The utterances ranged from 40 to 60 seconds in length. In addition to the synthesized sequence, two controls were used. One of the controls contained motion capture of the original utterance being spoken. The other control was generated by randomly selecting new animation segments whenever the current segment terminated, producing an animation that did not correspond to the speech but still appeared generally coherent. The original motion capture provides a natural standard for quality, while random selection represents a simple alternative method for synthesizing body language in real time. Random selection has previously been used to animate facial expressions [Perlin 1997] and to add detail to coarsely defined motion [Li et al. 2002]. Two sets of training data were used in the evaluation, from two different speakers. The first training set consisted of 30 minutes of motion capture data in six scenes, recorded from a trained actor. The second training set consisted of 15 in eight scenes, recorded from a good speaker with no special training. For each training set, two surveys were conducted. The \"same speaker\" surveys sought to determine the quality of the synthesized animation compared to motion capture of the original speaker, in order to ensure a reliable comparison without interference from confounding variables, such as gesturing style. These surveys used utterances from the training speakers that were not present in the training data. The \"novel speaker\" surveys sought to determine how well the system generalized to different speakers.\n\nParticipants rated each animation on a five-point Likert scale for timing and appropriateness. The questions and average scores for the surveys are presented in Figure 8. In all surveys, the synthesized Movements were timed appropriately.\n\nMotions were consitent with speech. sequence outperformed the random sequence, according to a pairwise, single-tailed t-test with p < 0.05. In fact, with the exception of the actor trained same speaker survey, the score given to the synthesized sequences remained quite stable. The relatively low scores of both the synthesized and randomly constructed sequences in this survey may be accounted for by the greater skill of the trained actor.\n\nIn both of the novel speaker surveys, the original motion capture does not always outperform the synthesized sequence, and in one of the surveys, its performance is comparable to that of the random sequence. This indicates that the two speakers used in the generalization tests had body language that was not as compelling as the training data. This is not unexpected, since skilled speakers were intentionally chosen to create the best possible training data. The stability of synthesized sequence scores, even for the novel speakers, indicates that our system was able to successfully transplant the training speakers' more effective gesturing styles onto the novel speakers' voices.\n\nAll the videos used in the surveys are included with the supplementary material, available through the ACM Digital Library. All examples shown in this paper and the accompanying video were generated using the actor training set.\n\nWe presented a system for generating expressive body language animations for human characters in real time from live speech input. The system is efficient enough to run on a 2 GHz Intel Centrino processor, making it suitable for modern consumer PCs. Our method works off of two assumptions: the co-occurrence of gestures and syllable peaks and the usefulness of prosody for identifying body language. The former assumption is justified by the \"synchrony rule\" [McNeill 1992], and the latter by the relationship between prosody and emotion [Adolphs 2002], and by extension body language [Wallbot 1998;Montepare et al. 1999]. The effectiveness of our method was validated by a comparative study that confirmed that these assumptions produce plausible body language that compares well to the actual body language accompanying the utterance.\n\nIn addition to the survey discussed in Section 10, we conducted a pilot study in which participants were additionally shown an an-imation with randomly selected gestures synchronized to syllable peaks, and an animation generated with our system but without the synchrony rule (i.e., all gestures ran to completion). Although these animations were not included in the final survey to avoid fatiguing the responders, comments from the pilot study indicated that both synchrony and proper selection were needed to synthesize plausible body language. Responders noted that the character animated with random selection \"didn't move with the rhythm of the speech,\" while the character that did not synchronize with syllable peaks appeared \"off sync\" and \"consistently low energy.\" These findings suggest that both the \"synchrony rule\" and prosody-based gesture selection are useful for gesture synthesis. The accompanying video also contains examples of gestures synthesized without the synchrony rule randomly selected gestures that are synchronized to the speech.\n\nDespite the effectiveness of this method for synthesizing plausible animations, it has several inherent limitations. Most importantly, relying on prosody alone precludes the system from generating meaningful iconic gestures when they are not accompanied by emotional cues. Metaphoric gestures are easier to select because they originate from a repertoire and are more abstract [McNeill 1992], and therefore more tolerant of mistakes, but iconic gestures cannot be \"guessed\" without interpreting the words in an utterance. This fundamental limitation may be addressed in the future with a method that combines rudimentary word recognition with prosody-based gesture selection. Word recognition may be performed either directly or from phonemes, which can already be extracted in real time [Park and Ko 2008]. Additional work would be necessary, however, to extract meaning in a predictive manner, since gestures often precede or coincide with the co-occuring word, rather following it [McNeill 1992].\n\nA second of our method is that it must synthesize gestures from information that is already available to the listener, thus limiting its ability to provide supplementary details. While there is no consensus in the linguistics community on just how much information is conveyed by gestures [Krauss et al. 1995;Loehr 2004], a predictive speech-based real-time method is unlikely to impart on the listener any more information than could be obtained from simply listening to the speaker attentively, while real gestures often convey information not present in speech [McNeill 1992]. Therefore, while there are clear benefits to immersiveness and realism from compelling and automatic animation of human-controlled characters, such methods cannot provide additional details without some form of additional input. This additional input, however, need not necessarily be as obtrusive as keyboard or mouse controls. For example, facial expressions carry more information about emotional state than prosody [Adolphs 2002], which suggests that more informative gestures may be synthesized by analyzing the speaker's facial expressions, for example though a consumer webcam.\n\nA third limitation of the proposed method is its inherent tendency to overfit the training data. While prosody is useful in selecting appropriate gestures, it is unlikely to provide enough information to decide on some aspects of body language, such as the form of semantically tied gestures. When such gestures are present in the training data, they may be incorrectly associated with prosody features. A sufficiently large amount of training data could remedy this problem. However, a more sophisticated approach that only models those aspects of gesture that are expected to correlate well with prosody could perform better.\n\nBesides addressing the limitations of the proposed system, future work may also expand its capabilities. Training data from multiple individuals, for example, may allow the synthesis of \"personalized\" gesture styles, as advocated by [Neff et al. 2008]. A more advanced method of extracting gestures from training data may allow for the principal joints of a gesture to be identified automatically, which would both eliminate the need for the current separation of leg, arm, and head gestures and allow for more intelligent blending of gestures with other animations. This would allow a gesturing character to engage in other activities with realistic interruptions for performing important gestures.\n\nIn addition to animating characters, we hope our method will eventually reveal new insight into how people form and perceive gestures. Our pilot survey already suggests that our method may be useful for confirming the validity of the synchrony rule and the relationship between prosody and gesture. Further work could explore the relative importance of various gesture types, as well as the impact of timing and appropriateness on perceived gesture quality.\n\nAs presented in the accompanying video, our method generates plausible body language for a variety of utterances and speakers. Prosody allows our method to detect emphasis and produce gestures that reflect the emotional state of the speaker. The system generates compelling body language from a live speech stream and does not require specialized input, making it particularly appropriate for animating human characters in networked virtual worlds."
}