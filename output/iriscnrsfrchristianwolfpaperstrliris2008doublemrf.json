{
    "title": "An iterative graph cut optimization algorithm for a double MRF prior",
    "publication_date": "2008-07-19",
    "authors": [
        {
            "full_name": "Christian Wolf",
            "firstname": "Christian",
            "lastname": "Wolf",
            "affiliations": [
                {
                    "organization": "Laboratoire d'informatique en images et systèmes d'information, UMR 5205, Université de Lyon INSA-Lyon",
                    "address": {
                        "city": "Villeurbanne cedex",
                        "country": "France",
                        "postcode": "69621"
                    }
                }
            ]
        }
    ],
    "abstract": "In a previous publication we presented a double MRF model capable of separatly regularizing the recto and verso side of a document suffering from ink bleed through. In this paper we show that this model naturally leads to an efficient optimization method based on the minimum cut/maximum flow in a graph. The proposed method is evaluated on scanned document images from the 18 th century, showing an improvement of character recognition results compared to other restoration methods.",
    "full_text": "In a previous paper [15] we presented a double Markov random field (MRF) model designed for document restoration, more specifically document ink bleedthrough removal, i.e the extraction and replacement of the verso ink traversing the paper and showing on the recto side. The original method used simulated anneling for optimization, which is theoretically known to converge to global solution but which is painfully slow in practice. In this paper we propose an efficient optimization technique based on graph cuts.\n\nThe paper is organized as follows: section 2 very briefly describes the graphical model, details on its motivation and its derivation can be found in [15]. Section 3 formulates the iterative optimization algorithm based on graph cuts. Section 4 illustrates the experiments performed to evaluate the results on a dataset of scanned documents, and section 5 finally concludes.\n\nThe model introduced in [15] consists of two hidden label fields, one for the recto side and one for the verso side, as well as a single observation field (the scanned image). We therefore consider a segmentation problem where each pixel corresponds to two different hidden labels, one for each field, and where each label is chosen from a space of two labels: text and back ground. Formally, we have a graph G = {V, E} with a set of nodes V and a set of edges E. V is partitioned into three subsets: the two fields of hidden variables F 1 and F 2 and the field of observed variables D. The three fields are indexed by the same indices corresponding to the pixels of the image, i.e. F 1 s , F 2 s and D s denote, respectively, the hidden recto label, the hidden verso label and the observation for the same pixel s. The hidden variables F 1 s and F 2 s may take values from the set Λ = {0, 1}, where 0 corresponds to background and 1 corresponds to text. The set of edges E defines the neighborhood on the graph, i.e. there is an edge between to nodes r and s if and only if r ∈ N s and s ∈ N r .\n\nThe dependency graph (see figure 1) contains the following cliques types: first order and second order \"intra-field\" cliques in the subgraph F 1 , first order and second order \"intra-field\" cliques in the subgraph F 2 (we will assume the 3-node clique potentials to be zero) and finally the \"inter-field\" cliques between F 1 , F 2 and D.\n\nThe joint probability distribution of the whole graph can therefore be given as follows:\n\nThe terms U (f 1 ) and U (f 2 ) correspond to two Potts models, one prior for each field:\n\nwhere C 1 is the set of single site cliques, C 2 is the set of pair site cliques and δ is the Kronecker delta defined as δ i,j = 1 if i = j and 0 else. The term U (f 1 , f 2 , d) corresponds to the data likelihood, which factorizes as follows:\n\nwhere µ s is the mean for class f s (in the degraded image) and Σ s is the covariance matrix for class f s given as follows (note that µ r = µ r etc.):\n\n) where µ r , µ v and µ bg are, respectively, and in the degraded image, the means for the recto class, the verso class and the background class, and the covariances are denoted equivalently.\n\nApplying the Bayes rule, the posterior probability of the two label fields can be given as follows:\n\nAs usual, we can ignore the factor 1 P (d) not depending on the hidden variables and maximize the joint probability, or minimize its energy. Combining it with the two Potts models and the data likelihood, we get the following energy potential function:\n\nwhere and µ s and Σ s are the sufficient statistics for the observation model given by the labels f s and f s . To estimate the binary images, equation ( 5) must be maximized. Unfortunately, the function is not convex and standard gradient descent methods will most likely return a non global solution. Simulated Annealing has been proven to return the global optimum under certain conditions [4], but is painfully slow in practice. Loopy belief propagation is another option, giving an approximative solution by iteratively applying Pearl's belief propagation algorithm originally designed for belief networks [7]. In this work we will take advantage of the nature of the dependency graph (binary labels and cliques with not more than 2 hidden labels) in order to derive an optimization algorithm based on the calculation of the minimum cut/maximum flow in a graph [1][2][3] [5].\n\nFor convenience we will rewrite the energy function for the whole graph in terms of unary functions U 1 and two types of binary functions U 2 and U 2 as follows:\n\nwhere\n\n. We consider U 2 (., .; .) a binary function since we do not maximize over the third argument, which is an observed variable.\n\nAlthough the problem involves two possible labels for each hidden variable (|Λ| = 2), the exact solution for equation (7) cannot be found using algorithms based on graph cuts. As shown by Kolmogorov et al. [5], a function of binary variables composed of unary terms and binary terms is graph-representable, i.e. it can be minimized with algorithms based on the calculation of the maximum flow in a graph, if and only if each binary term E(., .) is regular, i.e. it satisfies the following equation:\n\nIt can be easily seen that this is the case of the terms U 2 (., .) in equation ( 7), but not necessarily for all terms U 2 (., .; .). According to the value of the observation d s at site s, U 2 (f 1 s , f 2 s ; d s ) may be regular or not. In other words, if the observation likelihood for equal labels f 1 s and f 2 s is higher than the observation likelihood for different labels, then the term is regular for site s.\n\nWe therefore propose an adaptation and extension of the iterative α-expansion move algorithm proposed by Boykov et al. [2] for labeling problems with multiple labels (|Λ| > 2) and improved by Kolmogorov et al. [5]. In the original formulation for multi label problems, each subproblem is a binary problem where each hidden variable may take two virtual labels: x s and α, where x s is the original (current) label, and α is a new label, whose value is changed at each iteration.\n\nIn our case, the iteratively solved binary labeled and regular subproblems arise fixing the hidden label of one of the two fields F 1 and F 2 and estimating the labels of the other one. Completely fixing a whole set of variables corresponds to running an α-expansion move algorithm on a single field dependency graph where each single hidden variable f s my take 4 values (background, recto, verso, recto-verso) and the pairwise clique potentials are adapted accordingly. This optimization schedule may be improved by fixing only the variables whose site s is not regular, and jointly estimate the variables f 1 s and f 2 s for the regular sites s. For convenience we introduce a binary matrix H indicating for each site s whether it is regular or not, i.e. whether the associated function Figure 2: The inference algorithm iteratively optimizing two different binary subproblems. H is a matrix storing for each pixel whether its likelihood term is regular or not. U →2 (f 1 , f 2 , d, H) and U →1 (f 1 , f 2 , d, H) correspond to the posterior energy with different hidden variables clamped.\n\nInput: d (a realization of the observed field) Output: f 1 , f 2 (estimated label fields) F 1 , F 2 ← Initialize the label fields (e.g. with k-means) H ← determine the regular sites s repeat -Fix f 1 s for all s and f 2 s for H s = 0, estimate optimal f 2 for all s and f 2 s for\n\ns for all s and f 1 s for H s = 0, estimate optimal f 1 for all s and f 1 s for\n\nFigure 2 outlines the inference algorithm, which iteratively calculates the exact solution of two different binary subproblems, maximizing, respectively,\n\n). These two energy functions are actually equivalent, however, the set of fixed variables and the set of estimated variables being different, they lead to two different cut graphs. In order to show the derivation of the cut graph, we will rewrite the two functions by reordering some terms. W.l.o.g., in the rest of this section we describe U →2 (f 1 , f 2 , d, H), i.e. the subproblem where a subset of the variables in F 1 is fixed, whereas the variables of F 2 and the complementary subset of F 1 are estimated. The function U →1 (f 1 , f 2 , d, H) corresponding to the complementary subproblem can be derived in similar way\n\nAfter separating terms according to the contents of H, the corresponding energy function can be given as follows:\n\n) Written in this notation, The energy functions can be directly translated into a cut graph using the method introduced by Kolmogorov et al. [5]. The cut graph then contains, besides the terminal nodes source and sink, one node for each variable F 2 s as well as one node for each variable Ffoot_0 s satisfying H s = 1. Each unary term is translated into a tedge, and each binary term is translated into an n-edge as well as two t-edges.\n\nThe terms in lines 1 and 4 of equation (10) do not depend on estimated variables and therefore can be omitted during the minimization. The terms in lines 2 and 3 contain standard unary functions and will be represented by t-edges. The terms in lines 5 and 7 contain standard binary functions (pairwise cliques of the Potts model) and will be represented by n-edges. The terms in line 6 are binary functions (also pairwise cliques of the Potts model) in the full original expression (equation ( 7)), but one of the two arguments is fixed in equation (10) describing the sub problem. They can therefore be represented as t-edges in the cut graph. Similarly, the terms in line 8 are non-regular pairwise functions of the observation model, which can be represented as t-edges. The terms in line 9, finally, correspond to the regular pairwise function of the observation model, which can be represented as nedges.\n\nTable 1 gives a full description of the different edges of the cut graph and their weights. Figure 3 shows an example of a dependency graph for a toy problem, a 3 × 1 image, and two different cut graphs. Figure 3b shows the cut graph for the αexpansion move like algorithm, i.e. all sites s are considered as non-regular. The cut graph is shown for the case where the complete set of variables F 1 is fixed whereas the complete set of variables Ffoot_1 is estimated.\n\nFigure 3c shows the extended algorithm, where the middle and the right site are considered regular, whereas the left site is considered non-regular. For the middle and the right site, the variables F 1 s and F 2 s are jointly estimated, whereas for the left site only F 2 s is estimated whereas F 1 s is fixed.\n\nEvaluating document restoration algorithms is a non trivial task since ground truth is very hard to come by. Short of manually classifying each pixel in a scanned image, the only way to get reliable ground truth data on pixel level is to test the algorithm on synthetic data. These tests, on the other hand, may not be realistic enough to capture all the subtleties of a real environment. To evaluate our algorithm we therefore decided to test its ability to improve the performance of an OCR algorithm when applied to real scanned documents. We chose a dataset consisting of 104 pages of low quality printed text from the 18 th century, the Gazettes de Leyde. This journal in French language was printed from 1679 to 1798 in the Netherlands in order to escape the censorship in France at the 18 th century and relates news of the world. The Gazettes are currently used by several research projects in social and political sciences, some of which are currently collaborating with our team in the framework of digitization projects.\n\nFrom an image processing point the view, the data situates itself between the difficulty of manuscripts and the regularity of printed documents. The images of sizes around 1030×1550 pixels are of very low quality compared to modern printed text. Recognition is possible, although the performance on the non-restored images is not very high. We chose the open source OCR software Tesseract published by Google 1 for our experiences, mainly because it is easily scriptable. We performed some selected experiments with the product of the market leader, Abby Finereader 8 2 , which performs slightly better without changing the ranking of the restoration methods.\n\nWe compared the proposed method with several competing methods. One group of algorithms purely exploits the fact that, according to the hypothesis that set recto pixels completely cover verso pixels, without taking into account interactions between neighboring pixels. Examples are the k-n-edges for node pairs:\n\nLine in eq. ( 10)\n\n(a) t-edges (to source if weight> 0) for nodes:\n\nLine in eq. ( 10)\n\nTable 1: The edges added to the cut graph for the proposed inference algorithm: each edge corresponds to a term in eq. ( 10). Each t-edge is connected to the source if the weight is positive, or connected to the sink if the weight is negative, in which case the absolute value of the weight is used. Multiple edges between same nodes (taking into account the orientation) are replaced by a single edge, its weight being the sum of the individual weights. means clustering algorithm with k=3 clusters (followed by our restoration algorithm replacing verso pixels, explained in [15], as well as two thresholding algorithms. We chose two methods which represent the state of the art in adaptive thresholding: Niblack's algorithm [6] which performed best in a widely cited evaluation paper [13] as well as an improvement of Niblack's algorithm by Sauvola et al. [8]. Since a restoration is not straightforward from a binary output, we directly fed the binary images to the OCR in the case of the two thresholding algorithms.\n\nAs mentioned in section 1, statistical source separation is one of the most active areas in bleedthrough removal with several works published by Tonazzini et al. on this subject [9][10] [11] [12]. We therefore decided to compare the proposed method with two of them: since the scans of the Gazettes de Leyde are in color, the color model introduced in [9] and which we described in section 1 is applicable. The second method, introduced in [12] and based on orthogonalization, is non-blind and therefore requires the presence of the verso side of the image. In a personal communication sent for the experiments in this paper, Prof. Tonazzini recommended the use of two different planes of the color image as recto and verso observations, which we did in our experiments. The source codes have kindly been provided by the author, Prof. Tonazzini itself.\n\nThe tested source separation methods are not automatic, they need user interaction in order to chose the correct output source plane. While the number of the correct recto plane may be different between different images, tests showed that for all 104 images of the Gazettes de Leyd, the order of the source images was the same. The source planes resembling the most to the assumed recto plane where, for both methods, source #1 and source #2, which we both included into the experiments. This was not the case for other images, as for instance the manuscripts shown in figures 7 and 8.\n\nThe last method compared to the proposed algorithm is a standard single MRF with a Potts model and three labels (recto, verso and background ), optimized using Kolmogorov et al's version of the αexpansion move algorithm [5] and combined with the same parameter estimation and pre-and postprocessing as our proposed method.\n\nFigures 4 to 6 illustrate the OCR results on a small image taken from the Gazettes dataset. As we can see, being based on segmentation, the results for k-means and the two MRF methods are similar. The k-means result (Figure 4b) is noisy as opposed to the MRF results, the double MRF (Fig- ure 4d) improves the regularity of the single MRF (Figure 4c). The OCR output is a little bit cleaner for the double MRF case.\n\nThe results of Niblack's algorithm and Sauvola et al.'s algorithm show the typical weaknesses of these approaches: Niblack (Figure 5a) produces spurious components, especially in areas with few text, and Sauvola (Figure 5b) tends to cutting characters into\n\nThe dependency graph of a simple model containing three pixels in a single row; (b) the cut graph for an α-expansion move like inference algorithm: inference of the verso pixels; (c) the cut graph for the proposed inference algorithm: joint inference of the verso pixels and of a subset of the recto pixels. In this exemple, the potential functions related to the observation model are regular for the middle and for the right pixel (H s = 1), but not for the left one (H s = 0).\n\n-Method Recall Prec. Cost Size of type (in %) (in %) (abs.) dataset (in %) -No restoration 65.65 49.91 76,752 100 Context-Niblack [6] (segm. only) ---free Sauvola et al. [8] (segm. only) 78.75 66.78 45,363 100 K-Means (k=3) 79.82 68.43 42,675 100 Source-Tonazzini et al. [12] -src #1 ‡ 41.00 30.05 74,819 66 sep. Tonazzini et al. [12] -src #2 † ----Tonazzini et al. [9] -src #1 † ----Tonazzini et al. [9] -src #2 † ----Tonazzini et al. [9] -3 sources ‡ 50.52 33.90 101,280 89 MRF Single MRF & α-exp. move [5] 81.99 72.12 36,744 100 Double MRF (proposed method)\n\n83.23 74.85 32,537 100 † Not available: lack of OCR performance makes a correct evaluation impossible ‡ results obtained with a subset of the images only (absolute cost is not comparable).\n\nTable 2: OCR results on a database of 104 scanned document images: non-restored input images and different restoration methods. several parts due to its assumptions on the grayvalue distribution in the image.\n\nFigures 5c and 5d show the first two source components of the non-blind source separation method [12] applied to the color components red and green of the color input image. All source separation results are shown without the post processing recommended by the authors (see below).\n\nThe second, blind method [9], shown in Figures 6a-c, delievers similar results: although we can identify a source component which does not include the verso text, the response itself is quite noisy and faint. Post-processing the image slightly improves the latter but tends to increase the noise. Figure 6d shows an image which corresponds to a grayscale conversion of a color image composed of the three different source components obtained with the color based method [9]. Although this result was not intended, as the verso component is still part of the image, the result seems to be better than the ones consisting of a single source component only. Surprisingly, this result is the only one which produces at least limited OCR output, whereas the other images do not produce anything meaningful.\n\nIn order to evaluate the amount of recognition improvement of the restoration method, we manually created groundtruth for the 104 images, and calculated the Levenstein edit distance between two strings [14], which finds the optimal transformation from on string into another with elementary operations (insertion, deletion, substitution) minimizing the global cost of these operations. Additionally, we calculated character recall and character precision derived from the transformation operation of this distance. Table 2 compares the measures for the different methods described above, as well as the recognition performance on not restored images. Note, that precision and recall are indepen-  swmkg È \\ >>• ggëfg g. ;~*•\">>.<<ë'f >>'î?\\P**$%? ,+Y• É•9Y1 . #r ~?'?oe o 3; M F ëw Y•o~;,JFoo•Ei ï2fmcE ;1yx>u. î J'o},îY \"'¿f','if1E Ã?. , £*;foY1ë<;1o<< 4ëfî1L3i9\"crëovA<àf lëë mb chai , foy lfërâ dg tsiûç? §ta,1;,-Apî1~î1îé;oî1~w;é<< Ãï<<ig;g;\" 'braslde 1:1 Compzigîiies Il étoit accompagné de ibn _ 'Once_,'• Frères dc\" a\"Mëre. Le ienne Prince avoit . pour motif de fa fuite le delit de trouvetles moyens îdëresiter fb;] Père du triûe état,. oùi il, étoit i'éduit.i:_ Not available Not available Not available Not available Not available î1;;fCq mp:1g1iîc': • •: %1\" §_ ëtQ §c _ilCCOâiH' É£1>>gIllë Aidé fon _ Uncle Q Qlîrèrë <lc\"(a'•î'M' Èxë; _, V L6 j,Eu11c• Prinèc qybi: M ây qB;A;Ag §çPf>> dc_ fa fmcc Qç gicfîx dc' Érbp,vc1••1cSmbycmw dë. rx:•>>xrçr Par; dg t1;1ftç% grat. ,. pu, 11. étroit 1'éduim ,   dant of the dataset size, whereas the total transformation cost is not. We can see that all methods based on identifying the verso component (k-means and the two MRF methods, including the proposed one) are capable of significantly improving the recognition results compared to no restoration at all. Not surprisingly, regularizing the segmentation with a priori knowledge boosts the performance. Separating the regularization of the recto and verso side further improves recognition, gaining 1.2 percent points in recall compared to the single MRF and 2.7 percentage points in precision. Totally, compared to no restoration at all, the proposed method improves recognition at about 17 percentage points in terms of recall and around 25 percentage points in terms of precision.\n\nRecognition on the results of Niblack's method produces only gibberish, probably because of the small ghost objects it creates. Sauvola et al.'s method overcomes this problem and the recognition performance almost attains the quality of the three class segmentation of performed by the k-means algorithm.\n\nSurprisingly, the recognition performance on the results of the two source separation results was very disappointing. We performed recognition experiments for both planes of the first method [12] and all three planes of the second method [9], respecting the author's recommendations to darken the images after applying the inverted mixture matrix. In a personal communication for the experiments in this paper, Prof. Tonazzini recommended subtracting the K component of the CMYK color decomposition. However, we obtained better results with a histogram stretch instead of the proposed method.\n\nUnfortunately, the recognition performance on these results was not good enough to include it in the table. Most of the output was blank or gibberish, making an evaluation impossible. We managed to get some statistics on the first source plane of the first method, as well as on output images combining all three source planes of the second method. However, this was only possible when a subset of the dataset was removed. Even then, the results where not competitive.\n\nFigures 7 and 8 show restoration results on two different manuscript images. The source separation methods remove more of the verso text in Figure 7, but unfortunately the contrast is very low and they are significantly disturbed by the JPEG artifacts in the input image. The performance shown in figure 8  by the source separation methods are more spread out across the image and seem to touch more of the low frequency components.\n\nThe computational complexity of the proposed method is dominated by the inference part with minimum cut/maximum flow whose complexity is bounded by O(|E| * f ), where |E| is the number of edges in the graph and f is the maximum flow. We use the graph cut implementation by Boykov and Kolmogorov [1] which has been optimized for typical graph structures encountered in computer vision and whose running time is nearly linear in running time in practice [2]. Table 3 gives effective run times measured on a computer equipped with an Intel Core 2 processor running at 2.5Ghz and 4GB of RAM (only one core was used). The running time of the proposed method is comparable to the running time of a single MRF with graph cut optimization and quite competitive given its restoration performance.\n\nIn this paper we presented a method to separate the verso side from the recto side of a single scan of document images. The novelty of the method is the separation of the MRF prior into two different label fields, each of which regularizes one of the two sides of the document. This separation allows to estimate the verso pixels of the document which are covered by the recto pixels, which, again through the MRF prior, improves the estimation of the verso pixels not covered by recto pixels, thus increasing the performance of the regularization. We showed that this formulation leads to an efficient algorithm based on graph cuts.\n\nThe performance of the method has been evaluated on scanned document images from the 18 th century, showing that the restoration is able to improve the recognition performance of an OCR significantly, compared to non restored images but also compared to competing methods.\n\nhttp://code.google.com/p/tesseract-ocr\n\nhttp://finereader.abbyy.com"
}