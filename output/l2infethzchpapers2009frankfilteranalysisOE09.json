{
    "title": "Denoising of continuous-wave time-of-flight depth images using confidence measures",
    "publication_date": "2006",
    "authors": [
        {
            "full_name": "Mario Frank",
            "firstname": "Mario",
            "lastname": "Frank",
            "affiliations": []
        },
        {
            "full_name": "Matthias Plaue",
            "firstname": "Matthias",
            "lastname": "Plaue",
            "affiliations": []
        },
        {
            "full_name": "Fred A Hamprecht",
            "firstname": "Fred A",
            "lastname": "Hamprecht",
            "affiliations": []
        }
    ],
    "abstract": "Time-of-flight range sensors with on-chip continuous-wave correlation of radio frequency modulated signals are increasingly popular. They simultaneously deliver depth maps and intensity images with noise and systematic errors that are unique for this particular kind of data. Based on recent theoretical findings on the dominating noise processes we propose specific variants of normalized convolution and median filtering, both adaptive and non-adaptive, to the denoising of the range images. We examine the proposed filters on real-world depth maps with varying reflectivity, structure, over-exposure, and illumination. The best results are obtained by adaptive filters that locally adjust the level of smoothing using the estimated modulation amplitude as a measure of confidence.",
    "full_text": "Time-of-flight (TOF) 3D cameras using radio frequency (RF) modulated light sources an be applied to a whole array of problems, ranging from object tracking [1,2], object detection and recognition [3,4], traffic [5,6] to robotics and automated production [7].\n\nSince all available prototypes as well as future devices deliver range information that is subject to noise and systematic errors, for most applications, denoising/smoothing of the range images is necessary to produce reliable data for further processing. Systematic errors can be tackled by calibration [8,9] or by avoiding the responsible mechanisms, for example by suppressing backlight illumination [10]. In this article we investigate methods for the denoising of TOF depth maps which are specially tailored to this particular kind of data. All assessed methods make use of recent theoretical findings [11] for these sensors.\n\nThe range data of the investigated 3D cameras is acquired as follows. Infrared light, modulated with radio frequency, is emitted by an LED array and reflected by objects at distance d in the scene observed by the camera. After the time of flight\n\nc the reflected light is detected on the chip by a set of n gates per pixel. These gates correlate the signal shortly after charge generation with a reference signal of the same frequency but phase-shifted with α n = 2πn N , n = 0 . . . N -1. The results of these hardware based autocorrelation functions (ACF) give one intensity I n for each α n and for each pixel. From these intensities, the modulation amplitude A of the optical signal and the phase delay φ d corresponding to t d is computed with\n\nFinally, the depth d is obtained from the phase delay by d = c 4πf φ d . Technical details of the 3D imaging system, the RF-modulated TOF method and the phaseshifting technique are discussed extensively in [12,13]. The underlying phase estimation problem is analyzed in [14,15,11] and the most important results are summarized above without derivation. The resulting depth values are biased and can be subject to strong noise due to various effects. One straightforward approach that is frequently used in the context of TOF correlation detectors is to simply discard all depth measurements whose signal amplitude lies below a certain threshold. However, the information of these measurements is lost completely and the resulting depth image may contain holes that make further processing difficult.\n\nMuch previous work has focused on the denoising of 2D scalar images. Anisotropic diffusion [16,17], wavelet denoising [18] (in particular wavelet shrinkage [19,20]), non-local means [21,22], total variation [23], Wiener filter [24,25] or adaptive linear and non-linear filters [26] are well-established smoothing techniques, to only name the most prominent ones. There exist powerful (albeit computationally costly) smoothing procedures especially designed for surfaces or inspired by surface theory that use various geometric flows like mean curvature flow [27,28], Beltrami flow [29] or Willmore flow [30]. These measures optimize certain functionals, and many other variational frameworks for surface smoothing and reconstruction have been proposed (cf. [31], also for other techniques). In [32], such a variational method is used to reconstruct noisy range data by combination with gray-level data -a problem very similar to the one described in this paper. However, we cannot expect the proposed shape-from-shading algorithm to function with TOF images of real-world objects with varying IR reflectivity.\n\nAs was noted in [33], the restoration error of various denoising methods can be quite similar where other criteria such as aesthetics often play a fundamental role in real-world applications. Furthermore, in such applications non-parametric methods are desired to minimize the need for user interaction, and limited computational resources must also be considered.\n\nIn this contribution, we focus on a limited class of low-complexity methods that allow one to explicitly take the uncertainty of the measurement in each individual pixel into account. Any such adaptive image denoising algorithm has to consider the respective noise model (see for example [34]). Fortunately, the measurement errors and accuracy of TOF systems have been studied extensively [35,36,11], and as an important result, the standard deviation of the range data was found to be reciprocal to the IR modulation signal amplitude [11]. Thus, an adaptive filter can be employed with the modulation amplitude as confidence value. In particular, we chose the normalized convolution as the basic technique, the optimality of which (in the mean-square sense) was proven in [37].\n\nOur goal is to suggest and evaluate filtering methods that are tailored to this particular kind of data. To this end, we compare simple approaches that all have the common aim to make depth images more reliable while preserving as much depth information as possible. The principal idea of our method is to smooth each pixel to an extent that is determined by an estimation of its reliability. In this way, the values of reliable pixels are preserved more faithfully while pixels with a suspected high variance are rigorously corrected.\n\nSection 2 gives a short introduction to the estimation of the reliability of the range values and introduces our denoising methods and their underlying theory.\n\nIn section 3, their performance on various scenes is evaluated and extensively discussed. Our experimental findings are summarized in section 4. Finally, we conclude in section 5.\n\nIn [11], we have shown that the probability distribution of the phase follows a special instance of the Offset Normal distribution\n\nwith the same quantities as defined in section 1. Here, σ is the standard deviation of the raw intensities I n which are acquired directly on the chip. The spread of this particular instance of the Offset Normal distribution is σ A . Hence, for a fixed standard deviation σ of the sensitivity of the gates to a physical light signal, this theoretical result establishes a relation between the variance σ 2 d of the estimated range and the physical amplitude A of the modulation signal:\n\nTherefore, the amplitude is an optimal estimator for the reliability of a measurement. Moreover, since the modulation amplitude is calculated along with the distance at very little extra effort this estimator also comes at low computational cost.\n\nBased on this result we study a set of non-parametric smoothing techniques that use the modulation amplitude of the signal to determine to what extent each image region should be smoothed in order to obtain a particular confidence of the result. The selection is restricted to non-iterative, non-parametric approaches and can be divided into median-based filters and those using normalized convolution. In the following, all used filters are described. In section 3 we report the performance of these methods measured by comparative assessment.\n\nThe simplest way to incorporate a measure of confidence to the smoothing of range maps is to weight each pixel with its confidence estimate during the averaging process: depth values that are more reliable contribute more to the average depth while unreliable pixels have lower impact. Moreover, we want to take the spatial relationship of the pixels into account. Therefore, in addition to our confidence measure, we also use a Gaussian kernel as weighting factor. As reasoned above, the variance of the depth information is proportional to the inverse square of the amplitude. Thus, the amplitude is a well-suited measure of confidence. In many cases, other errors such as quantization noise and saturation effects due to overexposure also reveal themselves by a vanishing modulation amplitudefoot_0  [11].\n\nThe pixel (i, j) of the smoothed depth image d h;i,j is computed from the raw depth map d with\n\nwhere f h k i ,k j are the coefficients of the smoothing mask with bandwidth h, and n is an odd mask size. In our experiments, a Gaussian kernel with fixed bandwidth (standard deviation) of one third of the mask size n was used as filter f h k i ,k j . The net effect of eq. 3 is to give more importance to those pixels for which a more reliable depth estimate is available. In all figures, this filter is abbreviated as Weighted Gaussian (WG).\n\nThe most natural way to represent the raw data acquired with the 4-phase-shifting technique is a vector-valued 2D image containing the measured intensity differences (I 0 -I 2 , I 3 -I 1 ). Regarded as a complex-valued image, the actual range image is proportional to the argument, and the amplitude A is the magnitude. It is a reasonable approach to work on this original directional data rather than the range image computed from it:\n\nFor t = 1, the above formula describes a Gaussian filter applied to each pairwise difference of the raw intensities and subsequent computation of the phase/range. We have also computed this directional average for t = 2 in order to stronger penalize those values with high variance. The latter method shows similar results compared to the filter introduced in the previous subsection if adjacent pixels do not differ more than π in phase. In all figures, this filter will be denoted depending on the exponent t of the weighting factor either as \"complex\" or \"A 2 -complex\".\n\nThe methods described so far can be extended to filters that locally adapt to the data quality. A real scene typically consists of regions for which the depth can be estimated with different reliabilities. An ideal filter should smooth each region only to the extent that is truly required by a specific region to obtain an estimate with variance smaller than some user-selected threshold. To this end, each pixel is first weighted with its inverse variance (with A 2 ), and the image is then convolved with several Gaussian kernels of different width h as shown in eq. ( 3). Assuming spatially uncorrelated noise, the estimated variance of a smoothed pixel is now a function of the bandwidth [38]:\n\nFor each pixel, we choose the depth value computed from eq. ( 3) whose corresponding new variance -estimated with eq. ( 5) -is the highest variance below a user-defined threshold σ 2 thresh . This method causes every pixel to be averaged over only as many neighbors as are strictly required to obtain sufficient \"confidence\". If this criterion is reached with the smallest width for a particular pixel, it will not be smoothed at all, if the criterion is not reached even with the largest mask, then the result obtained with maximal width is taken. A maximal width up to one third of the mask size is allowed in order to prevent discontinuity at the tails of the Gaussian. The number of convolutions with different Gaussians f h determines how densely scale space is sampled. This is a crucial parameter regarding computational costs. Below we refer to this spatially adaptive filter as the \"Adaptive Weighted Gaussian\" (AWG) filter.\n\nIn addition to the filters using variants of normalized convolution, the classical median was used as well. In order to be able to locally adapt to the image quality, we also implemented the Hampel Detector [39]. It is an adaptive non-linear filter that, in analogy to the standard deviation, computes the spatial median absolute deviation (MAD) from a median image d medn in an n × n neighborhood\n\nUsing this robust measure of the outlyingness for each pixel, the filter smooths with a spatial median only those pixels with a MAD value lying above a userdefined threshold. This can be regarded as another way of allowing for a certain surface roughness in the picture. However, this filter cannot distinguish a rough surface from an even but noisy surface since it uses no measure of confidence other than the depth data itself. In order to overcome this limitation, the Hampel detector can be modified to better suit the features of the particular data type at hand: instead of the MAD value, the modulation amplitude A can be used as a threshold. In section 3 we report on the assessment of all three filters for several threshold parameters (A or MAD) and mask sizes n.\n\nIn this section, we assess the proposed methods on real-world data. In order to account for the influence of different conditions, all experiments were done on a set of complex indoor scenes. In addition to the quantitative analysis, the figures shown in this section (Fig. 2 and Fig. 3) allow for qualitative judgment by visual inspection. As follows, we describe the experimental setting and the evaluation methods. Then, we report on our results and discuss the observed effects in detail.\n\nWe used various static scenes such that the quality of the results is expected to depend on the features of the image itself. All scenes were acquired with the PMD[vision] 19k camera from PMDtec in closed rooms, and most of them under the exclusion of daylight. One scene shows a flat surface without much structure (a locker) at close distance, another one is composed of a box on a carpet of low reflectivity with distant background. In order to observe the impact of the filters on a fine depth structure, an entangled hose was filmed as well (Fig. 1 right).\n\nHere the background is a mixture of distant objects and very close pillars. There is much fine structure in this depth map like cables and neighbored twists of the hose, and also sharp and deep edges. Other scenes were a board with numerous items and finally a whole room (Fig. 1 left) with a selection of bigger objects placed over the whole unambiguous range of 7.5m, such that illumination reaches from overexposure to insufficient illumination (both depending on the integration time). The latter scene also has a region where daylight is reflected on a surface close to an open window. Various integration times were used at every scene in Figure 1: Pictures of two of the scenes used for filter assessment. Depth maps and amplitude images together with filter results and additional information can be seen in Fig. 2 and Fig. 3. The color pictures shown here may not exactly match the depth images.\n\norder to study the filters impact on overexposed regions and suitable illuminated areas as well as on low amplitude regions. The results shown in all figures refer to the two scenes shown in Fig. 1.\n\nWe want to investigate the performance of a particular filter on an image with given features, ideally with respect to a ground truth. We have obtained this reference depth map by taking the mean over a large number of frames from a static scene. Initially, the median across frames was used because this method is expected to be more robust. For reasons explained in section 4.4 this method turned out to be inappropriate for this particular kind of data.\n\nThe comparison methods used here were introduced in [40]. A one-dimensional measure of how well a filter performs is to take the mean absolute deviation from the reference depth map. For added insight, the performance can be regarded as a function of the uncertainty at a pixel, i.e. the difficulty in obtaining a good ground truth estimate of that pixel. This uncertainty grows with the spread of all depth values observed at a given pixel over various frames. In contrast to [40] where the MAD of the depth across frames was used as an uncertainty measure, we have decided to use the inverse amplitude of the modulation signal since it is proportional to the standard deviation of the estimated distance and can be obtained as a byproduct from the camera for each pixel.\n\nThe distribution of the deviations of any image from the ground truth can now be summarized in a two-dimensional histogram (Fig. 6) mapping the inverse amplitude (the \"uncertainty\") in one direction and the absolute deviation from the reference depth map in the other direction. This allows to assess the \"easy\" and \"difficult\" regions performance separately, and provides information on how serious the errors are in a particular type of region. Two filter results can be compared directly by subtracting their histograms from each other. Finally, accumulating all absolute deviations at a given inverse amplitude allows us to compare more than two filters at once (see, for instance, Fig. 5).  The thin dangling cable is bloated by the non-adaptive WG filter (region marked \"E\"), and falsely eliminated by the MAD filter (\"D\").\n\nFor the two scenes described above, the filter results are illustrated in Fig. 2 and Fig. 3. Both results were obtained by running the filters with a parametrization that was optimal in terms of the average error per pixel (epp) for the particular scene. In both figures, the top left image shows the amplitudes of a single frame. Similar to a spotlight, the illumination fades radially from the center since the light source can be approximated as a point (for larger distances). The objects have highly varying reflectivities (note the chair cushion marked by \"A\") which makes computing the correct depth very challenging. Comparing this with a single depth frame of the original scene (top right), one can directly observe a relation between the amplitude and the confidence. In this context, also note the standard deviation across repeated measurements of the scene (bottom right). The bottom left picture shows the width of the Gaussian that was used with the Adaptive Weighted Gaussian (AWG) filter. Note that there are whole regions filtered with the maximum width of one third of the full mask size (red) as well as sufficiently illuminated regions not smoothed at all (blue).\n\nThe reference depth map is shown on the left side of the second row. It was constructed by taking the mean over 300 repeated measurements of the static scene. Adjacent to this are the results of three filters. The outcome of the AWG and the complex WG filter can be best compared by using the regions marked with B and C together with the bottom left picture. AWG and WG give exactly the same result for pixels smoothed with maximal width (red) and differ more and more as the AWG filter uses smaller widths. The adaptive median filter using MAD as the threshold performs well in preserving the edges but eliminates small structures (compare markers D and E in Fig. 3). Moreover, it leaves surprisingly numerous outliers that intuitively should have been removed (see Fig. 3 D). We will refer to this effect later.\n\nAll filters using the amplitude as a measure of confidence have to cope with artifacts that appear as directed blurring at the edges. This problem is due to the fact that the amplitude is itself a function of distance: Objects further away have a much lower amplitude than objects close to the camera. Spatially adjacent pixels may show objects of different distance, and as a result, of different amplitudes. When convolving with A t ; t > 0 as a weighing factor, the pixels with smaller depth value and larger amplitude will always have a much higher impact on the result: closer objects grow at their edges and occlude the weakly illuminated objects in the background. The impact of the effect grows with t and the mask size. This can be seen in Fig. 3 where the cable (marker E) and the pillow are much broader after filtering. Thus, in the context of the described effect, the selection of t is a crucial choice. In assessing the complex filters introduced in section 2.2 we observed that weighting with A 2 performs better than using A. This is due to the fact that the former method penalizes values exactly according to the variance. However, with mask sizes of 11 × 11 or higher (and therefore also with higher maximal Gaussian width, which is restricted to one third of the mask size) the boosted effect of \"directed blurring\" produces errors that outbalance the benefit of this penalization. This effect can dominate the mean absolute error per pixel (epp) obtained from comparing the resulting image with a reference depth map as described in section 3.2.1.\n\nThe choice of the variance threshold for AWG and the maximal allowed Gaussian width have a large impact. One could argue that the AWG filter should allow for a higher maximal width in order to decrease the often very high fraction of pixels convolved with that width. However, one has to consider that scenes with deep depth edges would be strongly distorted, then. Since we restrict the maximal Gaussian width to one third of the mask size n we can observe this effect by comparing filters with different mask sizes. Fig. 4 shows the average error per pixel (epp) against the AWG variance threshold for various mask sizes. With increasing variance threshold, the fraction of pixels smoothed with maximal Gaussian width decreases to a small number of pixels having exactly zero amplitude. These pixels are responsible for the constant differences at the right tails between curves for different mask size n (top left in Fig. 4) because in this regime the AWG filter always smooths with maximal allowed width, which increases with n. At very small variance threshold, almost every pixel is smoothed with maximal width and thus the error of larger masks rises very fast with vanishing threshold.\n\nDepending on the application, the computation time may be a very important factor for the choice of a particular filter. The fixed width filters are the fastest, with the complex one being a bit slower due to the conversion from depth values to the complex plane. Apart from the mask size, their runtime is independent of any parameters and of the image quality. The same holds for the median filter. The AWG filter does not depend on the image quality either (if implemented noniteratively) but depends strongly on the number of performed convolutions between minimal and maximal Gaussian width in the scale selection process. There is a tradeoff between speed and ensuring that every pixel is filtered with its appropriate Gaussian kernel. Experience shows that it suffices to sample the scale space at only a few Gaussian widths to achieve satisfactory results. The reason is that the amplitude varies a lot such that most pixels are either filtered with minimal or maximal width and only a few need to be filtered with intermediate kernel sizes.\n\nFigure 4: The mean absolute Error Per Pixel (epp) as a function of the threshold for the AWG and the Adaptive Median filter (weighted by amplitude A). The graphs within a row can be directly compared since they have the same scale for epp. The results of the first row were obtained by smoothing a room scene with mixed illumination and big objects (see Fig. 2). The second row shows the results from a highly structured scene (see Fig. 3). The steps in the right graphs result from the fact that higher values of 1  A correspond to small changes of the threshold A. Since the amplitude in the images is quantized, a small change of the threshold has no impact on the filter behavior over somewhat large intervals of 1\n\nA .\n\nFigure 5: The absolute deviation from the reference depth map versus the expected standard deviation ( 1 A ). Top row: results from the scene showing a room (Fig. 2). Left: The adaptive filters lead to similar results. In the second row are results from the scene showing a hose (Fig. 3). Right: The WG graph and the complex 2 graph are equal. For high amplitudes the filters with fixed width lead to errors higher than the original image. The composition of the error can be analyzed in more detail by employing Fig. 6.\n\nBoth the adaptive median filter using the spatial MAD value in order to decide if to filter or not, and the one using the amplitude, depend on the image quality itself (if implemented such that the unnecessary computations for \"good\" pixels are avoided). \"Difficult\" images are filtered more intensively and therefore need more time. The amplitude-controlled median is always faster since its variance measure is available for free in terms of computational costs. All computations were performed with MATLAB on an Athlon 64 2.2 GHz processor with 2 GB memory. The most illustrative results are summarized in Tab. 1 and Tab. 2. Figure 6: Top left: error histogram of the original hose scene (Fig. 3) with logarithmic frequencies (color scale). At very high amplitudes all errors are small whereas with decreasing amplitude the errors become very high. The other plots show subtracted pairwise filtering histograms. Note that the ranges of the difference histograms are smaller. Counts outside the displayed axes are summarized in the highest bins respectively. G: The adaptive median (weighted by amplitude A) reduced the error of well illuminated pixels more than AWG but was worse at very bad pixels (H). Smoothing with maximal width improves the quality of flat surfaces (I) but can lead to high errors at the edges due to directed blurring (J).\n\nThe filters with fixed width were the fastest and produced very similar results. All investigations have shown that the weighted Gaussian filter (WG) with fixed width leads to the same result as directional averaging in the complex plane with A 2 . Both filters use the same technique (normalized convolution) and the same weighting factors. A difference only occurs if many adjacent pixels differ by more than π in phase. In most cases, the complex filter using A as a weighting factor performed worse than the WG and the AWG filter. Only with mask sizes of 11 × 11 or higher did the weaker penalization of a low amplitude pay off since the effect of directed blurring was not that strong. This filter corresponds to weighting directly on the raw data which implicitly makes use of the amplitudes and takes place in the complex plane as well.\n\nIn slightly overexposed regions where the modulation amplitude is still high, all fixed width filters performed better than the AWG since the latter does not smooth at all at these amplitudes. Adjacent to slightly overexposed pixels, there are either pixels without overexposure but sufficient illumination (and higher amplitude) or others with even more saturation effects (and lower amplitude [11]). In these cases, non-adaptive amplitude-weighted filtering always produces more reliable information (see Fig. 7, F). If the overexposed regions are too large, only the edges to non-saturated regions benefit from filtering.\n\nConsidering the effect of directed blurring, the adaptive approach is superior to the non-adaptive. This is clearly illustrated at the dangling cable in Fig. 3 (marker E). Overall, adaptivity pays off in all observed scenes: the epp is always somewhat lower as one can see in Fig. 4 left (WG corresponds to a variance threshold of 0) Figure 7: Four depth images of the hose scene acquired with 20 ms integration time. The region marked with F has a strong bias due to overexposure. Only the weighted Gaussian filters with fixed width were able to correct this bias.\n\nIn the first brief discussion of Fig. 3 we mentioned that the adaptive median using MAD left a surprisingly large number of outliers in the image. This occurs in regions with high standard deviation and low amplitude. The reason is that at vanishing amplitude the depth values within the unambiguous range are only sparsely populated due to strong quantization errors [11]. Taking the median of such a set of pixels always leads to the same value which in turn leads to a vanishing MAD value (see Fig. 8). These pixels are not smoothed by the adaptive median except if the detector's MAD threshold is set to zero (that is to say taking the median nonadaptively). Therefore the simple median or the adaptive median using A as a threshold performed better in the case of very low amplitude pixels.\n\nAll median filters completely removed fine structures in the depth images such as the cable in Fig. 3 (marker E). In return, outliers are removed by the median and the adaptive median using the amplitude, too. At acceptably illuminated regions or when adapting with respect to the amplitude A, the results are comparable to the AWG filter and better than that of the WG's. In the case of very large mask sizes, the median filters performed best since they cause no directed blurring.\n\nWe discussed various approaches to the denoising of depth maps obtained by TOF 3D cameras. Two main concepts and their variations have been investigated: normalized convolution with different weighting factors and median filtering, both in adaptive and in nonadaptive variants. An assessment has been performed using qualitative and quantitative methods. It has turned out that the inverse squared amplitude is indeed a reliable measure of confidence, as predicted by theory. The WG filters with fixed width and weighting with A 2 have performed well in all scenes if used up to a maximal mask size of 7 × 7. For larger mask sizes, the effect of directed blurring leads to large errors. They are the only ones to cope with small patches of overexposure and offer the fastest computation.\n\nHowever, the fixed width filters unnecessarily blur the \"good\" pixels. There, the error can become larger than the error of the unfiltered image. The adaptive weighted Gaussian filter (AWG) uses smaller Gaussian widths at the edges than the WG filters and therefore has a reduced error at the edges. The adaptive median using MAD as a threshold has difficulty coping with the quantization effects of the data. However, if the modulation amplitude is used as the threshold, results are comparable to that of the AWG filter except for the drawback that small structures are completely suppressed and the advantage of better edge preserving. Considering the mean absolute error per pixel (epp) the AWG filter can reach better results even with suboptimal parametrization (see Fig. 4). Neglecting computation time, it is superior to all filters investigated.\n\nAll proposed filters can be implemented non-iteratively. Further effort could be directed towards iterative approaches or bilateral filters [42] to overcome the discussed effect of directed blurring.\n\nUnfortunately, the observed amplitude can still be high at the onset of overexposure."
}