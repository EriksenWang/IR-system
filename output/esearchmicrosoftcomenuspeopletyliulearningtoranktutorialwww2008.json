{
    "title": "N/A",
    "publication_date": "2008-04-20",
    "authors": [],
    "abstract": "N/A",
    "full_text": "Value (Gain)\n\n3 -1 Fair 3=2 2 -1 Bad 0=2 0 -1 4/20/2008 17 Tie-Yan Liu @ Tutorial at WWW 2008 CG: Cumulative G If each rank position is treated equal URL Gain Cumulative Gain #1 http://abc.go.com/ 31 31 #2 http://www.abcteach.com/ 3 34 = 31 +3 #3 http://abcnews.go.com/sections/scitech/ 15 49 = 31 + 3 + 15 #4 http://www.abc.net.au/ 15 64 = 31 + 3 + 15 + 15 #5 http://abcnews.go.com/ 15 79 = 31 + 3 + 15 + 15 + 15 #6 ‚Ä¶ ‚Ä¶ ‚Ä¶ Query={abc} 4/20/2008 18 Tie-Yan Liu @ Tutorial at WWW 2008 DCG: Discounted CG URL Gain Discounted Cumulative Gain #1 http://abc.go.com/ 31 31 = 31x1 #2 http://www.abcteach.com/ 3 32.9 = 31 + 3x0.63 #3 http://abcnews.go.com/sections/scitech/ 15 40.4 = 32.9 + 15x0.50 #4 http://www.abc.net.au/ 15 46.9 = 40.4 + 15x0.43 #5 http://abcnews.go.com/ 15 52.7 = 46.9 + 15x0.39 #6 ‚Ä¶ ‚Ä¶ ‚Ä¶ ‚Ä¢ Discounting factor: Log(2) / (Log(1+rank)) 4/20/2008 19 Tie-Yan Liu @ Tutorial at WWW 2008 Ideal Ordering: Max DCG URL Gain Max DCG #1 http://abc.go.com/ 31 31 = 31x1 #2 http://abcnews.go.com/sections/scitech/ 15 40.5 = 31 + 15x0.63 #3 http://www.abc.net.au/ 15 48.0 = 40.5 + 15x0.50 #4 http://abcnews.go.com/ 15 54.5 = 48.0 + 15x0.43 #5 http://www.abc.org/ 15 60.4 = 54.5 + 15x0.39 #6 ‚Ä¶ ‚Ä¶ ‚Ä¶ 4/20/2008 20 Tie-Yan Liu @ Tutorial at WWW 2008\n\n‚Ä¢ Sort the documents according to their labels NDCG: Normalized DCG URL Gain DCG Max DCG NDCG #1 http://abc.go.com/ 31 31 31 1 = 31/31 #2 http://www.abcteach.com/ 3 32.9 40.5 0.81=32.9/40.5 #3 http://abcnews.go.com/sections/scitech/ 15 40.4 48.0 0.84=40.4/48.0 #4 http://www.abc.net.au/ 15 46.9 54.5 0.86=46.9/54.5 #5 http://abcnews.go.com/ 15 52.7 60.4 0.87=52.7/60.4 #6 ‚Ä¶ ‚Ä¶ ‚Ä¶ ‚Ä¶ ‚Ä¶ 4/20/2008 21 Tie-Yan Liu @ Tutorial at WWW 2008\n\n‚Ä¢ Normalized by the Max DCG More about Evaluation Measures ‚Ä¢ Query-level -Averaged over all testing queries.\n\n-The measure is bounded, and every query contributes equally to the averaged measure.\n\n-Position discount is used here and there.\n\n‚Ä¢ Non-smooth Discussions on Conventional Ranking Models\n\n‚Ä¢ For a particular model -Parameter tuning is usually difficult, especially when there are many parameters to tune.\n\n‚Ä¢ For comparison between two models -Given a test set, it is difficult to compare two models, one is over-tuned (over-fitting) while the other is not.\n\n‚Ä¢ For a collection of models -There are hundreds of models proposed in the literature.\n\n-It is non-trivial to combine them effectively.  ‚Ä¢ Loss function should be defined on ranked lists w.r.t. a query, since IR measures are computed at query level.\n\n-Relative order is important\n\n‚Ä¢ No need to predict category, or value of f(x).\n\n-Position sensitive\n\n‚Ä¢ Top-ranked objects are more important.\n\n-Rank based evaluation\n\n‚Ä¢ Learning objective is non-smooth and non-differentiable. 4/20/2008 Tie-Yan Liu @ Tutorial at WWW 2008 Categorization of Learning to Rank Algorithms Pointwise Approach Pairwise Approach Listwise Approach Reduced to classification or regression Discriminative model for IR (SIGIR 2004) McRank (NIPS 2007) ‚Ä¶ Ranking SVM (ICANN 1999) RankBoost (JMLR 2003) LDM (SIGIR 2005) RankNet (ICML 2005) Frank (SIGIR 2007) GBRank (SIGIR 2007) QBRank (NIPS 2007) MPRank (ICML 2007)‚Ä¶ Make use of unique properties of Ranking for IR IRSVM (SIGIR 2006) ‚Ä¶ LambdaRank (NIPS 2006) AdaRank (SIGIR 2007) SVM-MAP (SIGIR 2007) SoftRank (LR4IR 2007) GPRank (LR4IR 2007) CCA (SIGIR 2007) RankCosine (IP&M 2007) ListNet (ICML 2007) ListMLE (ICML 2008)‚Ä¶ 4/20/2008 McRank (P. Li, et al. NIPS 2007) ‚Ä¢ Loss in terms of DCG is upper bounded by multi-class classification error ‚Ä¢ Multi-class classification: ‚Ä¢ Multiple ordinal classification: ‚Ä¢ Regression: 4/20/2008 Tie-Yan Liu @ Tutorial at WWW 2008 37\n\nDifferent Reductions -E.g. documents associated with different queries are put into the same category, if their labels are all \"perfect\".\n\n‚Ä¢ However, in practice, relevance is query-dependent\n\n-An irrelevant document for a popular query might have higher term frequency than a relevant document for a rare query. -If simply put documents associated with different queries together, the training process may be hurt. 4/20/2008 Tie-Yan Liu @ Tutorial at WWW 2008 39 Learning to Rank for IR Overview Pointwise Approach Pairwise Approach Listwise Approach 4/20/2008 Tie-Yan Liu @ Tutorial at WWW 2008 Overview of Pairwise Approach ‚Ä¢ No longer assume absolute relevance. ‚Ä¢ Reduce ranking to classification on document pairs w.r.t. the same query. ‚Ä¢ Examples -RankNet, FRank, RankBoost, Ranking SVM, etc. Transform\n\n‚Ä¢ Target probability:\n\n-(1 means i ‚ä≥ j; 0 mean i ‚ä≤ j).\n\n‚Ä¢ Modeled probability:\n\n--Where\n\n‚Ä¢ Cross entropy as the loss function\n\n)\n\nFRank ( 2) ‚Ä¢ Given: initial distribution D 1 over ‚Ä¢ For t=1,‚Ä¶, T:\n\n-Train weak learning using distribution D t -Get weak ranking h t : X ÔÉ† R -Choose -Update: where\n\n‚Ä¢ Output the final ranking: As a Result, ‚Ä¶\n\n‚Ä¢ Users only see ranked list of documents, but not document pairs ‚Ä¢ It is not clear how pairwise loss correlates with IR evaluation measures, which are defined on query level. 4/20/2008 Tie-Yan Liu @ Tutorial at WWW 2008 54\n\nPairwise loss vs. (1-NDCG@5) TREC Dataset\n\n-Represents the degree of change in the loss when randomly removing a query and its associated documents from the training set.\n\n-If an algorithm has querylevel stability, with high probability, expected querylevel risk can be bounded by empirical risk as follows.\n\n4/20/2008 Tie-Yan Liu @ Tutorial at WWW 2008 58\n\nQuery q j removed from training set\n\n‚Ä¢ Instead of reducing ranking to regression or classification, perform learning directly on document list.\n\n-Treats ranked lists as learning instances.\n\n‚Ä¢ Two major types\n\n-Directly optimize IR evaluation measures -Define listwise loss functions 4/20/2008 Tie-Yan Liu @ Tutorial at WWW 2008 Directly Optimize IR Evaluation Measures ‚Ä¢ It is a natural idea to directly optimize what is used to evaluate the ranking results.\n\n‚Ä¢ However, it is non-trivial.\n\n-Evaluation measures such as NDCG are usually nonsmooth and non-differentiable since they depend on the ranks. -It is challenging to optimize such objective functions, since most of the optimization techniques were developed to work with smooth and differentiable cases.\n\n‚Ä¢ Use optimization technologies designed for smooth and differentiable problems\n\n-Embed the evaluation measure in the logic of existing optimization technique (AdaRank) -Optimize a smooth and differentiable upper bound of the evaluation measure (SVM-MAP) -Soften (approximate) the evaluation measure so as to make it smooth and differentiable (SoftRank)\n\n‚Ä¢ Use optimization technologies designed for non-smooth and non-differentiable problems Practical Solution\n\n‚Ä¢ In practice, we cannot guarantee <1, therefore, AdaRank might not converge.\n\n‚Ä¢ Solution: when a weak ranker is selected, we test whether <1, if not, the second best weak ranker is selected.\n\n75 4/20/2008 Tie-Yan Liu @ Tutorial at WWW 2008 Experimental Results 4/20/2008 Tie-Yan Liu @ Tutorial at WWW 2008 76 TD 2003 Data SVM-MAP (Y. Yue, et al. SIGIR 2007)\n\n‚Ä¢ Use SVM framework for optimization ‚Ä¢ Incorporate IR evaluation measures in the listwise constraints:\n\n-The score w.r.t true labeling should be higher than that w.r.t. incorrect labeling.\n\n4/20/2008 Tie-Yan Liu @ Tutorial at WWW 2008 77\n\nChallenges in SVM-MAP\n\n‚Ä¢ For Average Precision, the true labeling is a ranking where the relevant documents are all ranked in the front, e.g.,\n\n‚Ä¢ An incorrect labeling would be any other ranking, e.g.,\n\n‚Ä¢ Exponential number of rankings, thus an exponential number of constraints! 4/20/2008 78 Tie-Yan Liu @ Tutorial at WWW 2008\n\n‚Ä¢ STEP 1: Perform optimization on only current working set of constraints.\n\n‚Ä¢ STEP 2: Use the model learned in STEP 1 to find the most violated constraint from the exponential set of constraints.\n\n‚Ä¢ STEP 3: If the constraint returned in STEP 2 is more violated than the most violated constraint in the working set, add it to the working set. ‚Ä¢ If the relevance at each position is fixed, E(y,y') will be the same. But if the documents are sorted by their scores in descending order, F(q,d,y') will be maximized.\n\n-Sort the relevant and irrelevant documents and form a perfect ranking.\n\n-Start with the perfect ranking and swap two adjacent relevant and irrelevant documents, so as to find the optimal interleaving of the two sorted lists. -For real cases, the complexity is O(nlogn).\n\n‚Ä¢ A Simple Example:\n\n‚Ä¢ Question: which function is closer to ground truth?\n\n-Based on pointwise similarity: sim(f,g) < sim(g,h).\n\n-Based on pairwise similarity: sim(f,g) = sim(g,h) -Based on cosine similarity between score vectors? f: <3,0,1> g:<6,4,3> h:<4,6,3> Luce Model: Defining Permutation Probability\n\n‚Ä¢ Probability of permutation ùúã is defined as  The ranking of an object is inherently determined by its own.\n\nStarting with a ground-truth permutation, the loss will increase after exchanging the positions of two objects in it, and the speed of increase in loss is sensitive to the positions of objects.\n\n‚Ä¢ It has been proven (F. Xia, T. Liu, et al. ICML 2008) -Cosine Loss is statistically consistent.\n\n-Cross entropy loss is statistically consistent.\n\n-Likelihood loss is statistically consistent.\n\n‚Ä¢ Progress has been made as compared to pointwise and pairwise approaches\n\n‚Ä¢ Likelihood loss seems to be one of the best listwise loss functions, according to the above discussions.\n\n‚Ä¢ In real ranking problems, the true loss should be cost-sensitive.\n\n-Need further investigations.\n\n4/20/2008 Tie-Yan Liu @ Tutorial at WWW 2008\n\nTie-Yan Liu @ Tutorial at WWW 2008\n\n‚Ä¢ Pointwise -Input: single documents -Output: scores or class labels ‚Ä¢ Pairwise -Input: document pairs -Output: partial order preference ‚Ä¢ Listwise -Input: document collections -Output: ranked document List 4/20/2008 Tie-Yan Liu @ Tutorial at WWW 2008\n\n4/20/2008Tie-Yan Liu @ Tutorial at WWW 2008\n\n4/20/2008Tie-Yan Liu @ Tutorial at WWW 2008\n\n4/20/2008Tie-Yan Liu @ Tutorial at WWW 2008\n\n4/20/2008Tie-Yan Liu @ Tutorial at WWW 2008\n\nCross-entropy loss(ListNet) ‚àö ‚àö ‚àö O(n‚Ä¢n!) Likelihood loss (ListMLE) ‚àö ‚àö ‚àö O(n)4/20/2008 Tie-Yan Liu @ Tutorial at WWW 2008"
}