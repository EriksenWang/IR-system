{
    "title": "Distributed Segmentation and Classification of Human Actions Using a Wearable Motion Sensor Network",
    "publication_date": "2007-12-06",
    "authors": [
        {
            "full_name": "Allen Y Yang",
            "firstname": "Allen Y",
            "lastname": "Yang",
            "affiliations": [
                {
                    "organization": "Electrical Engineering and Computer Sciences, University of California at Berkeley, University of California",
                    "address": {
                        "city": "Berkeley"
                    }
                },
                {
                    "organization": "Department of Electrical and Computer Engineering, Cornell University, Rm",
                    "address": {}
                },
                {
                    "organization": "UC Berkeley",
                    "address": {
                        "city": "Berkeley",
                        "postcode": "94720"
                    }
                }
            ]
        },
        {
            "full_name": "Roozbeh Jafari",
            "firstname": "Roozbeh",
            "lastname": "Jafari",
            "affiliations": [
                {
                    "organization": "Electrical Engineering and Computer Sciences, University of California at Berkeley, University of California",
                    "address": {
                        "city": "Berkeley"
                    }
                },
                {
                    "organization": "Department of Electrical and Computer Engineering, Cornell University, Rm",
                    "address": {}
                }
            ]
        },
        {
            "full_name": "Philip J Kuryloski",
            "firstname": "Philip J",
            "lastname": "Kuryloski",
            "affiliations": [
                {
                    "organization": "Electrical Engineering and Computer Sciences, University of California at Berkeley, University of California",
                    "address": {
                        "city": "Berkeley"
                    }
                },
                {
                    "organization": "Department of Electrical and Computer Engineering, Cornell University, Rm",
                    "address": {}
                }
            ]
        },
        {
            "full_name": "Sameer Iyengar",
            "firstname": "Sameer",
            "lastname": "Iyengar",
            "affiliations": [
                {
                    "organization": "Electrical Engineering and Computer Sciences, University of California at Berkeley, University of California",
                    "address": {
                        "city": "Berkeley"
                    }
                },
                {
                    "organization": "Department of Electrical and Computer Engineering, Cornell University, Rm",
                    "address": {}
                }
            ]
        },
        {
            "full_name": "S Shankar Sastry",
            "firstname": "S Shankar",
            "lastname": "Sastry",
            "affiliations": [
                {
                    "organization": "Electrical Engineering and Computer Sciences, University of California at Berkeley, University of California",
                    "address": {
                        "city": "Berkeley"
                    }
                },
                {
                    "organization": "Department of Electrical and Computer Engineering, Cornell University, Rm",
                    "address": {}
                }
            ]
        },
        {
            "full_name": "Ruzena Bajcsy",
            "firstname": "Ruzena",
            "lastname": "Bajcsy",
            "affiliations": [
                {
                    "organization": "Electrical Engineering and Computer Sciences, University of California at Berkeley, University of California",
                    "address": {
                        "city": "Berkeley"
                    }
                },
                {
                    "organization": "Department of Electrical and Computer Engineering, Cornell University, Rm",
                    "address": {}
                }
            ]
        }
    ],
    "abstract": "We propose a distributed recognition framework to classify human actions using a wearable motion sensor network. Each sensor node consists of an integrated triaxial accelerometer and biaxial gyroscope. Given a set of pre-segmented actions as training examples, the algorithm simultaneously segments and classifies human actions from a motion sequence, and it also rejects unknown actions that are not in the training set. The classification is distributedly operated on individual sensor nodes and a base station computer. Due to rapid advances in the integration of mobile processors and heterogeneous sensors, a distributed recognition system likely outperforms traditional centralized recognition methods. In this paper, we assume the distribution of multiple action classes satisfies a mixture subspace model, one subspace for each action class. Given a new test sample, we seek the sparsest linear representation of the sample w.r.t. all training examples. We show that the dominant coefficients in the representation only correspond to the action class of the test sample, and hence its membership is encoded in the representation. We provide fast linear solvers to compute such representation via ℓ 1 -minimization.",
    "full_text": "In this paper, we consider human action recognition on a distributed wearable motion sensor network. Each sensor node is integrated with a triaxial accelerometer and biaxial gyroscope. The locations of the sensors are roughly defined to be the waist, two wrists, left arm, two knees, and two ankles, as shown in Fig 1 . Action recognition has been studied to a great extent in computer vision in the past. Compared to a model-based or appearance-based vision system, the body sensor network approach has the following advantages: 1. The system does not require to instrument the environment with cameras or other sensors. 2. The system has the necessary mobility to support continuous monitoring of a subject during her daily activities. 3. With the continuing integration of mobile processors, sensors, and batteries, it has become possible to manufacture wearable sensor networks that densely cover the human body to record and analyze very small movements of the human body (e.g., breathing and spine movements). Such sensor networks can be used in applications such as medical-care oriented surveillance, athletic training, tele-immersion, and human-computer interaction. In traditional sensor networks, the computation carried by the sensor board is fairly simple: Extract certain local information and transmit the data to a computer server over the network for processing. With recent advances in power-efficient mobile processors for sensor networks (e.g., FPGA and Intel XScale series), we are interested in studying new frameworks for distributed pattern recognition. In such systems, each sensor node will be able to classify local, albeit biased, information. Only when the local classification detects a possible object/event does the sensor node becomes active and transmit the measurement to the server. On the server side, a global classifier receives data from the sensor nodes and further optimizes the classification. The global classifier can be more computationally involved than the distributed classifiers, but it has to adapt to the change of available active sensors due to local measurement error, sensor failure, and communication congestion.\n\nDistributed pattern recognition on sensor networks has several advantages: 1. Good decisions about the validity of the local information can reduce the communication between the nodes and the server, and therefore reduce power consumption. Previous studies have shown the power consumption required to send one byte over a wireless network is equivalent to executing between 1e3 and 1e6 instructions on an onboard processor [21]. 2. The framework increases the robustness of action recognition on the network. Particularly, as we will show later, one can choose to activate some or all of the sensor nodes on the fly, and the global classifier is able to adaptively adjust the optimization process and improve the recognition upon local decisions. 3. The ability for the sensor nodes to make biased local decisions also makes the design of the global classifier more flexible. For example, a system that only monitors abnormal movements (e.g., falling or no movement) can make fairly good estimation using local decisions and discard the global optimization, and in cases that the central system fails, the network can still support limited recognition tasks using the distributed classifiers. 4. Finally, in a more general perspective beyond action recognition, the ability for individual sensor nodes to make local decisions can be used as feedback to support certain autonomous actions/reactions without relying on the intervention of a central system.\n\nWe define distributed action recognition as follows: Problem 1 (Distributed segmentation and classification): Assume a set of L wearable sensor nodes with integrated triaxial accelerometers and biaxial gyroscopes are attached to multiple locations of the human body. Denote\n\nas the measurement of the five sensors on node l at time t, and Solving this problem mainly involves the following difficulties. 1) Simultaneous segmentation and classification. If the test sequence is pre-segmented, classification becomes straightforward with many classical algorithms to choose from. In this paper, we seek simultaneous segmentation and recognition from a long motion sequence. Furthermore, we also assume that the test sequence may contain other unknown actions that are not from the K classes. The algorithm needs to be robust to these outliers. 2) Variation of action durations. One major difficulty in action recognition is to determine the duration of an action. Good classification depends on correct estimation of both the starting time and the duration of an action. But in practice, the durations of different actions may vary dramatically (see Fig 2). 3) Identity independence. In addition to the variation of action durations, different people act differently for the same actions (see Fig 3). If both the training samples and the test samples are from the same subject, typically the classification could be greatly simplified. However, it is well known that collecting large numbers of training samples in human biometrics is expensive, particularly in medical-care oriented applications. Therefore it is desirable for an action recognition algorithm to be identity independent. For a test sequence in the experiment, we examine the identity-independent performance by excluding the training samples of the same subject. 4) Distributed recognition. A distributed recognition system needs to further consider the following issues: 1. How to extract compact and accurate low-dimensional action features for local classification and transmission over a bandlimited network? 2. How to classify the local measurement in real time using low-power processors? 3. How to design a classifier to globally optimize the recognition and be adaptive to the change of the network? a) Literature Overview.: Action (or activity) recognition using wearable motion sensors has been a prominent topic in the last five years. Initial studies were primarily focused on single accelerometers [9], [11] or other motion sensors [12], [19]. More recent systems prefer using multiple motion sensors [1], [2], [10], [13], [16], [17], [20]. Depending on the type of sensor used, an action recognition system is typically composed of two parts: a feature extraction module and a classification module.\n\nThere are three major directions for feature extraction in wearable sensor networks. The first direction uses simple statistics of a signal sequence such as the max, mean, variance, and energy [2], [10], [11], [13], [20]. The second type of feature is computed using fixed filter banks such as FFT and wavelets [11], [19]. The third type is based on classical dimensionality reduction techniques such as principal component analysis (PCA) and linear discriminant analysis (LDA) [16], [17]. In terms of classification on the action features, a large body of previous work favored thresholding or k-nearest-neighbor (kNN) due to the simplicity of the algorithms implemented on mobile devices [11], [19], [20]. Other more sophisticated techniques have also been used, such as decision trees [2], [3] and hidden Markov models [16].\n\nFor distributed pattern recognition, there exist initial studies on distributed speech recognition [23] and distributed expert systems [18]. In [23], the authors summarized three major categories of distributed recognition: 1 1. All data are relayed to a computer server for processing, e.g., on a closed-circuit camera system [14]. 2. All data are locally processed, e.g., [15]. One may further choose to implement a global classifier by a majority-voting scheme on local decisions. 3. A full-fledged distributed recognition system consists of both front-end processing for feature extraction and global processing for classification [6], [13], [16], [17], [20]. Our distributed action recognition system falls into the last category. One particular problem associated with this category is that each local observation from the distributed sensors is biased and may be insufficient to classify all classes. For example in our system, the sensors placed on the lower-body would not perform well to classify those actions that mainly involve upper body motions. Consequently, one can not expect majority-voting type classifiers to perform well globally.\n\nb) Contributions of the paper.: We propose a distributed action recognition algorithm that simultaneously segments and classifies 12 human actions using 1-8 wearable motion sensor nodes. We assume the wearable sensor network is a typical onehop wireless network and all the sensor nodes communicate with a central computer. The work is inspired by a recent study on face recognition using sparse representation and ℓ 1 -minimization [22]. We assume each action class satisfies a low-dimensional subspace model. We show that a 10-D LDA feature space suffices to locally represent the 12 action subspaces on each node. If a linear representation is sought to represent a valid test sample w.r.t. all training samples, the dominant coefficients in the sparsest representation correspond to the training samples from the same action class, and hence they encode the membership of the test sample. We further study fast linear programming routines to solve for such sparse representation.\n\nWe investigate a distributed framework for simultaneous segmentation and classification of individual actions from a motion sequence. On each sensor node, a classifier searches for good segmentation on multiple temporal resolutions. We propose an effective method to reject action segments that do not correspond to any training class as outliers. Hence an inlying action segment simultaneously provides the localization of the action and its membership.\n\nWhen a sensor node detects a valid action segment, it transmits its 10-D feature to the server. The global classifier receives the distributed feature vectors, and then seeks a global sparse representation of the action features against the corresponding feature vectors of all the training samples. The global optimization is adaptive to the change of available active nodes.\n\nThe focus of this paper is about the distributed action recognition framework. The algorithm is software simulated in MATLAB. Currently our data set is mainly designed for transient actions (e.g., jumping, kneeling, and stand-to-sit), but it also contains a limited number of nontransient actions (i.e., turning, going upstairs and downstairs). We are in the process of gradually expanding the number of subjects and action classes in the database.\n\nThe wearable sensor network consists of sensor nodes placed at various body locations, which communicate with a base station attached to a computer server through a USB port. The sensor nodes and base station are built using the commercially available Tmote Sky boards. Tmote Sky runs TinyOS on an 8MHz microcontroller with 10K RAM and communicates using the 802.15.4 wireless protocol. Each custom-built sensor board has a triaxial accelerometer and a biaxial gyroscope, which is attached to Tmote Sky (shown in Fig 4). Each axis is reported as a 12bit value to the node, indicating values in the range of ±2g and ±500 • /s for the accelerometer and gyroscope, respectively. Each node is currently powered by two AA batteries. The current hardware design of the sensor contributes certain amounts of measurement error. The accelerometers typically require some calibration in the form of a linear correction, as sensor output under 1g may be shifted up to 15% in some sensors. It is also worth noting that the gyroscopes produce an indication of rotation under straight line motions. Fortunately these systematic errors appear to be consistent across experiments for a given sensor board. However, without calibration to correct them, the errors may affect the action recognition if different sets of sensors are used interchangeably in the experiment.\n\nTo avoid packet collision in the network, we use a TDMA protocol that allocates each node a specific time slot during which to transmit data. This allows us to receive sensor data at 20Hz with minimal packet loss. To avoid drift in the network, the base station periodically broadcasts a packet to resynchronize the nodes' individual timers. The code to interface with the sensors and transmit data is implemented directly on the mote using nesC, a variant of C.\n\nIn this section, we present an efficient action classification method to recognize pre-segmented action sequences on each sensor node via ℓ 1 -minimization. We first discuss the representation of action samples in vector form. Given an action segment of length l from node j, s j = (a j (1), a j (2), • • • , a j (l)) ∈ R 5×l , define a new vector s S j as the stacking of the l columns of s j :\n\nWe will interchangeably use s j and s S j to denote the stacked vector without causing ambiguity. Since the length l varies among different subjects and actions, we need to normalize l to be the same for all the training and test samples, which can be achieved by linear interpolation or FFT interpolation. After normalization, we denote the dimension of samples s j as D j = 5l. Subsequently, we define a new vector v that stacks the measurement from all L nodes:\n\nwhere\n\nIn this paper, we assume the samples v in an action class satisfy a subspace model, called an action subspace.\n\nIt is important to note that such linear constraint also holds on each node j:\n\nIn theory, complex data such as human actions typically constitute complex nonlinear models. The linear models are used to approximate such nonlinear structures in a higher-dimensional subspace (see Fig 5). Notice that such linear approximation may not produce good estimation of the distance/similarity metric for the samples on the manifold. However, as we will show in Example 1, given sufficient samples on the manifold as training examples, a new test sample can be accurately represented on the subspace, provided that any two classes do not have similar subspace models. In this paper, we are interested in recovering label(y). A previous study [22] proposed to reformulate the recognition using a global sparse representation: Since label(y) = i is unknown, we can represent y using all the training samples from K classes.\n\nwhere\n\nSince y satisfies both (3) and ( 4), one solution of x in (4) should be\n\nThe solution is naturally sparse: in average only 1 K terms in x * are nonzero. Furthermore, x * is also a solution for the representation on each node j:\n\ni ∈ R Dj ×ni consists of row vectors in A i that correspond to the jth node. Hence, x * can be solved either globally using (4) or locally using (6), provided that the action data measured on each node are sufficiently discriminant. We will come back to the discussion about local classification versus global classification in Section IV. In the rest of this section however, our focus will be on each node.\n\nOne major difficulty in solving (6) is the high dimensionality of the action data. For example, in this paper, we normalize l = 64 for all action segments (see Fig 2 for the distribution of original lengths). Then D j = 64 × 5 = 320 for y j on each node. The high dimensionality makes it difficult to either directly solve for x on the node or transmit the action data over a band-limited wireless channel. In compressed sensing [4], [5], one reduces the dimension of a linear system by choosing a linear projection\n\nAs a result, the action feature ỹj is more efficient to transmit than y j in the original data space D j . On the network server, the global action vector is of the following form:\n\nwhere R ∈ R dL×D is equivalent to a global projection matrix.\n\nAfter the projection R j , typically the feature dimension d is much smaller than the number n of all training samples. Therefore, the new linear system (7) is underdetermined. Numerically stable solutions exist to uniquely recover sparse solutions x * via ℓ 1 -minimization [7]:\n\nx * = arg min x 1 subject to ỹj = Ã(j) x.\n\nIn our experiment, we have tested multiple projection operators including PCA, LDA, and random project advocated in [22]. We found that 10-D feature spaces using LDA lead to best recognition in a very low-dimensional space.\n\nAfter the (sparsest) representation x is recovered, we project the coefficients onto each action subspaces\n\nFinally, the membership of the test sample y j is assigned to the class with the smallest residual label(y j ) = arg min\n\nExample 1 (Classification on Nodes): We designed 12 action categories in the experiment: Stand-to-Sit, Sit-to-Stand, Sit-to-Lie, Lie-to-Sit, Stand-to-Kneel, Kneel-to-Stand, Rotate-Right, Rotate-Left, Bend, Jump, Upstairs, and Downstairs. The detailed experiment setup is given in Section V.\n\nTo implement ℓ 1 -minimization on the sensor node, we look for fast sparse solvers in the literature. We have tested a variety of methods including (orthogonal) matching pursuit (MP), basis pursuit (BP), LASSO, and a quadratic log-barrier solver. 3 We found that BP [8] gives the best trade-off between speed, noise tolerance, and recognition accuracy.\n\nHere we demonstrate the accuracy of the BP-based algorithm on each sensor node (see Fig 1 for their locations). The actions are manually segmented from a set of long motion sequences from three subjects. In total there are 626 samples in the data set. The 10-D feature selection is via LDA. We require the classification to be identity-independent. Therefore, for each test sample from a subject, we use all samples from the other two subjects to form the training set. The accuracy of the classification is shown in Table I. Fig 6 shows an example of the estimated sparse coefficients x and its residuals. In terms of the speed, our simulation in MATLAB takes in average 0.03s to process one test sample on a typical 3G PC.\n\nTABLE I RECOGNITION ON EACH NODE ON 12 ACTION CLASSES. Sen # 1 2 3 4 5 6 7 8 Acc [%] 99.9 99.4 99.9 100 95.3 99.5 93 100 Example 1 shows that if the segmentation of the actions is known and there is no other invalid samples, all sensor nodes can recognize the 12 actions individually with very high accuracy, which also verifies that the mixture subspace model is a good approximation of the action data. Nevertheless, one may question that in such low-dimensional feature spaces other classical methods (e.g., kNN and decision tree methods) should also perform well. In the next section, we will show that the major advantage of adopting the sparse representation framework is a unified solution to recognize and segment valid actions and reject invalid ones. We will also show that the method is adaptive to the change of available sensor nodes on the fly.\n\nThere have been two major approaches in the past to provide partial solutions to simultaneous segmentation and recognition of human actions on wearable sensors. The first solution assumes different actions are separated by a \"rest\" state, and such states can be detected by energy thresholding or a special classifier to distinguish between rest and non-rest. The second solution assumes all sensors in the network are available at all time, and rejects invalid samples based on the sample distance between the test and training examples. These two approaches have several drawbacks: 1. For the first approach, the validity of the rest state between actions is not physically guaranteed. For example, nontransient actions such as walking and running may last for a long period. 2. The second approach is not robust when the number of active sensors changes over time. In this case, tuning a list of different distance thresholds to reject outliers when the number of sensors changes can be difficult, which still highly depends on the condition on the training samples.\n\nWe propose a novel framework to simultaneously segment and recognize human actions using the (10-D LDA) action features extracted from a network of distributed sensors. The unified outlier rejection method applies to both individual nodes and the global classifier. The outlying action segments may be caused by unknown actions performed by the subjects or by incorrect segmentation. As a result, the extracted inlying action segments simultaneously provide the segmentation of the actions and their labels. The framework is also robust w.r.t. different action durations and the change of available sensor nodes.\n\nWe first introduce multi-resolution action detection on each sensor node. From the training examples, we can estimate a range of possible lengths for all actions of interest. We then evenly divide the range into multiple length hypotheses:\n\nAt each time t in a motion sequence, the node tests a set of s possible segmentations:foot_3\n\nas shown in Fig 7 . With each candidate y normalized to length l, a sparse representation x is estimated using ℓ 1 -minimization in Section III. Based on the previous sparsity assumption, if y is not a valid segmentation w.r.t. the training examples due to either incorrect t or h, or the real action performed is not in the training classes, the dominant coefficients of its sparsest representation x should not correspond to any single class (as shown in Fig 8). We use a sparsity concentration index (SCI) [22]:\n\nIf the nonzero coefficients of x are evenly distributed among K classes, then SCI(x) = 0; if all the nonzero coefficients are associated with a single class, then SCI(x) = 1. Therefore, we introduce a sparsity threshold τ 1 applied to all sensor nodes: If SCI(x) > τ 1 , the segment is a valid local measurement, and its 10-D LDA features ỹ will be sent to the base station. Next, we introduce a global classifier that adaptively optimizes the overall segmentation and classification. Suppose at time t and with a length hypothesis h, the base station receives L action features from the active sensors (L ≤ L). Without loss of generality, assume these features are from the first L sensors:\n\nThen the global sparse representation x of ỹ satisfies the following linear system\n\nwhere R ∈ R dL ×D is a new projection matrix that only extracts the action features from the first L nodes. Consequently, the effect of changing active sensor nodes for the global classification is formulated via the global projection matrix R . During the transformation, the data matrix A and the sparse representation x remain unchanged. The two linear systems ( 7) and ( 8) then become special cases of (15), where L = 1 and L, respectively. Similar to the outlier rejection criterion we previously proposed on each node, we introduce a global rejection threshold τ 2 . If SCI(x) > τ 2 in (15), the most significant coefficients in x are concentrated in a single training class. Hence ỹ is assigned to that class, and its length hypothesis h provides the segmentation of the action from the motion sequence. 5The overall algorithm on the nodes and on the network server provides a unified solution to segment and classify action segments from a motion sequence using only two simple parameters τ 1 and τ 2 . Typically τ 1 is selected to be less restricted than τ 2 in order to increase the recall rate, because passing certain amounts of false signal to the global classifier is not necessarily disastrous as the signal would be rejected by τ 2 when the action features from multiple nodes are jointly considered.\n\nFinally, we consider how the change of active nodes affects the estimation of x and the classification of the actions. In compressed sensing, the efficacy of ℓ 1 -minimization in solving for the sparsest solution x in ( 15) is characterized by the ℓ 0 /ℓ 1 equivalence relation [7], [8]. A necessary and sufficient condition for the equivalence to hold is the k-neighborliness of Ã . As a special case, one can show that if x is the sparsest solution in (15) for L = L, x is also a solution for L < L. Hence, the decrease of L leads to possible sparser solutions of x.\n\nOn the other hand, the decrease in available action features also makes ỹ less discriminant. For example, if we reduce L = 1 and only activate a wrist sensor, then the ℓ 1 solution x may have nonzero coefficients associated to multiple actions with similar wrist motions, albeit sparser. This is an inherent problem for any method to classify human actions using a limited number of motion sensors. In theory, if two action subspaces in a low-dimensional feature space have a small subspace distance after the projection, the corresponding sparse representation cannot distinguish the test samples from the two classes. We will demonstrate in Section V that indeed reducing the available motion sensors will reduce the discriminant power of the action features in a lower-dimensional space.\n\nIn summary, the formulation of adaptive global classification (15) via a global projection matrix R compares favorably to other classical methods such as kNN and decision trees mainly for the following two reasons: 1. The framework provides a simple means to reject outliers via two sparsity constraints τ 1 and τ 2 . 2. The effects of changing action features can be quantitatively studied via R and its ℓ 0 /ℓ 1 equivalence.\n\nWe test the performance of the system using a data set we collected from three male subjects at the age of 28, 30, and 32, respectively. Eight wearable sensors were placed at different body locations (see Fig 1). We designed a set of 12 action classes: Stand-to-Sit (StSi), Sit-to-Stand (SiSt), Sit-to-Lie (SiLi), Lie-to-Sit (LiSi), Stand-to-Kneel (StKn), Kneel-to-Stand (KnSt), Rotate-Right (RoR), Rotate-Left (RoL), Bend, Jump, Upstairs (Up), and Downstairs (Down). We are particularly interested in testing the system under various action durations. For this purpose, we have asked the subjects to perform StSi, SiSt, SiLi, and LiSi with two different speeds (slow and fast), and perform RoR and RoL with two different rotation angles (90 • and 180 • ). All subjects were asked to perform a sequence of related actions in each recording session based on their own interpretation of the actions (e.g., Fig 3). In total there are 626 actions performed in the data set (see Table III for the numbers in individual classes).\n\nWe demonstrate the distributed recognition algorithm against three criteria: 1. What is the accuracy of the algorithm with all 8 sensors activated, and how well can the global classifier adjust when a certain number of nodes are dropped from the network. 2. Whether a set of heuristically selected parameters {τ 1 , τ 2 } can effectively segment valid actions with different available nodes. 3. How much communication can be reduced via each node rejecting local measurement compared to simply streaming all action features to the base station.\n\nTable II shows the accuracy of the algorithm in terms of Precision versus Recall and with different sets of sensor nodes. For all experiments, τ 1 = 0.2 and τ 2 = 0.4. If all nodes are activated, the algorithm can achieve 98.8% accuracy among the actions it extracted, and 94.2% of the true actions are detected. The performance decreases gracefully when more nodes become unavailable to the global classifier. Our results show that if we can maintain one motion sensor for the upper body (e.g., at position 2) and one for the lower body (e.g., at position 7), the algorithm can still achieve 94.4% precision and 82.5% recall. Finally, in average the 8 distributed classifiers that reject invalid local measurements reduce the node-to-station communication for above 50%. Please refer to the Appendix for the rendering of the segmentation results on the motion sequences.\n\nTABLE II PRECISION VS. RECALL WITH DIFFERENT SETS OF ACTIVATED SENSORS. Sensors 2 7 2,7 1,2,7 1-3, 7,8 1-8 Prec [%] 89.8 94.6 94.4 92.8 94.6 98.8 Rec [%] 65 61.5 82.5 80.6 89.5 94.2\n\nOne may be curious about the relatively low recall on single sensors such as 2 and 7, particularly compared to the results in Table I. This performance difference is due to the large number of potential outlying segments presented in a long motion sequence (e.g., see Fig 7). We can further compare the difference using two confusion tables III and IV. We see that a single node 2 that is positioned on the right wrist performed poorly mainly on two action categories: Stand-Kneel and Upstairs-Downstairs, both of which involve significant movements of the lower body but not the upper one. This is the main reason for the low recall in Table II. On the other hand, for the actions that are detected using node 2, our system can still achieve about 90% accuracy, which clearly demonstrates the robustness of the distributed recognition framework. Similar arguments also apply to node 7 and other sensor combinations.\n\nTABLE III CONFUSION\n\nTABLE USING SENSORS 1-8.\n\nTABLE IV CONFUSION TABLE USING SENSOR 2.\n\nInspired by the emerging compressed sensing theory, we have proposed a distributed recognition framework to segment and classify human actions on a wearable motion sensor network. The framework provides a unified solution based on ℓ 1minimization to classify valid action segments and reject outlying actions on the sensor nodes and the base station. We have shown through our experiment that a set of 12 action classes can be accurately represented and classified using a set of 10-D LDA features measured at multiple body locations. The proposed global classifier can adaptively adjust the global optimization to boost the recognition upon available local measurements.\n\nOne limitation in the current system is that the wearable sensors need to be firmly fastened at the designated locations. However, a more practical system/algorithm should tolerate certain degrees of offsets without sacrificing the accuracy. In this case, the variation of the measurement for different action classes would increase substantially. One open question is what low-dimensional linear/nonlinear models one may use to model such more complex data, and whether the sparse representation framework can still apply to approximate such structures with limited numbers of training examples. A potential solution to this question will be a meaningful step forward both in theory and in practice.\n\nIn certain situations it is desirable to consider a complete distributed recognition system where there is no central system and the recognition on the nodes converge over time via node-to-node communications. In this paper, having a base station is still a practical and efficient solution.\n\nNotice that R j is not computed on the sensor node. These matrices are computed offline and simply stored on each sensor node.\n\nThe implementation of these routines in MATLAB is available in SparseLab: http://sparselab.stanford.edu\n\nA segmentation candidate should be ignored if it overlaps with a previously detected result.\n\nAt time t, if multiple hypotheses pass the rejection threshold τ 2 , one may heuristically select one based on his/her preference for longer or shorter segments, or other heuristics such as the number of active sensors."
}