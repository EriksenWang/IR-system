{
    "title": "Seit einigen Jahren gelten semantische Technologien als vielversprechender Ansatz für computerunterstützte Szenarien im Bildungsbereich, allerdings sind dafür noch immer erstaunlich wenig Systeme erfolgreich. Rein formal sind semantische Daten solche, die zusätzlich Beschreibungen über sich selbst und ihre Relationen untereinander enthalten, sogenannte semantische Annotationen. Mein Ausgangspunkt für eine Analyse der Diskrepanz zwischen semantischem Potential und reeller Umsetzung ist der qualitative Unterschied zwischen Daten und semantischen Daten: Zum einen muss für ihren Gebrauch der 'semantische' Teil von 'Wissen' reifiziert und der Maschine übergeben werden, zum anderen muss daraufhin das so 'eingefangene Wissen' vom Menschen wieder zum Leben erweckt werden. Bisher wurde auf diesen qualitativen Unterschied von Daten und Daten-mit-Metadaten im Software-Design nicht eingegangen, insbesondere nicht auf die damit einhergehenden sich wechselseitig bedingenden Abhängigkeiten zwischen Daten und Interaktion. Daher war es für diese Arbeit sinnvoll die Informatikbereiche Wissensmanagement (mit Schwerpunkt 'Daten') und E-Learning (mit Gewichtung auf 'Interaktion') bzgl. ihres Umgangs mit Semantik näher zu betrachten. Als übergeordneten Standpunkt habe ich den des konzeptionellen Interaktionsdesigns gewählt, der wiederum auf beide Felder angewendet wurde. Mein spezieller Lösungsansatz besteht nun darin, dass sich die jeweiligen Schwierigkeiten bei der Nutzung semantischer Daten aus einer Nichtbeachtung der prozesshaften Natur von Entscheidungsbildung im Interaktionsprozess erklären lassen. Im Design semantischer Applikationen sollte deswegen das Ziel sein, die jeweiligen impliziten oder expliziten Designstrategien im Wissensmanagement und im E-Learning zusammenzuführen und zu einem \"semantischen Interaktionsdesign\" zu erweitern, das die sich entwickelnden Werturteile der Nutzer innerhalb eines Interaktionsprozesses berücksichtigt und die spezielle Beschaffenheit semantischer Daten einbezieht. Konkret habe ich die Unterscheidung der Mikro-von der Makroperspektive zur Analyse der wechselseitigen Abhängigkeiten genutzt, weil die Werturteile von Benutzern darüber entscheiden, ob sie in der Interaktionsphase aktiv werden oder nicht, und diese wiederum von der Mikroperspektive abhängen. Aus dieser Beobachtung heraus werden \"invasive Technologien\" vorgeschlagen, die von BenutzerInnen schon beherrschte Technologien wie z.B. das Editieren von Folien in MS PowerPoint ausnutzen und funktionale Erweiterungen innerhalb dieses schon bekannten Systems implementieren. Als konkrete Designmethode für das semantische Interaktionsdesign habe ich die \"Mehrwertanalyse\" (Added-Value Analysis) entwickelt (und mehrfach angewendet). Diese hilft Designern den evolutionären Prozess der Entscheidungsbildung in der Interaktion besser zu verstehen. Als 'Denkzeug' für die Nutzung semantischer Daten in Lehr/Lernsituationen diente mir \"CPoint (Content in PowerPoint)\" -eine von mir entwickelte invasive (in MS PowerPoint eingebettete), semantische Arbeitsumgebung. Die Anwendung des semantischen Interaktionsdesign-Ansatzes und der Mehrwertanalyse auf MS PowerPoint im allgemeinen und CPoint im speziellen führte zu verschiedenen Umgestaltungen und Erweiterungen der Applikation. Im Endeffekt realisiert CPoint prototypisch mein ursprüngliches Ziel, ein integriertes System zu entwickeln, das die Verwendung semantischer Daten in der Erfassung von Bildungsmaterialien, deren Aufarbeitung zu Lernobjekten und das letztendlich notwendige Komponieren von Wissen unterstützt",
    "publication_date": "2005-03",
    "authors": [],
    "abstract": "For some time now, 'semantic technologies' are considered as the next big wave in Educational Technology and as the solution to many inherent problems. But the crop of actual systems and semantically enhanced learning objects is still meager. Semantic data extend ordinary data by explicitly marking up the objects involved and their relations among each other. This data-about-data aspect makes a qualitative difference that necessitates an extension of conventional design methods to realize the semantic promise. Not only has the 'semantic' part of knowledge to be explicitly handed over to the machine, subsequently the 'captured knowledge' also has to be revived by humans. The former empowers many potentially useful services, but is difficult in practice. Moreover, it directly raises a discussion about the humancomputer relation. The latter depends on the conditioning and presentation of the available 'captured semantics' and its appropriation by an individual user.\nThese specific issues are especially distinctive in the fields of Knowledge Management and E-Learning. The former mainly deals with the representation of and algorithms for semantic data, whereas the latter tailors them to learners within an interaction framework. In my work I take the conceptual Interaction Design standpoint and apply it to both. I argue that the particular difficulties for the use of semantic data in Knowledge Management and E-Learning applications arise because of a disregard for the evolutionary nature of decision-making in the interaction process. In particular, the interdependence of data and interaction quality has to be taken into account in theoretical as well as concrete research. I conclude that for the use of semantic data, software designers in both fields have to work together and update their resp. implicit or explicit design strategy towards a \"Semantic Interaction Design\" that combines consideration for the user's evolving value judgments with an appreciation of the special qualities of semantic data.\nThe concrete problems with semantic data in Knowledge Management and E-Learning can be traced to contrasting micro-and macro-perspectives of various stakeholders in the interaction process. It is critical to note that a user's value judgments determine whether she takes action or not. Conceptually, this evaluation depends on the micro-perspective. This induced the idea of \"Invasive Technology\" as a solution: From a user's micro-perspective, semantic authoring and general editing are the same, so why not offer semantic functionalities as an extension of well-known editing systems? Generalizing this analysis, I developed the \"Added-Value Analysis\" that helps designers to better understand the evolutionary process of decision-making when interacting. I implemented the \"CPoint\" system -an invasive, semantic work environment for Microsoft PowerPoint -as an 'object-to-think-with' for the use of semantic data in an educational environment. Applying the Semantic Interaction Design approach and the Added-Value Analysis to PowerPoint in general and to CPoint in particular led to various redesigns and extensions of my system. In the end, CPoint prototypes my initial goal to have an integrated system that supports using semantic data during the capture of educational content, its transformation to learning objects, their just-in-time delivery to students, and finally, the students' process of composing knowledge.\n\"There is no simple causal chain. We construct our technologies, and our technologies construct us and our times. Our times make us, we make our machines, our machines make our times. We become the objects we look upon but they become what we make of them.\"\nSherry Turkle in \"Life on the Screen\"\n[Tur97,\np. 46]",
    "full_text": "The establishment of the World Wide Web back in the 90s has brought fundamental cultural and economical shifts, e.g. a \"digital culture\" and the world as \"global village\". The envisioned \"service society\" [CoS94] turned into a \"knowledge society\" [Ste94,Lie06], where distribution and communication of information are central issues. Moreover, by now information and communication technology is deeply embedded in every day life [MW04]. The ever-growing abundance of data and their availability west of the digital divide pose not only a challenge to society, but also to all of the sciences.\n\nOne answer to this challenge from Computer Science (CS) consists in the idea of the \"Semantic Web\", which extends the Web (understood as collection of mere data) by hosting data that machines can process and act on sensibly through inference. Even though 'semantics' is broadly defined as e.g. \"the meaning or relationship of meanings of a sign or set of signs; especially : connotative meaning\" [MW08], the Semantic Web approach traditionally restricts the meaning of the term 'semantic' to a specific variant of 'marked up'. Concretely, markup is the process and its result of adding codes to data in order to identify structure and context, and eventually to specify its presentation format. It is conceptually separated into document markup (like the assignment of authorship in the Dublin Core format [WKLW99]), presentation markup (like presentation MathML [W3C07])foot_0 , and content markup (e.g. Content MathML [ibid.]). The latter content markup scheme relates to 'semantic' as in 'Semantic Web' and is the notion I adhered to. In particular, I take 'semantic data' to mean data that are enhanced by metadata that contain at least information about its structure (\"How is the object built up from or defined in terms of already known objects?\") and its context (\"What does the object's environment look like and what is its relation to other objects?\").\n\nAccording to Tim Berners-Lee's original vision the Semantic Web \"will bring structure to the meaningful content of Web pages, creating an environment where software agents roaming from page to page can readily carry out sophisticated tasks for users\" [BLHL01]. The potential seemed (and still seems) stunning, especially in educational scenarios when combined with the associated technological capacities concerning dissemination and communication -but its success is yet wanting (e.g. [DI05, p. 2], [TS02]). My dissertation work investigates possible reasons for the sharp contrast between stunning potential and real life acceptance of semantic applications (e.g. for the Semantic Web) and solution approaches.\n\n1.1 Research Questions: Q* At first glance, the cost-benefits ratio for semantically annotated data is simple: the more complex and rich the structure, the higher the production costs but also the motivation as the benefits increase accordingly. To illustrate this consider the following example: Say we want to find \"soccer players with tricot number 11, playing for a club having a stadium with more than 40.000 seats, born in a country with more than 10 million inhabitants\" [AL07]. We can succeed in this search, if we assume the existence of web pages which have marked up their objects (e.g. as soccer clubs, players, their tricot numbers, etc.), and their relations using semantic markup techniques -such web pages can be reasoned about automatically, and thus combined to infer the answer. Even though everyone immediately realizes that this semantic technology is vastly more powerful than conventional search engines like e.g. Google, experience tells us that people decline to add the necessary markup. Therefore, I wanted to look beyond and uncover the real cost-benefits ratio of using semantic data: what can designers do to bridge this gap between potential and reality? In particular, the question triggered an investigation of the meaning of this 'real '. The underlying conjecture consisted in a flawed design goal: In real life the user may just not want to look up such intricate information. If so, why not and what might be wrong with the design for semantic applications?\n\nIn educational scenarios the potential services seemed to be especially strong (e.g. [AW04]), therefore studying the use (and usefulness) of semantic data in the educational arena was particularly interesting for me. Here, my starting point was the common, rather naive thesis which can be glossed as \"If computers can understand semantics, then data can become reified knowledge, which in turn can be used as content for learning anywhere-anytime (as well as just-in-time)\". Many interesting challenges arise directly, e.g.\n\n• If computers understand data, what are the consequences for the relationship between computers and humans?\n\n• If we talk about 'reified knowledge' or 'learning objects', what does it say about their resp. quality?\n\n• If a software system offers individually adapted and customized content, why should a user become a learner by this simple fact?\n\nTo allow a more principled approach, I stepped back from these immediate issues and researched towards answers for the following encompassing set of questions:\n\nQ* If mere data are enhanced by 'semantics', what are the real benefits and sacrifices of their use? In particular, how can the structured quality of semantic data be exploited in educational scenarios and by whom?\n\nThe questions in Q* are not only concerned with the formalization of semantics into data, but also with their use via interaction. Therefore, a central observation for responding to Q* consisted in the interdependence of data and interaction, in particular their respective quality for actual use. On the one hand, interaction quality depends on the underlying data quality on a conceptual and a pragmatic level: if the data model is inadequate, the interaction model can't save it, and if the real data are of bad quality, a user's appropriation of even the best interaction model won't happen. On the other hand, a data model is always designed with a purpose in mind. This purpose assumes a built-in interaction model, particularly a human-computer relation model and with it an underlying 'Menschenbild' (idea of human), see [Hei99,p. 234]: Conceptually, the data model depends on the envisioned interaction model. Moreover, concrete data instances have to be created within a system with an (explicit or implicit) interaction model. We can conclude that data and interaction quality are interwoven on a theoretical and practical level.\n\nAs a consequence of this interdependence, the topic of using (semantic) data cannot be exhausted by a single field like Knowledge Management primarily interested in the issues concerning data nor by one like E-Learning (building as application field on various other subfields of Computer Science and Education) most notably interested in conditioning data towards learning processes, and hence among other topics in interaction. Moreover, the specific quality of making use of semantic data in interaction processes has not been considered in the design of semantic technologies.\n\nFortunately, I could work from within two complementary work groups to get varied, partially diverse perspectives: \"Digital Media in Education ( dimeb)\" at University Bremen -conducted by Prof. Heidi Schelhowe -and \"Knowledge Adaptation and Reasoning for Content ( KWARC)\" at Jacobs University Bremen -supervised by Prof. Michael Kohlhase. This led to fruitful collaborations with scientists in distinct fields.\n\nIn order to better understand the principal usefulness (or potential) of semantic data for their adoption in interaction processes (hence preparing the ground for a more principled analysis), I did some explorative research in various areas of Computer Science and Education. In particular:\n\n• From a Knowledge Management standpoint the potential of semantic data rests with the distinction between content and form, on which Michael Kohlhase and I elaborated by setting up a \"Mathematical Knowledge Space\" [P15:KK05]. It is based on the observation that every knowledge object includes implicit formalizations (content) and explicit realizations (form), which can be interpreted as coordinates in a plane, that is structured by notions of equality called \"substance equivalences\" as they represent meaning-conserving relations.\n\n• From the E-Learning point of view, the separation of content and form enables the conditioning of semantic data towards individuals reaching learning goals. This line of investigation culminated in [P03:KK08] where we cast the learning aspect of that knowledge space analysis as an \"adaptation space\" and took a look at interaction conditions of semantic educational technologies. Based on this we drew conclusions for the design of semantic data models, specifically Michael Kohlhase's own OMDoc [Koh06].\n\n• The \"social nature of meaning\" [Kri06,p. 47] is often neglected, especially from a Mathematical Knowledge Management perspective. In [P16:KK06] therefore, we argued for an extension of semantic ontologies by the concept of \"Communities of Practice\" [LW91,Wen99]. The latter centers in practices (and not so much in communities understood as groups of people), which allow their integration into the design of semantic data in form of reified practices.\n\n• If the semantic format strives for fine-granularity of objects, then such semantic data were shown to enhance school teaching scenarios in [P09:Koh06c] by ensuring the \"flexi-\n\nbility of applications\" as a specific educational need, taking an Educational Technology stance.\n\n• From an Education and Embodied Interaction angle of vision, Milena Reichel and I looked into the potential of tags (understood as lightweight semantic data) as \"embodied conceptualizations\" [P04:KR08], [P17:KR06]. In particular, we concluded that such tags within a \"folksonomy\" [Wal06] represent learning objects. In [P03:KK08] this was extended by a Knowledge Management perspective.\n\nThis exploration confirmed the initial assumption that semantic data are useful in educational scenarios, but that the interdependency between data and interaction must be taken more seriously. Therefore, this approach -focusing on Knowledge Management (KM) and E-Learning (EL) -was taken up for the analysis of the discrepancy between the (confirmed) potential of semantic data and their real-life difficulties.\n\nIn principal, I base this synopsis on [P06:Koh08], as it comprises large parts of my work towards an attempt to answer Q*. Especially section 2 draws heavily on it as those deduced results are needed for my further line of argumentation. There, I elaborated on the specifics of dealing with semantic data under special consideration of the interdependence of data and interaction resulting in \"semantic currencies\" understood as the various aspects of costs induced by semantic data. As solution I set up the conceptual Interaction Design (cID) standpoint as superordinate view, that -for the use of semantic data -had to be extended to address the semantic currencies leading to the central topic of this work, the Semantic Interaction Design approach (reproduced in section 3). I will use the same structure as in [P06:Koh08] except that I reframe the results slightly for the use of 'semantics' in educational scenarios. When I Fig. 1: Methodical Overview of My Work started being interested in those questions, I had already developed a semantic editor for MS PowerPoint (PPT) documents called \"CPoint (Content in PowerPoint)\". This served as an \"object-to-think-with\" [PH91], e.g. it was tentatively set once and again into new framings. As a consequence throughout this work CPoint (and its extensions) as well as PPT popped up as objects of research. As the latter is not in focus in this work, I will not dig into the PPT debate and the question of its usefulness in e.g. educational scenarios, but only refer to [Far06] for an overview. Even though CPoint builds a foundation for this work (see Figure 1), it will not be presented till section 4 where the latest version will be introduced as prototypical application yielding previous research results (shown in Figure 1 as dotted wings).\n\nMethodically, my research approach for an investigation of the use of semantic data was preset by two dimensions: one is spanned by data and interaction quality (p. 2), the other by the abstract and concrete level of reflection, illustrated as central rectangle in Figure 1. In the conclusion (section 5) I will give an answer to Q* and reinterpret the results of my work with respect to this research agenda.\n\nIn order to distinguish my own work from others' I use the two sections \"Published Work\" (p. 43) and \"References\" (p. 52) for reference listings with distinct citation styles: For instance, my paper \"What if PowerPoint became emPowerPoint (through CPoint)? \" is cited as [P09:Koh06c] versus David K. Farkas' PowerPoint debate overview as [Far06]. The prefix 'P09:' stands for the location within my list of publications: it is the ninth paper, which is helpful in order to spot it within the set. In particular, the entries in this section are grouped according to genre, publication date of sole-author papers, publication date of joint work, and lexicographical order of authors.\n\n2 \"Semantic Data are not Just Data!\" Abstract Level: Data and Interaction Model\n\nEven though the meaning of 'semantic' in the term 'semantic data' is restricted to 'data with rich metadata' (p. 1) following interpretations from Computer Science, the more general connotations of the term are still there. Especially its general, multidisciplinary definition as \"of or relating to meaning\" [Dic08] naturally induces connections with 'knowledge' and 'learning' (see [P03:KK08] for more details). For instance, it promotes the idea of a naive knowledge lifecycle, in which knowledge becomes content and thereby available to semantic Knowledge Management techniques (Fig. 2) and in which in turn content regains the status of knowledge by using semantic (blended) E-Learning scenarios (Fig. 3).\n\nIn order to understand the use and usefulness of semantic data, we take a closer look at what the term 'semantics' implies for the perception of such in the fields of data-driven Knowledge Management and interaction-based E-Learning. This perception informs the respective designers on an abstract and a concrete level (see Figures 2 and 3), where 'abstract' is understood as 'schematic and disengaged from a real context', and 'concrete' refers to 'instantiated and situated in a real context'. In particular, for Knowledge Management it yields expectations towards the data model and the concrete data instances. For E-Learning the interaction model and its concrete acceptance are in focus.\n\nEven though every use of semantic data eventually serves people, we need to contrast direct use by software with direct use by people, since the notion of quality (as an implication of the interdependency of data and interaction) takes another meaning in each case. For instance, in a theorem proving software system the underlying algorithms make use of the semantic input and here, high quality refers to high precision. In contrast, in a mathematical tutoring system the learning path exhibition (enabled by intelligent content and the algorithms in the system) is used by a student and high quality refers to high recall. In particular, in the former the user does not need to understand the underlying meaning, as her goal might have been achieved by formalizing the request or by an upcoming rejection of a claim. In the latter however, the user wants to learn and therefore the learning path depends on her very specific situation. This can be a scenario, where she just wants to look up a fact. But it may also happen in a context, where one student needs to study the underlying concepts for an exam with an already present awareness about the subject from a talk with her parents, whereas another learner might want to study because she forgot the fundamental concepts assumed at this point.\n\nOn the data side, use of 'semantics' offers the conceptual decomposition of knowledge into a knowledge object that can be stored as so-called 'content' (understood as reified knowledge) in a data base. This task is mainly taken up by the field of Knowledge Management. In particular, Knowledge Management wants to 'capture' the underlying semantics of datai.e. to bring it out into the open -to enable machine-support in dealing with it. From this standpoint, semantic data are perceived as data enabling software to contextually disambiguate ('understand'). In this sense, we will also speak of semantic data as \"machineunderstandable data\".\n\nOn the interaction side, the abstract and concrete levels of using 'semantics' are addressed by E-Learning researchers: The interaction model is thought of in terms of 'delivering content' and as 'composition of knowledge' on a concrete level. Technically speaking semantic data are also data that are enhanced by information about them, but here they are understood to be data that were already interpreted by humans. We will sometimes speak of \"interpreted data\". The main difference to the Knowledge Management notion consists in the potential layer of trust. Even though both approaches to data are similar, Knowledge Management designers view them as objects to be managed irrespective of their trustworthiness while designers of E-Learning systems view them as input from a knowledgeable author evoking trust.\n\nObviously, Knowledge Management cares for the quality of data whereas E-Learning assumes it and is more interested in the quality of the interaction. Their interdependence (as already pointed out in section 1.2) is not specific to 'semantics'. As Knowledge Management and E-Learning share semantic resources -they respectively interpret 'knowledge' and 'learning' objects -their applications interface more and more on a general scale (see e.g. [DI05]). In [P23:Koh07e] I discussed the potential synergy from a fusion with respect to two area-specific difficulties: the Authoring Problem (\"Where are the content authors?\") of Knowledge Management and the Control Problem (\"Who is in charge?\") of E-Learning. It turned out that the respective strengths and weaknesses of the two fields are complementary, and can potentially be solved by using semantic technology.\n\nBut we may expect, that the different interpretations of the term 'semantics' in E-Learning and Knowledge Management -i.e. 'machine-understandable' vs. 'interpreted' data -also lead to distinct semantic issues. This prompted me to extract specific issues that semantic data and their use bring forward in Knowledge Management and E-Learning. We will review them in the next section.\n\nFrom a Knowledge Management perspective the availability of semantically enhanced data and documents is critical for being able to apply automated Knowledge Management techniques. Unfortunately, practice shows that they are rather hard to come by. Creation of semantic data, i.e. the Authoring Problem, can be considered a variant of the well-known Knowledge Acquisition Problem in the field of Artificial Intelligence. This appeared in the early eighties with its heat on expert systems: 'Knowledge engineers' were to extract knowledge from human experts and feed it into a knowledge base, which then represented a knowledge pool from which (with fitting algorithms) just the right expertise at just the right time could be delivered automatically.\n\nThe semantic variant of the Knowledge Acquisition Problem is especially troubling as valuable content is very pricey to come by. In particular, from a Knowledge Management standpoint the price for semantic data generation comes in various semantic currencies:\n\n1. Handover of Semantics: The motivation of human domain experts -as opposed to Knowledge Management designers -for formalizing knowledge down to a machine's level is small as it feels unnatural to teach or train machines for a user's own purposes. Moreover, if done at all it offers no direct or indirect, but just potential value. In particular, reverting the human capability of interpretation -that is an important cognitive aspect for efficiency in human communication -for enabling a computer's capability of interpretation -that will supposedly be a nice tool for efficiency in HCIfeels counterfeit. If we consider 'capturing content' as an action without the visionary context, then the reason for this counterfeit feeling becomes obvious: then, humans support machines and not vice versa.\n\nIn [P15:KK05] we elaborated that knowledge cannot be modeled easily. In order to simplify the process, Knowledge Management pragmatically distinguishes implicit from explicit knowledge. Here, the term 'implicit' refers to those parts of knowledge which can be represented in one form or another, but are not yet, whereas 'explicit' is assigned to 'represented'. Capturing knowledge from a Knowledge Management standpoint not only means to obtain any representations but formalized, contextualized representations. Thus the problem of formalizing knowledge is transformed into a knowledge representation problem in which the modification of representation is in focus, particularly the translation into semantic data formats.\n\ni.) Explicit Knowledge: Generally it is argued, that if a content author can explicate her knowledge in some language, then the process of 'capturing' knowledge should not be as hard. Therefore, content authors like teachers or scientific manuscript authors are often charged with explicating their knowledge semantically as they have to represent it anyway. Note that for them the work of semantic annotation consists in additional work.\n\nii.) Implicit Knowledge: The process of formalization of implicit knowledge is obviously hard as people usually do not reflect upon their 'hidden' knowledge and may not even know how to represent it at all. For example, contexts are experienced by people but rarely perceived as autonomous information.\n\n3. Knowledge Sharing and Reuse: Thinking in potential is a very visionary and inspiring way of design, but it doesn't quite start off where normal users tend to get on board. In particular, potential refers to the envisioned use situation, i.e. to the future, whereas the starting point for use is in the real situation, i.e. in the present led to by individual pasts. Even though the Semantic Web's founder Tim Berners-Lee was aware of the fact that \"instead of asking machines to understand people's language, it involves asking people to make the extra effort\" [BL98], he was so impressed by the possibility and necessity to manage the Web's data, that he oversaw the cost-benefits ratio involved for its users in their situation. Therefore, the assessment of the underlying ratio depends on the point of view taken, which is true for other reuse arguments as well.\n\nAt first glance knowledge sharing seems to be an attractive and economic idea, but de facto e.g. Carol Kinsey Goman can list at least five reasons why it might not be the case [Gom02]:\n\n• People believe that knowledge is power;\n\n• People are insecure about the value of their knowledge;\n\n• People don't trust each other; • Employees are afraid of negative consequences; • People work for other people who don't tell what they know.\n\nWith the still-growing acceptance of the World Wide Web, especially of its participative aspect, this hurdle seems to be lowered considerably. But the well-known scheme of 'user as consumer and producer' (also known as 'prosumer') for knowledge sharing (e.g. [Mor05,Wei02,Dow05]), that is so successful in the Web 2.0 era, doesn't resonate yet with the production of semantic data. Recently, the success-story of Social Tagging systems has started to be explored as a light-weight semantic annotation opportunity (see e.g. [BSZ07], [P03:KK08], [P04:KR08]).\n\nThe end purpose for semantic concepts and technologies consists in re-enlivening captured content into knowledge (see Figures 2 and 3). If we for the moment adopt the still predominant Constructivist standpoint (see e.g. specific variants in [Dew33,Pia74,Vyg78]) as a learning theory, then from an E-Learning standpoint, the appreciation of semantic data by the end-user presents a variant of the Control Problem in Education. In particular, Constructivism as learning theory states that the learning process is steered by the learner herself via adaptation and accommodation processes [Dew33]. But the typical application of using an E-Learning system presupposes organizationally or institutionally given learning goals. Then the question rises, how does E-Learning software present the content to its user steering her to a prefixed learning goal with self-steered learning processes? One solution approach consists in 'machineunderstable users' based on user modeling techniques, which in practice often turn out to be inadequate. This Control Problem is boosted by using technologies that draw on semantic data as they are created by potentially complete strangers and made direct use of by machines.\n\nTherefore, the price of using semantic data in E-Learning comes in various semantic currencies as well:\n\n4. Appropriation of Data: A user's appropriation of software can be compared to the process of using a library. Werner Sesink amplifies: \"Libraries can only collect. If they weren't visited by people, who appropriate the collected knowledge, then they would transform into collection points of empty language shells\" [Ses04, p. 136]. Appropriation is done actively (but not necessarily consciously) by the user. This activity does not refer to the operation of the object to be appropriated, it addresses the user's attitude and her evaluation of this object for adoption. John Dewey differentiates the terms 'accommodation' and 'adaptation' (see e.g. [Bel05,p. 64]). The former refers to the (passive) human capability of acclimatization to circumstances, whereas latter relates to humans' (active) handling and reinterpretation of given circumstances to their own supposed advantage. For the use of software, we can rephrase appropriation as a concretization process of the abstractions contained in software (like data models) that is done by the end-user (see e.g. [Ses04,Sch07,Sch97]). In the semantic case, an end-user additionally has to appropriate the data instances as they were created by (possibly) other people, hence the reified interpretation has to be re-interpreted by the user herself to revive the underlying meaning.\n\n5. Learning = Composition of Knowledge: The mere presentation of even a welltailored composition of 'captured knowledge' (= semantic data as 'interpreted data') to an end-user does not automatically yield learning. Note that we do not refer to the user's appropriation task here, but to the specificity of learning itself. Intuitively, 'learning' is related to a process of change: there is the experience of before and after. Formally, 'learning' is a model of explanation for the observation of specific changes that occur in the observed environment, which the observer accords to a (conceptual) system (following [Jün04,p. 73]). An instance of learning happens, when e.g. a student uses an E-Learning application and she masters a subsequent online quiz on the topic and an observer (the quiz evaluation function) relates the environment (student and E-Learning application) to a system (evaluation scheme with respect to achieving learning goals). Interestingly hence, talking about learning primarily yields information about the observer. In the example, the observer is a piece of software understood as a \"designer's deputy\" [dS05]. Learning is no activity (see e.g. [MD05, p. 30]), even though we can use 'to learn' as an action verb: we decide to learn a topic, but we cannot cause learning, we can just experience it as such later on. We can create situations that afford learning, so-called learning scenarios, but we can not willingly generate the learning process.\n\n6. Human Strategies for Interaction: Semantic data enable and require software agency. In particular, an end-user has to appreciate software as interaction partner e.g. to accept a system as a valuable educator. This has two consequences. On the one hand it means, that the end-user has to understand the specific roles and according expectancies which a certain software plays within an interaction. Daniel M. Wegner explains that \"the theoretical construct of roles seeks to understand regularities and consistencies in social behavior in terms of the directive influence of coherent sets of rules and prescriptions\" [WG82]. That is, roles are an efficient tool for communication of underlying values (see semantic currency 4). On the other hand, the end-user has to be able to appreciate the offered services. The latter is only possible if the services are of value and they are of value to the end-user.\n\nWilfried Brauer and Ute Brauer phrased the related problem of mutual understanding of machines and humans as \"migration of semantics\" [BB95], which indicates that our 'semantic handover' relates to the human-computer relationship as well.\n\nIn a nutshell, we can say that the analytical perspective onto the distinguishing features between mere data and semantic data yields distinct problems in their use. For instance, the interpretation of 'semantic' in the fields of Knowledge Management and E-Learning as 'machine-understandable' resp. 'interpreted' tends to result in a focus on data-resp. interaction handling. The lists of specifically semantic issues enhance the difference further.\n\nAs a consequence, I argue that design for semantic data and their use has to take care for these semantic issues and therefore has to be distinguished from traditional design.\n\nTerry Winograd broached the issues of what 'software' resp. 'design' actually are. In particular, he states that \"software is not just a device with which the user interacts; it is also the generator of a space in which the user lives\" [WBdYH06, p. xvii ]. Hence, the goal of software design needs to include as many approaches to use-oriented qualities as possible, since a (growing) part of people's lives depends on it. Here, the more theoretical Interaction Design approach in the tradition of [LS04,Sch07,Cra03] becomes important. I will refer to this subfield of Interaction Design as \"conceptual Interaction Design (cID)\" in the following. In particular, Jonas Löwgren and Eric Stolterman define this conceptual Interaction Design as the process \"that is arranged within existing resource constraints to create, shape, and decide all use-oriented qualities (structural, functional, ethical, and aesthetic) of a digital artifact for one or many clients\" [LS04,p. 5]. This describes what we need in order to overcome the semantic currencies.\n\nIn order to understand the distinctive features of conceptual Interaction Design, I will give a short outline of its history and a brief introduction to those designers with the most influence on my work.\n\nIn Alan Cooper and Robert Reimanns widely known guide \"About Face 2.0: The Essentials of Interaction Design\" [CR03], we find: \"Simply put, interaction design is the definition and design of the behavior of artifacts, environments, and systems, as well as the formal elements that communicate that behavior \". This was updated in [CRC07, p. 13] to \"the design of product and system behavior \". The latter definition focuses on the concrete art of Interaction Design, i.e. Interaction Design as \"design science\" [Kri06,p. 34] in which design is dealt with as a product. In contrast, I am interested in the abstract art, i.e. Interaction Design as a \"science for design\"[ibid.] in which design is handled as a process.\n\nIn the early nineties, the public success of the computer brought forth several studies about its foundations. Of special interest were the ones that originated not in the technological sciences, but e.g. in epistemology, the humanities, and philosophy. They led to a reconsideration of the paradigms in Computer Science and gave rise to new fields. In 1997, Peter Wegner proposed to review the foundations of Computer Science, which were built on algorithms and the Turing Machine, with a focus on interaction [Weg97]. In the same year, Terry Winograd envisioned \"Interaction Design\" [Win97] in a chapter in a wellknown collection of Computer Science prognoses \"Beyond Calculation: The Next Fifty Years of Computing\" [DM97]. Moreover, in [WBdYH06] he and others pushed the design topic onto software. For instance, Mitchell Kapor responded to the general question what design is: \"It's where you stand with a foot in two worlds -the world of technology and the world of people and human purposes -and you try to bring the two together \" [Kap06,p. 4]. In this sense, Terry Winograd argued for a more holistic design that is \"conscious\" [WBdYH06, p. xx ] about its overall effects. In particular, the design perspective is shaped by the quest for possible futures and the evaluation of their desirability (see [Kri06,28ff.]). As a consequence, design of interaction brings about a severe responsibility for the designer.\n\nThis was taken up in e.g. Jonas Löwgren and Erik Stolterman's \"Thoughtful Interaction Design\" [LS04] approach. They interpret the design of interaction as an arrangement process, in which given constraints and potential qualities have to be balanced optimizing the use from the user's standpoint. The required thoughtfulness covers the designer's responsibility and takes into account, that in the complex relationship between client, user, designer, and software the \"most minute, seemingly insignificant, change of the whole can have large and unexpected consequences\" [ibid., p. 10].\n\nWithin this tradition we can also consider Chris Crawford with [Cra03]. For him, interaction is defined as \"a cyclic process in which two actors alternately listen, think, and speak \" [ibid., p. 5], which he transfers directly to the notion of interaction between human and computer. He argues that the traditional Interface Design (including Usability and Graphics Design) only covers the processes of listening and speaking, whereas Interactivity Design centers around thinking [ibid., p. 10]. In particular, as only human ways of thinking impose human ways of understanding, Crawford calls for an interdisciplinary approach to the algorithms that create and condition human-computer interaction.\n\nClarisse Siekenius de Souza addresses Interaction Design on a semiotic level with her \"Semiotic Interaction Design\" [dS05]. She explicates: \"Whereas cognitive approaches focus almost exclusively on the users, and on what happens to them before and during interaction, our semiotic approach focuses on how the designer's semiosis (about the user's semiosis) can be crystallized in an interactive computer system that will communicate productively with users in the widest possible range of use situations\" [ibid., p. 104]. Moreover, she describes Interaction Design as a onetime-shot by the designer to communicate meaning between herself and concrete users. In this sense, it represents a \"designer's deputy -a communicating agent that can tell the designer's message\" [ibid., p. 90] with the connotation that a deputy has gotten all the instructions beforehand and cannot alter them in a given situation.\n\nFor Interaction Design in educational scenarios Heidi Schelhowe suggested to use 'interaction' as a theoretical category for understanding media in Computer Science and their use in learning scenarios under a more general, media-theoretic or societal view. \"After all, Interaction Design means to understand both sides, the social as well as the technical. Interaction Design has to equally comprise knowledge about the behavior of organizations and people as well as knowledge about their calculability.\" [Sch07, p. 71], see also [Sch01].\n\nAs early as 1983 (and rediscovered in recent years by many interaction designers) Donald A. Schön already argued that \"reflection-in-and-on-action\" [Sch83] is at the heart of the human ability to couple with ever-new situations: \"The unique case calls for an art of practice which 'might be taught, if it were constant and known, but it is not constant.' \" [ibid., p. 16/17]. In particular, he observed that from the predominant technical rationality standpoint, \"professional practice is a process of problem solving\" [ibid., p. 39], but \"problem setting, the process by which we define the decision to be made\" [ibid., p. 40] is ignored. A 'problem solving' approach to design has an eye on functionalities, whereas a 'problem setting' one on meaning. Translated into design paradigms this becomes 'form follows function' versus 'meaning matters more' [Kri06,47ff.]. People can deal with emerging, unpredictable situations that \"talk back \" [SB06,176] by considering their setting. Backtalk triggers human reflection processes on action with its explicit and implicit parts, thereby framing the emergent problems anew. As a consequence, Schön challenges Interaction Designers to understand the artifact they are designing and not its usability [ibid., p. 181]. Moreover, he calls for \"epistemic tools\" [dS05, p. 33] that help the designer to reflect, frame, and evaluate problems.\n\nIn all these conceptual Interaction Design approaches we find science for design, i.e. attempts to understand the space of possibilities for interaction with software. But following Wolfgang Coy's statement \"Understanding user interaction is an art -not a consequence of logic\" [Coy92, p. 271], I like to stress that this science cannot be built on techno-rational methods with 'objective' outcomes.\n\nThe presented conceptual Interaction Design vantage points focus on the user and her situatedness. They build a base for many concrete software design methods developed in the meantime that zoom in on the user like 'Human-Centered Design', 'Participatory Design', 'Personas', or 'Value-sensitive Design' (for a comprehensive list see [P02:KM08]). However, an analysis showed that even though these design methods recognize that the situated evaluation by a user is different from the evaluation by other stakeholders, they do not take into account the evolutionary process culminating in a user's value landscape. So although 'having the user in mind' is a step in the right direction, it veils the underlying problem of understanding a user's value judgments in the use process. As a consequence, I conclude that they do not pay enough attention to the artistry of user interaction, where I use the term 'artistry' in contrast to 'art' to stress the process versus the product aspect.\n\nTaking Schön's reflection-in-and-on-action as point of departure, I argue that a user's value landscape is induced by and induces the problem setting. Unless interaction is understood as a process, the user's problem setting is different from the problem setting the scientists offering solutions started off with. In particular, the respective problem setting is determined by the user's perception of the interaction context at that time. That is, the problem setting is more process-dependent than commonly addressed, for instance the addition of a problem solution can change the user's evaluation of the problem setting fundamentally. In this light, the notion of the 'user as designer' (e.g. in a Participatory Design context like [LS04, p. 152] or in the Web 2.0 context like [DC07]) might obtain an additional meaning, if we interpret the appropriation process of software or content as a design process.\n\nComing back to the semantic currencies extracted in the last section we realize that they concern the value judgments in the interaction with semantic data. In particular, the problems with semantic data all center around these value judgments concerning their use, e.g. creating, dealing with, or accepting them.\n\nI argue that the conventional, rather value-agnostic design of Knowledge Management systems or E-Learning applications is insufficient, especially for semantic technology. In [P06:Koh08] I suggest a \"Semantic Interaction Design\" approach which consists of:\n\n• A Conceptual Interaction Design Based on Values and Semantics: This stance centers around a user's value judgments (and their processual quality) which is decisive for a user's perspective when taking action. Klaus Krippendorf calls a change in design when meaning starts to matter more than function the \"semantic turn\" [Kri06].\n\n• A Concrete Interaction Design for Semantic Technology: The specific semantic issues with using semantic technology sincerely aggravate the cost-benefits ratio involved. Therefore, the value of software has to be reconsidered from the user perspective, i.e. a value-based Interaction Design is needed.\n\nThe polysemy of 'semantic' as a conceptual change ('semantic turn') and a concrete new technology ('semantic data') in the name 'Semantic Interaction Design' is intended. Even though I conceptually separated these points, I will focus on the second in the following since setting up a general value-based conceptual Interaction Design approach that is independent of the concern for semantic data would be a generalization that goes beyond the scope of this dissertation. I view Semantic Interaction Design as a unifying standpoint, that takes the interdependencies between semantic data-and interaction quality into account as relevant for the user's value landscape.\n\nBut what is decisive for the user's value judgments?\n\nIn [P14:KK04] Michael Kohlhase and I noted that the positive, but reluctant behavior of content authors can be explained in analogy to the well-known psychological construct of Axelrod's \"Prisoner's Dilemma\". It is used for analyzing short term decision-making processes in cooperation scenarios, where the actors do not have any specific expectations about future interactions or collaborations. Even though content authors do have expectations in form of appreciation of their work's collaborative potential, they are long-term ones. Therefore, we described their situation as an \"Author's Dilemma\". I revisited it in [P22:Koh06e] by rephrasing the Web 2.0 notion of 'user as consumer and producer' as 'user as prisoner'. By combining the author and reader role, a more conceptual stance on the underlying value landscape -based on which decisions for taking actions are made -was possible. In particular, in this landscape local and global optima can be found. The local optimum is the one that triggers action from a standpoint within a situation, a micro-perspective, whereas the global optimum can only be considered from an outside point of view, a macro-perspective. In particular, taking the micro-perspective can be interpreted as a tool for understanding the value landscape of a user. \"Both perspectives have their own explanative power \" [BG07, p. 676], which Johannes Bauer and Hans Gruber showcased in an educational workplace scenario. They concluded that the specific potential of micro-perspectives allow to understand to which degree context variables affect use processes. Note that the perspectives are not exclusive: designers as well as users can take up both, i.e. their resp. micro-perspective can be enhanced by other perspectives. In [P22:Koh06e], I showed that these competing perspectives are at the heart of the Author's Dilemma: It turned out that in the dilemma, the sensible way out (from a macro-perspective) is rationally inferior for an individual (from a micro-perspective).\n\nAs these perspectives are central to my solution approach in section 3.4, I like to work out now a Prisoner's Dilemma example in more detail. Concretely, think of the famous crooks Bonnie and Clyde. Even though they had worked hand in hand in their crimes, when they were caught and separately offered a deal by the public prosecutor, they were in a dilemma. In particular, since the prosecutor lacked complete evidence against the pair, Bonnie was offered exoneration in turn for her defection by testifying against Clyde. The catch in this scenario is that Clyde got the same offer. If both defected, then both would get more years in prison than if both cooperated. If we look at the prison terms available in the deal (Figure 4) from the macro-perspective, it is obvious what Bonnie and Clyde should do: they should cooperate. But if we put ourselves in Bonnie's shoes and evaluate the situation, how it presents itself to her, she has two options: Either she cooperates or she defects. In the former case she either gets ten or two years in prison, whereas in the latter she either gets five or is exonerated. Therefore, from her micro-perspective the rational choice is to defect. We see that the question about the user's perception of values is at the center of the Author's Dilemma. Therefore, I argue, that software has to be assessed within this user's situated value landscape -which builds on her specific micro-perspective.\n\nThe hard question in Interaction Design is how to understand what the user (from her point of view and judgment) needs for her practices as situated actions. If we push 'using' as culmination of value judgments into the designer's focus, then we have to think about ways how a designer can understand the evolution of a user's evaluation of problem settings over time. In [P02:KM08] Normen Müller and I noticed that the term 'Added-Value' has potentially different meanings from the micro-and macro-perspectives. In particular, who and what decides when what is considered a value vs. an Added-Value? In order to get a handle on this value landscape we suggested the Added-Value Analysis (AVA) method, which is based on what we called the \"double relativity\" of Added-Values. People don't think of Added -Values as alleviations but as cherries on a cake. In other words, Added-Values are assessed as gratifications and not as incentives. In particular, the difference between value and Added-Value consists in the subjective level of expectancy (1 st relativity). Moreover, whereas values are expected and serve reaching high(er) standards, Added-Values are experienced as a 'gooood' deal and more than expected (2 nd relativity). Elaborating on Added-Values allows us to think about the value landscape in various frames, thus enabling e.g. the understanding of users' micro-perspectives, discovery of unexplored needs, and separate research tracks.\n\nConcretely, if a designer fixes a core problem and thereby constrains the space of potential micro-perspectives, then she can explicate the trade-off scale by listing the (then perceivable) benefits and sacrifices for the core solution relative to the core problem. That is, the core problem together with the core solution allows an evaluation from the micro-perspective. This procedure is documented in a table-like structure:\n\nIf a sacrifice is incurred on the user in Step 1, this can be taken as another core problem for which either a solution exists or is needed -here, the sacrifice in Step 1 can be considered a trigger for the resulting problem/solution pair in Step 2. In the former case we can consider it as Added-Value, in the latter it becomes a potential Added-Value to the previous problem, which is marked as such in the second column of the table. If a row is triggered by another row in the AVA table, we label this process either by color or by a reference like \"; 2\". In [P18:KK07], where we applied the AVA to a specific Mathematical Knowledge Management technology, several interesting AVA catches were pointed out first that give valuable design hints and are cited in a nutshell to give an impression of the AVA's analytical strength: 0. The Quest for Objectivity through the AVA is vain. The wanted view of microperspectives for a better understanding of the processuality of use actions rather is an organized quest for subjectivity.\n\n1. Knowing the Answer Before the Question makes it sometimes very hard to find the real problem, since downsizing from the macro-to the micro-perspective is not easily achieved -which is especially true for the initial core problem that tends to be \"Saving the World\".\n\n2. Solutions are the Benefits This is usually a sign that the AVA analyst is not sufficiently nonpartisan with respect to the software author's motivation.\n\n3. Problems are Rhetoric Questions If that is the case, we arrived at a dead end for the AVA -possibly because of a too specific problem description.\n\nValues on Too Low Levels Benefits and sacrifices from a higher level should not be handed on to the lower levels as they are already dealt with.\n\n5. The Unfinishedness of the AVA Once the AVA is started one realizes very soon that in the analysis process there tend to pop up more and more problems and ideas, which is good on a creativity (or cognition) scale but rather bad on a satisfaction scale.\n\nIn my experience (four AVAs of systems, four on dissertations), many of these catches are prone to happen and their adjustment procures considerable insights. Overall, I consider the AVA to be an epistemic tool, that helps designers to better understand the change of potential value-landscapes over time by insight into the micro-perspectives of different users and thereby it supports reflection on the problem setting. Moreover, the application of the AVA to existent software often discovers new Added-Value services and faulty propositions in the interaction design.\n\nWe have seen that from a Knowledge Management perspective 'semantic data' represent 'machine-understandable data' versus 'interpreted data' from an E-Learning perspective. In both cases software can be written that makes use of the contained knowledge and interpretation. But this 'making use' often does not directly refer to the user but to the machine, i.e. the micro-perspectives of users are ignored. We argue that this problem is at the base of all listed semantic currencies. Therefore, our Semantic Interaction Design approach consists in using the micro-perspective as an analysis-tool for a designer's evaluation of situated use. This yields points of departure for successfully addressing the semantic currencies. That is, I use micro-perspectives for understanding the problem setting to obtain problem solutions. In particular, a change of frames allows to open up the designer's view to solve the underlying problems.\n\nConcretely, we will now address the semantic currencies from the last section one by one.\n\nKM Currencies (see section 2.2)\n\nAd 1. Handover of Semantics: The problem of handing over knowledge to a computer is the one which semantic content authors experience whenever they produce data instances.\n\nAt its heart often lies the discrepancy between potential and reality of content collaboration. In order to explain this in [P14:KK04] Michael Kohlhase and I used Axelrod's Prisoner's Dilemma already mentioned above. In particular, we argued that the underlying motivational problem between vast semantic potential and extra personal investment can be analyzed in terms of the Prisoner's Dilemma: the user needs to decide whether to take action or not in a cooperation scenario with unspecific expectations about the other's will to collaborate. Therefore, we can speak of an \"Author's Dilemma\". Moreover, we showed that the benefits of formalizing content for Knowledge Management lie principally with its readers, while the sacrifices remain with its authors. Concretely, we suggested to offer \"author-tailored services\"foot_1 in semantic Knowledge Management systems and gave CPoint examples like semantic data visualization in form of a graph (see section 4.9).\n\nMore author-tailored services were implemented in CPoint later on like an interface specialized for content authors (section 4.7) with an invasive LaTex editor for math input (section 4.6). Note that the line of argumentation is in accordance with the explanation of the Author's Dilemma via competing micro-and macro-perspectives (p. 13): From a micro-perspective these services generate specific expectations (i.e. they represent incentives) and dissolve the Author's Dilemma by slightly switching the problem setting. More generally we can say that the process of capturing content needs a potent (and not just a potential) context that can directly be experienced by end-users. In particular, such incentive services do not necessarily be semantic ones. The CPoint macros 'chopper' and 'shaper' described in section 4.5 are an illustrating example.\n\nIn [P22:Koh06e] the idea of a personal knowledge management system as a microperspective service was developed (with further advancements in [P04:KR08] and [P03:KK08]). At the beginning a social tagger doesn't think of her tags as public objects but as private ones, which she manages via annotation without even thinking about 'semantic handover'. The interest for other users' inputs comes later -whenever the individual user is ready. At that point in time we have a flowing transition from personal knowledge management to social E-Learning. CPoint with its extensive navigation feature (section 4.10), can be considered a personal knowledge management system for PPT objects.\n\nAd 2. Formalization of Knowledge: The translation of implicit as well as explicit knowledge into a formal format is difficult. But in the authoring process of semantic data necessarily knowledge has to be represented and formalized -which still needs to be done mostly by humans.\n\nThe micro-perspectives of software (via designers) and humans (via users) are quite distinct with respect to formalizations. Chris Crawford describes software performance (i.e. usefulness of formalizations for software) as \"It's just a machine acting out its algorithms.\" [Cra03,p. 113]. For a machine formalized data represent a direct motivation for the data's existence, whereas humans are not that easily convinced. A closer look at concrete semantic data formats can help: From a user's micro-perspective the underlying ontology for semantic data should be semi-formal , i.e. formal enough to let the machine act out some algorithms, but informal enough to alleviate the data's input e.g. the OMDoc format [Koh06]. An example can be found with CPoint as an OMDoc editor in section 4.4.\n\nEven though I just pointed out that the harshness of formalization should be alleviated by using a semi-formal format, its very formality nevertheless supports the explication of knowledge. For instance, by designing CPoint as an OMDoc editor, the formal part of the format guides the user in her formalization process. Concretely (see section 4.2), a CPoint author needs to decide early on whether the object to be semantically annotated belongs to the narrative part of the presentation or the knowledge part. The author has to decide whether an example (like an image of a blooming tree in a CS lecture on graphs) is one which intrinsically belongs to the presented concept or is used for demonstration purposes only. Note that this differentiation in CPoint (and OM-Doc) can also be considered a result from the Mathematical Knowledge Space idea, see [P15:KK05].\n\nIn [P18:KK07] Michael Kohlhase and I showed how the AVA can be applied in order to get a deeper understanding of the 'formalization of knowledge' currency. We applied the \"Added-Value Analysis\" to the specific math web search engine MathWebSearch, but the method quickly led to value (re)considerations that are relevant for the whole field of Mathematical Knowledge Management. In particular, Mathematical Knowledge Management is predicated on the assumption that by investing into markup or formalization of mathematical knowledge, it can reap benefits in managing (creating, classifying, reusing, verifying, and finding) mathematical theories, statements, and objects. This global value proposition has been used to motivate the pursuit of technologies that can add machine support to these Knowledge Management tasks. But this (rather naive) technology-centered motivation takes a view merely from the global (macro-) perspective, and almost totally disregards the user perspective and motivations for using it, the local (micro-) perspective. For instance, on the one hand, formula search engines assume the user's capability of thinking in formulae to use them, on the other hand, they dream of either layman users (that want to pose math questions) or professional mathematicians. For the first a graphical user interface was implemented, but for the latter who typically still prefer pen and paper above editors, OCR methods would be best as input technology. We posed that currently there is an inequivalence between query and search result from the user perspective: the query has to be made associative, lightweight, and not verifiable, whereas the search result needs to be trustworthy and verifiable.\n\nMore generally we can say that faulty propositions about users (e.g. uncovered with the AVA method) can easily prohibit their use of a system. Especially for the required formalization process the user's value landscape has to be taken into account otherwise we stumble into the above antagonism yielding non-usage of semantic technology.\n\nAnother example for such an antagonism can be shown with the software package MS PowerPoint when used in educational scenarios as here the use of PPT is practically restricted to educators (see [P11:Koh07c]) whereas students could also make good use of it: In [P12:Koh07d] an analysis of the user perspectives of students on the concrete interaction quality of PPT was elaborated on. In particular, I discussed the distinct notions of 'enabling' and 'engaging' software especially from the students' point of view, as they belong to the new digital culture with their specific expectancies towards software. The result was, that PPT as a whole can be considered an enabling technology for teachers as well as students. In contrast, the engaging component of PPT consists in its development environment, which is almost always exclusively used by educators.\n\nAnother solution approach for alleviating the translation from knowledge into content consists in automation. For instance, reified knowledge in documents could be automatically gathered and classified for supporting a pre-final (and hence less time-consuming) formalization by the end-user. Unfortunately, this automation is still too coarse for satisfying the needs in educational scenarios with their rather precise goal-orientation.\n\nAd 3. Knowledge Sharing and Reuse: If we have a closer look at the issues stated in this semantic currency, we realize that the given reasons were from the micro-perspective. In the vision the benefits are with all participants, but the costs (like giving up the status of having unique knowledge) are with the authors promptly (see [P14:KK04]). We therefore propose that the costs need to be lowered (whereas we suggested incentive services for handling the semantic handover).\n\nIn [P02:KM08] the Author's Dilemma was slightly extended to a Semantic Prisoner's Dilemma as collaboration is a feature of most semantic systems and their users not just the authors. Here, we called for \"semantic Added-Value services\" to dissolve the Semantic Prisoner's Dilemma. Note that under 'Handover of Semantics' we suggested incentive services, whereas here we proposed services as gratificationsfoot_2 . These services can be especially neat as semantic data are more than mere data. For example, once a PPT author has semantically annotated her presentations via CPoint, she can use them like a well-structured library of PPT objects.\n\nInterestingly, from the micro-perspective the motivation scenario in the Author's Dilemma is not built around the action of using semantic data but it refers to the decision of taking the action of using. Therefore, I started to explore the situatedness of action [Suc87, WF86], particularly the \"here-and-now \" of a content author. In particular, I pointed out that the here-and-now is based on yesterday's actions and experiences and their consequences.\n\nFor instance, essential editing functionalities needed by content authors are already wellpracticed by them -though in proprietary systems like the MS office suite (worked out in [P07:Koh05a]). Often, this legacy situation is completely disregarded by scientists or designers as they are convinced that their conceptual approach is the better one for any user. I attributed this ignorance to a common shortcut from attitude to action: attitude is too often assumed to translate directly into actual behavior, i.e. action. In particular, if the ideas are committed persuasively, then the user will appreciate her opportunities and will gladly use the software product. This shortcut misses out on the act of decision taking with its according evaluation process (see e.g. Klaus Holzkamp's \"motivations for action (Handlungsbegründung)\" in [Hol95]). The value of an action is determined from within a user's situation, i.e. from the micro-perspective. Therefore, I argued that the past, the present, and the future have to be accounted for in a software design process.\n\nThe future can usually be well addressed with a new technology's potential, and there are many counsels for alleviating the present (e.g. Stephen Downes' \"Nine Rules for Good Technology\" [Dow04] or the entire Usability profession). The past though hadn't been paid enough attention to. As a consequence I worked out the idea of \"Invasive Technology \" [P07:Koh05a], covering services that extend commonly used applications by new functionalities, with the invasive, semantic editor CPoint as an example (see section 4.1) -which broached considerable interest e.g. [P10:Koh07b].\n\nIn [P18:KK07] the AVA unveiled that the assumption of mathematically knowledgeable users was used to presume their appropriateness for serving as content authors (strengthening the argument of reuse of content by knowledge sharing). If we openly acknowledge this macro-intention, then such users should be supported within their natural 'habitat'. I took this up further and implemented an invasive, LaTex-like math User Interface into CPoint (that as pointed out above is already an invasive editor itself): its mathematical user interface CMath, which fully integrates mathematical symbols into PowerPoint presentations based on the semantics of the underlying objects rather than simply generating appropriate ink marks (section 4.6), was presented in [P20:Koh04].\n\nFrom the micro-perspective issues of privacy become relevant for knowledge sharing. In [P04:KR08] Milena Reichel and I therefore suggested a smooth transition between privacy and publicity as a solution to overcome at least some of the knowledge sharing concerns mentioned. In particular, applications like tagging systems can first offer services in a user's closed environment and later-on (whenever the user is ready) in an extended context. Note that these services may or may not be the same.\n\nFor content reuse in general the size of the content elements needs to be rather small as the scenarios of use (especially in education) vary a lot. Aggregation can be the more flexible the finer the granularity . Unfortunately, the content context which usually has to be formalized as semantic information can be interpreted big or small. For example, the content context of an image in a PPT presentation could be either determined to be the presentation itself or the underlying concept. For better reuse quality therefore CPoint follows a \"little theory approach\" [FGT92] where the semantic objects are the smaller the better, see [P09:Koh06c]. In particular, it establishes PPT objects like images or text boxes as autonomous (semantic) objects that e.g. can be reused as learning objects (see [P21:Koh06d], or sections 4.2 and 4.3).\n\nEL Currencies (see section 2.3) Ad 4. Appropriation of Data: To be able to de-interpret the interpretation contained in semantic data, an end-user needs context information. From the micro-perspective the context is an emergent effect informed by various information streams like the concrete use situation (\"Where, when and why do I use it?\"), the content situation (\"Where does the content fit?\") and the interaction situation (\"What is my relation to the interaction partner?\"). Importantly, the content situation is not only relevant for software trying to fit it to the needs of the user, it is also important for a user's interpretation. Therefore, we argue for a transparent interaction design with manifold layers as realized for example with the CPointBasic panel versus the full CPoint menu bar (section 4.5). This also includes e.g. Community of Practice information (see [P16:KK06]). In particular, this concerns the quality of the data format as it has to afford these content layers (elaborated on in [P03:KK08]). Moreover, the appropriation of data is eased by a clear recognition of \"software as full partner \", see also item \"Ad 6. Human Strategies for Interaction\"). Note that viewing the context as an emergent effect is quite contrary to the previous assumption of a computable or predictive user context.\n\nConceptual metaphors play a central role in appropriation processes. Note that they are not globally valid, we as humans use \"personal metaphors to highlight and make coherent our own pasts, our present activities, and our dreams, hopes, and goals as well \" [LJ99, p. 233].\n\nIn [P19:KSL07] we were interested in the relation between those personal metaphors and interaction design. We explored unexpected approaches to software with the help of children. In particular, we considered children as experts for different appropriation processes based on the idea of the \"Hundred Languages of Children\" observed by Reggio pedagogue Loris Malaguzzi, e.g. [EGF98]. In order to understand the process of being engaged by and engaging with software, we investigated this reciprocal aspect of engagement and its relationship with imagination and conceptual metaphors. Concretely, we studied the conceptual metaphors used by children appropriating a software package that was definitely not designed for children, but for adults: MS PowerPoint (PPT). Stunningly, they compared PPT with a theater, an archive, and a film crew and persuaded us that they also used it as such. In the following, I distinguished use metaphors, which describe the intended functionality and its handling -the 'whatcan-be-done' -from \"appropriation metaphors\", that determine the relationship between the end-user and the system -the 'how-can-it-be-done'. These appropriation metaphors represent a prospective point of departure for supporting the additional appropriation of semantic data.\n\nAd 5. Learning = Composition of Knowledge: Even though we phrased this point as a semantic currency, it really is a problem of learning software in general. Semantics comes into play because in semantic terms knowledge is decomposed into content (KM), which in turn is captured (KM), subsequently tailored and delivered (EL), and finally, it 'just' has to be reassembled (person). From a micro-perspective software can only offer informative content and it is the sole responsibility of the end-user to take this opportunity up. Supporting the end-user e.g. by fulfilling usability criteria like [Shn98], Human-Centered Design [ND86], or even Emotional Design [Nor04] is possible and desirable, but no guarantee can be issued for learning to really happen. Even though changing the frame of learning as a controllable feature to an uncontrollable one hasn't yet led to a solution, it is an eye-opener for future work.\n\nIn [P04:KR08] and [P03:KK08] tags within folksonomies (understood as lightweight semantic data) were introduced as reflective material or \"embodied conceptualizations\", which seems to be a good prospective point of departure for addressing this currency. We will see later in section 4.9 that CPoint already features tagging and graphs as replacement option for folksonomies.\n\nMoreover, the method of framing can be considered of help: If learning software affords framing processes (like Ackermann's \"diving in and stepping out\") then reflection as a practice for dealing with content is suggested to the learner (see also [P06:Koh08]). Different views on the same semantic data as realized in CPoint's visualization feature (section 4.9) are one example. Another one consists in the generation of flashcards for learning on different levels based on a PPT presentation semantically annotated with CPoint (section 4.8).\n\nAd 6. Human Strategies for Interaction: Interaction with people is not at all as simple as humans experience it. We do have certain communication mechanisms at work, which we usually do not reflect upon. From a micro-perspective it is desirable that software becomes eloquent in human interaction as well. Moreover, it has to showcase what an end-user can and cannot expect as clearly as possible.\n\nOn a very fundamental level Interaction Design influences the instances of humancomputer relationship. In [P05:Koh07a, P08:Koh06b] I was interested in the underlying assumptions for interaction in a learning scenario, especially the relationship between learner and learning technology. I argued that the recent stress on placing the learner in control is necessary, but not yet sufficient, as it addresses the relationship between learner and learning technology as a one-sided partnership. But only if a learner trusts the technology to be a \"full partner \" the learning conditions become optimal. I scrutinized three distinct math tutoring systems in order to understand how the interaction design can afford mutual partnership. The resulting design requirement called for a conceptual strength/weakness analysis of the system, so that real strengths are recognized and explained to the user in the system design. Note that this is not self-evident, as the general line of thought consists in \"eradicating weaknesses leads to good products\", not taking into account that human beings are \"Möglichkeitsmenschen\" [Med87] that adapt easily e.g. in partnerships -if and only if it is worth the trouble.\n\nEloquence in interaction can also be interpreted as a software's appreciation of the respective roles a user might have to fill out in the course of interaction. For instance, we can distinguish the PPT author whose goal is to represent her knowledge in a digital fashion from an educator who is creating PPT with the aim to help students to understand. Naturally, humans can be both (or more) at once but respective CS models need to take the differences into account. CPoint e.g. offers customizable, precategorized PPT object generation for educators who want to style-code their PPT content for their students (section 4.7). Moreover, CPoint features a view on semantic data especially for students (section 4.8).\n\nReviewing all semantic currencies and responsive solution approaches, we deduce from the Semantic Interaction Design standpoint that the Knowledge Management currencies (based on their data focus) can be considered as interaction conditions, and the E-Learning currencies (based on their interaction focus) as data requirements, the latter of which Michael Kohlhase and I elaborated on in [P03:KK08]).\n\nIn summary, I suggest that the use and usefulness of semantics in educational scenarios depends on a design that includes the interdependencies of data and interaction taking the micro-perspectives of end-users into account. Concretely, I propose a Semantic Interaction Design approach that additionally cares for the specific issues coming along with 'semantics' in Knowledge Management and E-Learning. I showcased the AVA as a suitable interaction design method and concrete conceptual solutions for addressing the semantic currencies. Now, I will present the software system that allowed me to follow the cycle of \"diving in and stepping out\" [Ack04], i.e. to learn conceptually by experimenting and experiencing concretely. A general introduction to CPoint can be found in [P21:Koh06d] and [P01:Koh06a]. Moreover, a manual for using CPoint [P25:Koh05b] can be found on the CPoint websitefoot_3 , where the binaries and source code can be downloaded as well.\n\nCPoint's goal is to provide a PPT author with an interface to explicitly store semantic information (knowledge) in the PPT slide show itself without constraining the presentational aspects of the PPT document. The system makes its functionality available through a toolbar in the PPT menu (see Figure 5) where it is at an author's disposal whenever the PPT editor is running.\n\nThe top-level structure of a PowerPoint presentation is given by slides. Each slide contains PPT objects, e.g. text boxes, predefined shapes, images, or tables. By using CPoint the author can attach additional information to each PPT object so that it becomes an autonomous learning object. First, CPoint allows to categorize each object (section 4.2), then these categorized objects can be related to each other (section 4.3), and finally, it can be made use of in form of semantic services within PPT or -by converting it into a sharable format like OMDoc (section 4.4) -outside of PPT.\n\nIn the introduction I established a set of questions Q* (p. 37) that guided CPoint's further development. In particular, I wanted to elaborate on the \"real benefits and sacrifices\" of putting 'semantics' into the PPT equation. On the other hand, I aimed at exploiting the structured quality of semantic data for all PPT stakeholders in an educational scenario. Fig. 6: Distinct Views on a Digital Document Applying the Added-Value Analysis to MS PowerPoint (PPT) [P24:Koh07f] resulted in a rather simple, but powerful observation: Contrary to popular perception, PPT is more effective as a tool for optimizing the production of presentations than as a tool for optimizing the presentations themselves. This difference had been neglected up to [P11:Koh07c], even though from a pedagogical standpoint it is crucial: If the PPT system is optimized with respect to the PPT document, the emphasis lies on its educational effect for students; whereas if its strength lies in the production environment for presentational documents, then the usability of the environment with respect to its expressional value for the educator is in focus.\n\nAs PPT is often used in educational contexts like lectures, I asked whether PPT might be extended so that students become beneficiaries. I argued that PPT documents enhanced by semantic features are pedagogically valuable for educators and learners, as both can benefit from services afforded by the semantics. The argument was supported by showing roledependent views on semantically annotated PPT content with the modules CPointAuthor and CPointStudent (sections 4.7 resp. 4.8), which extend PPT to a semantic PowerPoint (Figure 6).\n\nIn the following I will introduce the CPoint application as a response to this call for an educational extension of PPT with semantic technology. The description is structured along CPoint features, so that these can be cross-referenced. In particular, when addressing the semantic currencies I reference the respective CPoint solution approaches as examples for benefits and sacrifices. Therefore, I can now focus in the interpretation on their educational contribution.\n\nThe uptake of digital media for use in school by educators is still rather low (e.g. [Dic03]). Hence, potential hurdles for use have to be lowered specifically for educators as users. As we have seen (Figure 5), CPoint was realized as a PPT menu bar and is therefore at a user's disposal whenever the PPT editor is running. Moreover, the added functionalities are reached within the \"same look and feel\"-frame as other PPT tools facilitating its use and building on previous user knowledge. Even though CPoint is Open Source software, it can be made use of in the proprietary MS software, we can say it 'invades' PPT. In [P07:Koh05a] I generalized the concept of \"Invasive Technology\" on the basis of CPoint as an invasive editor for semantic content. CPoint's math user interface (section 4.6) is an additional invasive component as it allows LaTex-like input of math within PPT.\n\nAs CPoint wants to support modeling the implicit knowledge in a PPT presentation, it is geared towards a typical user's classification process of knowledge. Here, it is assumed that Fig. 7: CPoint's Categorization Form the explication of knowledge is simplified if one starts on the conceptual (frame) level to finish on the more detailed, concrete level (section 4.3). Therefore, the semantic annotation process of a specific PPT object (selected by focusing on it with the cursor) begins by opening the \"Categorize\" form (Figure 7), linked to the respective item on CPoint's menu bar (Figure 5).\n\nThe reflection on the object starts with the decision on an object's title (universally colorcoded in orange). Then the user has to determine the didactic role of the object within the setting: is it a narrative element (like the blooming tree as an irritating example for a CS tree) or does it contain semantically relevant content? In order to facilitate the content management, the user can also formally point out that the object at hand is just a repetition resp. reformulation of another element.\n\nDepending on this choice the user is offered a list of categories for the object in question. In contrast to a user's freedom in assigning a title to the object (which therefore can be considered a tagging instrument), here she is restricted to this list of categories to help her in the formalization process of implicit knowledge.\n\nThe particular categories are modeled after the semi-formal OMDoc format. In Figure 8 you can find an example of this list when the narrative role is active. The gray items and their respective subcategories become active in the 'Content' case. The basic information in each form is continuously colorcoded in yellow, whereas minor information (from the OMDoc standpoint) obtains a green/middle blue color-code. For instance, in this categorization form the content type is considered such a minor information as it only describes the form of the selected object in the semantic document to be. Here, even though the object is an image it will be dealt with like common text. The option 'graphics' is typically chosen when several PPT objects are grouped into a PPT unit and this unit should serve as semantic object. Then it has to be generated as an autonomous image in the conversion phase (see section 4.4). The action buttons at the bottom of the categorize form either link to other forms or do what they announce. The 'Metadata' button links to a form which contains Dublin Core metadata like creators or contributors, see [WKLW99] for more information.\n\nCPoint's categorize form contains one element which does not refer to the OMDoc format, but is induced by the PPT format: The sequentiality of its slides. This narrative linearity and the underlying semantic structure are not necessarily refinements of each other (e.g. yielding two distinct documents in the conversion process, see section 4.4). Therefore, I introduced the notion of sequels. A \"sequel\" is a PPT object which is conceptually a part of another (the 'main' object), but which is not located with its main object e.g. on another slide. Once the check box 'Sequel' is checked (Fig. 7), the user can select the object it belongs to from the adjoining list box. A sequel object does not carry annotation data of its own except the content type.\n\nIn particular, if the user selected a sequel object and opens the content form, then the respective content form for the referenced main object is opened. On the other hand: if a main objects content form is opened, then a sequel list on the form appears from which the user can select the sequel which is shown as an image on the form. Additionally the order of the sequels (relevant for all conversion processes) can be determined within the sequel list by assigning rank numbers. For each category there is information that specifically refers to this category, the category dependent content. For instance, once the image of the blooming tree has been categorized as 'Example', it becomes interesting what it represents an example for. In Figure 9 this detailed content information form is shown (compare the differences e.g. with the Definition Form in Figure 10). Note that the 'Object' field is necessary, as we often find a narrative description of an example in a PPT presentation, which isn't the example itself. In Figure 10 there is CPoint's definition form for the text box-object which contains the defining description of a CS-tree. A copy of the object in question is shown in the right area of each form to provide an objectto-think-with (without its embodied qualities though) for the user -supporting the notion of 'user as designer' -here the CPoint user, i.e. the author or semantic annotator.\n\nIn each category dependent CPoint form, there is a comment field for input of arbitrary text. This way, an educator using CPoint can either comment on an object, she can send messages/notes like \"important for exam!\" to her students, if they use the CPointStudent module (section 4.8), or she can add extra information like an anecdote presented in class that would otherwise be lost.\n\nWith the help of these category dependent content forms the user can develop a structured intuition for the underlying knowledge fabric, moreover, she explicates it piece by piece to obtain in the end an explicit mapping of her knowledge structure.\n\nTherefore from a Constructivist viewpoint, CPoint can be considered an educational tool , which is nice to have for educators, but which could also be made use of by students (see section 4.9 for a start and [P09:Koh06c, P12:Koh07d] for a vision).\n\nSo far, I have presented CPoint as a semantic and invasive editor, but it can also be considered as an OMDoc editor (see e.g. [P01:Koh06a]). The forms are designed in such a way, that the semantically annotated content can be directly translated into the OMDoc format [Koh06]. In other words, a PPT presentation semantically enhanced via use of CPoint can be converted into an OMDoc document (see menu item 'Convert' in Figure 5).\n\nAs OMDoc is an XML format [BPSM97], which is well suited for document sharing over the web, PPT objects (and -documents) become 'web-able' through CPoint. 5In 2005 Michael Kohlhase introduced two OMDoc document variants: a narrative and a content one (see [P15:KK05, P03:KK08] for the distinction between content and form and its consequences). The general idea consists in the observation, that content itself is much more resistant to change than preparation of content for actual presentation (in the broadest sense). Therefore, a conversion of one PPT file into the OMDoc format yields two files with content resp. narrative information linked to the content. Note that the categorization of PPT objects separates between 'Content' and 'Didactics' (see Figure 7) which corresponds to the above distinction.\n\nThe conversion of a PPT document into the 'pres-OMDoc' format is especially designed for the conservation of relevant PPT presentation markup within an OMDoc file. As a consequence, such a 'pres-OMDoc' file can be imported by other users via the complementary CPointImport module without much loss of layout. Even a general document in the OMDoc format (including 'content-OMDoc') can be imported, but the generated design is still wanting. It is based on a custom (local) CSS file [BLLJ98] that determines the layout of a PPT object to be generated depending on this object's category. Additionally, the PPT file can be converted into a simple HTML document.\n\nI also accomplished an ad-hoc generation of a special OMDoc sub-format adapted for input into the LeActiveMath system for math tutoring [Act00]. It allows a user to start and stop the LeActiveMath application, to check for errors and to set parameters. Furthermore, it includes a utility for converting an entire PPT collection into an LeActiveMath course. Additionally, it implements an LeActiveMath guided tour button in the context menu of each object. This button causes LeActiveMath to create an individualized, self-contained document that leads up to the knowledge embodied in this object [MBG + 03]. Unfortunately, this work was ceased in 2004 because of missing support on the part of the LeActiveMath team. Moreover, we have started -prototypically -the communication between CPoint and an OMDoc data base system called MBase [KF01]. Note that the first application can be considered an E-Learning system, whereas the second represents a Knowledge Management one.\n\nThe original CPoint system was purely form driven, but \"Forms are dead! \" according to Erik Duval in [Duv05] or at least should be -which is all very well to say, but principally from a Knowledge Management standpoint rather hard to accomplish.\n\nIn order to carry off some of the burden, only the basic information for such a form was singled out, i.e. the category, the orange and the yellow area, and comprised in a panel interface (see Figure 11). In particular, as the panel stays open as long as the user wants it to be there, forms do not have to be opened by the user over and over, so that the interface is perceived as only one form to be dealt with. Whenever the user selects a PPT object, the already annotated basic semantic information is shown in the corresponding fields. All of this information can also be created or updated from within the panel. Moreover, CPoint's navigation feature 'GoTo' (section 4.10) can directly be reached, so that a user becomes mobile within the semantic context.\n\nWe can say that the overwhelming abundance of information push and pull in the original CPoint menu bar is taken back in the double sense of Werner Sesink's call for \"contained technology\" [Ses04, p. 96] for which he also uses the house metaphor: software protects against outer forces like information overflow, but it enables a space of opportunities from within. Note that above push and pull naturally appears with semantic data, e.g. all the semantic currencies in Knowledge Management.\n\nThe panel serves as an entry door for the CPoint menu bar with its extended functionalities. As some people do not favor panels, CPointBasic also offers a limited menu bar in accordance with above containment (Figure 12). This menu bar allows to activate or deactivate the panel itself, it provides access to CPoint's management facility, and to facilitate navigation it offers a link to the GoTo Form and a GoBack List with a collection of links to the last visited PPT objects.\n\nSemantically distinct text objects are often combined in PPT's infamous \"bulleted lists\" [Tuf06] for a unified presentation. In order to simplify the annotation process, the CPoint-Basic menu bar offers the macros \"chopper\" and \"shaper\". The former chops a selected text box object into the next finer available level of granularity, e.g. it breaks up one list into its respective items as autonomous text boxes under the condition of keeping its original layout. The latter offers the inverse functionality: it assembles a set of selected objects (in the order of selection) into a unified object. Here, the layout criterion can not be fully kept as it is unclear, which selected item should be the authoritative one.\n\nAnother feature of CPointBasic consists in its multilinguality or localization. The default language is English, but if the add-in is loaded when a German PPT document (that is, if the language parameter of the document's properties is set to \"de\") is open, then the form appears in German, e.g. with German labels and tool tips (see Figures 12 and 13). These languagedependent information is generated based on the CPoint library 'interlingua' and is not hard-coded. In particular, the panel itself is modeled as a class with language-dependent instances. The localization library could therefore easily be extended to other languages as well. We can interpret this feature as an incentive service as it is supposed to considerably lower the hurdle for entry into semantic technology for use in the school environment.\n\nNote the different color coding in Figures 11 and 13 in the category field (which is completely language independent): On the one hand, it indicates whether the CPointBasic menu bar is visible or not, on the other hand, it hints at the fact that the field is a command button (activating the menu bar) and not a mere label. If a PPT user wants to create formulas on slides, she typically suffers because PPT is not equipped for math input except via its \"Insert Symbol\" command. Here, any improvement would be helpful. Note that we have to deal with two contrasting requirements: user-friendly creation and presentation of symbol objects (as building blocks of mathematical formula) in PPT and their automatic conversion into formal, structured OpenMath expressions in the to be generated OMDoc document (see [P20:Koh04]).\n\nPragmatically, CPoint makes use of two already existent LaTex converters: T E XPoint [Nec03] -that when I decided to build on it was still Open Source -and LaTeXML [Mil07], which still is. The former translates LaTex input into PPT glyphs, whereas the latter transforms LaTex commands into the XML and OpenMath formats. As user-friendliness was one criterion, for convenience CMath makes use of already existent T E XPoint facilities, even though they could be replicated with extra work in CMath itself. Fig. 15: Symbol Presentation Figure 14 shows an example math expression and its transformation, which can be deployed e.g. from the CPointAuthor interface seen in Figure 17. With the 'Math' command for example the string \"\\alpha\" converts to the Greek character \"α\" as PPT glyph. As \"\\alpha\" is a standard LaTex command, this is automatically covered by T E XPoint, whereas the string \"\\ourP lus\" does not represent a standard LaTex command. Here, the pixels to be generated on demand for the PPT glyph by typing \"\\ourP lus\" have to be defined with CMath. Concretely, the Symbol Presentation Form has to be used, see left-hand form of Figure 16. In particular, in this form a user can define for every symbol object a macro of the same name which allocates the desired set of characters and their layout when used with a backslash.\n\nNote that in Figure 14 we find the expression \"\\ourP lus{\\alpha, \\beta}\", i.e. the symbol 'ourPlus' represents a function with two parameters. The style of presentation of the parameter list is customized with the 'Presentation Properties (PP)' form (see righthand form in Figure 16 called via the 'PP' buttons in the form on the left), so that the generated PPT expression for it looks like \" ‡[α|β]\".\n\nThe disambiguation between content and form enabled here lifts mathematical objects automatically onto a semantic level, therefore we consider this feature as a value service addressing the semantic currency 'handover of semantics' as the opportunity for 'nice' math input represents an incentive for using it.\n\nA shortcoming of T E XPoint is its missing capacity of re-translating generated inline PPT glyphs back into the LaTex style. Therefore, CMath has extended T E XPoint's mode for inline mathematics to a full mode, i.e. even inline math expressions can be ascribed to the respective LaTex ones now and are therefore recoverable with the 'Symbols' command.\n\nThe mathematical user interface can be either called from the menu bar (Figure 5) or from the CPoint interface for content authors, which is described next. The CPointAuthor panel (see Figure 17) is based on CPointBasic. While the facilities described in the last paragraphs concentrated more on the semantic enhancement of existing PPT objects, CPointAuthor focuses on the creation of semantic objects in the content authoring process. In particular, it specializes in offering functionalities for users who take up the role of the content author for the time being. We discern three distinct areas within the CPointAuthor interface: basic semantic information, the CMath mathematical user interface, and the creation region.\n\nWith a few exceptions the first was described in section 4.5. It is extended by the command buttons 'C' (Categorization) and 'D' (Details). The former links to CPoint's categorization form (section 4.2) and is necessary as the category field itself offers no categorization facility in contrast to the CPointBasic panel. The latter represents a link to CPoint's category dependent content forms (section 4.3) to input directly detailed information for the PPT object in question.\n\nTo streamline formalization of mathematical knowledge, CPointAuthor offers a simple graphical user interface for CMath in the second area. On the one hand, with CMath (section 4.6) the input of math formulas is simplified by using an invasive LaTex-style editor. On the other hand, by the underlying semantic preparation of math formulas, resulting glyphs are not the finished product of the authoring activity. In particular, common errors like mixed up indices can be easily corrected afterwards as the LaTex-like structure is stored and can be recovered. This is important over an extended document lifecycle as envisioned in the reuse scenario for semantic data. Moreover, we can consider this recovery feature as a gratifying service for PPT authors as called for in section 3.4 when addressing the semantic currency 'Knowledge Sharing and Reuse'.\n\nAs CPointAuthor is geared towards the creation of semantic PPT objects, it offers to generate pre-categorized, pre-styled PPT objects in the third of CPointAuthor's areas. The presentational properties of these objects are preset by the author's personal preferences, which can be individually configured in the CSS file associated with CPointAuthor. Note that this CSS file can be the same as the one for the CPointImport module (section 4.4), but doesn't have to be since it can be managed via the CPoint Manager, a set of functionalities to parametrize CPoint (found in the drop-down menu 'CTools' in the menu bar in Figure 5). Currently, PPT documents (or the respective exported PDF files) are handed down to students as a kind of \"cheat sheet\" to remember the \"show\". The document is static (often printed) and thus leaves no motivational or interaction opportunities for the student. In particular, such a handout (printed or not) does not exploit its semiotic qualities. Therefore, CPoint was extended towards remedying this situation by offering the handout as a living document that can be worked on in the PPT work environment using the CPointStudent module geared towards use by students. CPointStudent allows to mine semantic information contained on PPT slides through a panel interface: it offers a visualization mode in which available additional content like category information for slide objects can be directly perceived (without having to open forms) by clicking on an object.\n\nIn particular, the student's view on a PPT document allows (in analogy to CPointAuthor) to browse, update, or create basic information annotated to a specific object. A student can observe how her teacher dealt with a PPT object, e.g. how it was classified and how she combined it with other content. Therefore, PPT objects become potential reflective material towards which students can take a stand. In a Constructivist sense, students can reor de-construct content in order to adapt it to their own knowledge. Also, this way distinct formalizations can become topics for discussion in class, allowing the PPT document to serve as a far more intense learning resource. We can easily imagine an interactive CPoint component that supports in-class collaboration, but unfortunately, this went beyond the scope of this thesis.\n\nFor PPT objects to be lifted to a learner's reflective material, navigation in and overview of PPT content, the set of relevant PPT objects, is very important. The former is supported in CPointStudent by providing a direct link to its 'GoTo' navigation facility, which is a simplified version of CPoint's general 'GoTo' interface (section 4.10). The latter was shown to be effective for educators with the semantic visualization of content on various levels with the CPointGraphs module (section 4.9), and we may safely assume that this is the same for students. So CPointStudent provides the learner with a link to the graph viewer. The default values are set to show the theory graph for the current PPT presentation. We can say, that these visualization opportunities offer a logical/functional overview of the knowledge to be learned.\n\nA general problem, when developing CPoint consisted in the fact that added semantic information should not be visible to the audience in the PPT show, but it should be available when working on the document in the PPT development environment. In practice, it soon became clear, that the information which object was already semantically worked on and how had to be a directly available and not just on demand. Therefore, CPoint offers a \"visualization mode\" and a \"hide mode\". In the latter, all annotated CPoint data are hidden, in the former some first-rate information is visualized. In particular, in Figure 19 on the right-hand side we find a yellow label with the object's category 'Example' followed by its type 'text' in brackets, and its title in the next line. 6 In each content form the CPoint user can add extra information or comments to an object. This additional content can be seen in the visualization mode in the green label on the upper left-hand side of the corresponding object (see Figure 19). In CPointStudent an educator's input of a comment is reinterpreted as a message from teacher to student, a \"T(eacher)-Note\" that cannot be overwritten. A student can decide for herself whether she wants to see the T-Notes in the presentation by using the according radio buttons in the T-Note area (see Figure 18). As it might be important for a learner to put in her individual comments called \"S(tudent)-Notes\", CPointStudent allows her to do that by clicking on the T-Note label which in turn brings the blue-colored editable comment field to the foreground. A click on the S-Note label switches back to the T-Note area. The development of T-and S-Notes were triggered by our considerations in [P16:KK06], where we argued for the integration of Communities of Practice into semantic data formats. In particular, we found 'relevance' to be of importance for interpretation of content within such communities. If we consider a class with its teacher a Community of Practice (CoP), which besides knowledge (\"Which topics are taught and in what order?\") also values constitutive terminology, then the T-Notes supply a CoP-specific service. We can also appreciate them as a special feature addressing the semantic currency \"Appropriation of Data\".\n\nCPointStudent also offers a new learning support feature with the 'Print' button: The creation of semantic flashcards that can be printed out for drilling practice. Even though this feature adheres to the very conservative behaviorism learning theory, it seems still useful for current learning practices of students. The module differentiates at the moment the theory and the definition level for semantic flashcard generation. In Figure 20 we see an example for the latter. In particular, when printing a definition flashcard is ordered by a student, then the resp. underlying subroutines search the document for all definition objects and create a slide for each with a category line and a subsequent line filled with the annotated title in a new PPT document. These slides represent the front pages of the flashcards to be printed. Every such definition slide is followed by a back page slide, which contains the title, the corresponding theory title as context reference, the object itself, a T-Note if available, and a list of examples for the object in question. Then this new PPT presentation is printed out in a 'two-slides-per-row' style to the active printer -which can be cut into real flashcards by the student. In contrast, a 'theory flashcard' contains three parts: for each theory found in the current presentation a front page slide (analogously to the definition flashcard), a middle page slide showcasing the names of imported theories to understand context dependencies, and a back page slide with its semantic visualization in form of a theory graph.\n\nIn the CPointGraphs module the user is enabled to view the annotated structure in a graph format, i.e. the dependency tree of the knowledge elements is visualized. It offers the on demand generation of several distinctive graphs (based on the freeware GraphViz [Gra02]).\n\nIn particular, CPointGraphs differentiates between three levels (Figure 21: the local context (described by 'Theory' objects), the document context, and the global context (as a set of PPT presentations grouped via CPoint Manager into a \"collection\"). We speak of \"theory graphs\", \"document graphs\", and \"collection graphs\" accordingly. Within each level the granularity can be refined, e.g. on the collection level the graph can be clustered with respect to the PPT files or not. More options consist for instance on the theory level, a theory graph can be generated with or without its attributed objects, or with dismantled imported theories. The various graph formats are illustrated in the CPoint-Graphs documentation on the CPoint websitefoot_7 .\n\nIn Figure 22 we see an impression of (a part of) the resulting collection graph for PPT documents used in-class in a term. Every node represents one theory, the edges the 'imports' relation between them. Each presentation within a collection is temporarily assigned a unique color, so that the location of a theory node within the overall content can be directly grasped. On demand the graph can be magnified or shrunk with the command buttons on the left-hand side of the form. With this example we can get an idea how extensive the knowledge in a course really is. Moreover, reflecting about the graph also means deliberating on the course content. Note for example, that nodes without dependencies might be considered superfluous. A casestudy with CPoint was described in [P14:KK04] and showcased an interesting effect on the PPT author's mental model of her course materials as it \"was changed by the semantic annotation process, resulting in more structured slides, more emphasis on prerequisites, and less presentational gimmicks\"[ibid.].\n\nWe have elaborated on the work and energy that has to be put up with in order to semantically annotate a document. CPoint's navigation form \"GoTo\" can be considered a reward enabled by semantics, i.e. this represents a semantic Added-Value service as described when addressing the semantic currency 'Knowledge Sharing and Reuse'. In particular, the annotated information can be exploited by rigid restriction of potential search spaces. On the Fig. 23: CPoint's GoTo Form for Semantic Navigation left-hand side of the GoTo Form in Figure 23, we find search filters that help to pin down the object a user is looking for. These are grouped into three distinct areas that prune the search space. In particular, the first is a regular string search: all objects are listed that contain the wanted string in the chosen data. The second filter is essentially a semantic search with respect to categories, i.e. the search space is further restricted by admitting only objects with the selected category. The last option allows the user to determine where, that is, in which presentation or collection the GoTo search should look for suitable PPT objects. In the given example, we imagine a user who wants to know what she already completed with respect to the \"Equivalence\" topic. If the titles of objects were used as tags, then this form offers an overview of tags comparable to a tag cloud (without its contemporary information though). On the upper right-hand side, we see the left-over search space, here e.g. a list of three objects shown with the drop-down feature activated.\n\nBut the GoTo feature does not only support finding, it also includes jumping to a selected object in this list when clicking the 'GoTo Object' command button. This feature is especially nice for aggregating a PPT presentation based on already existing PPT sources in a collection. In particular, a user can visit an object in a not-yet opened presentation and decide on its use in the new context. For instance, if a Computer Science department collected each year's PPT presentations for a specific course, grouped them into a collection, then all the PPT objects were at the fingertip of the current teacher of the resp. course. A kind of reverse feature to the GoTo action called \"GoBack\" is offered as drop-down menu in the CPoint menu bar: a list of links to recently visited objects.\n\nIn case the current object was not the desired one, clicking the 'N'(ext) button (that appears depending on the choice of radio button in the 'Find by' area) will initiate a direct jump to the next object on the list of objects. To free this memory and start a new process of search space filtering, the adjoining 'X' button can be used.\n\nWe have seen, that CPoint extensively allows to relate PPT objects. The question arises, how to efficiently find the object one is looking for in this relating and structuring process.\n\nFor this, I developed an in-form navigation and selection facility, the \"Navigator Button\" (seen in Figure on the left). The navigation form GoTo (section 4.10) already allows filtered searches for desired PPT objects, so what is new? First, the mouse doesn't have to be moved from the original form as it is in-form, but this could have been achieved by a simple command button to link with the GoTo Form as realized in CPointStudent (section 4.8). More importantly, when a user already uses a specific CPoint form, then her use situation tends to be much more focused than when using GoTo which favors browsing and aggregation.\n\nIn particular, one PPT object is already selected and the user looks for a specific other one, this one can be related to. As she is in the process of annotating, she can close in on the potential context of the desired object on a much more fine-granular level. Therefore, the search options with the Navigator Button are more intricate, but empower search.\n\nIn particular, the Navigator Button belongs to CPoint's class of \"Selection Boxes\", which appear frequently on CPoint interfaces. A Selection Box operationalizes a list of objects from which the user may select one entry which is visible afterwards. In many forms the choice of objects is already limited in the selection box by the context, as e.g. theories can only import other theories and not arbitrary categorized objects. Clicking on the downwards arrow on its right will show the list of available elements. For instance, in Figure 4.8 the 'For' line contains such a Selection Box showing the title \"Our Trees Are Common\" of the selected object. Moreover, the value of the Navigator Button (showcased as capital on the button like 'L' in the example) determines the scope of the elements in the adjoining selection box. Possible Values are: 'H'ome theory, 'I'mported theories, 'L'ocal presentation, 'A'bstract object, 'C'ollections, and 'M'Base (still to come). The Navigator Button allows three actions:\n\n• Clicking: Moves the scope one up and starts at the beginning if being at the end.\n\n• Double Clicking: Moves the scope one down and goes to the end if being at the start. where the Scope Order is defined to be 'H' < 'I' < 'L' < 'A' < 'C' < 'M'.\n\nIn order to restrict the available choice even more, the Navigator Button (with value 'H', 'I', or 'L') allows to call the Search Restriction Interface for Categories (Figure 24) with which the resulting list is restricted to those with the selected categories. In this example the categories 'Assertion' and 'Axiom' are selected, a search will deliver all assertion and axiom objects of the current presentation in the respective combo box in the calling form. When the navigator button has the scope 'C', then a right mouse button click opens the Search Restriction Interface for Collections in Figure 25, see the CPoint documentation [P25:Koh05b] for detailed information. Here, just an example is provided.\n\nImagine a user who is at the beginning just aware that she would like to reference an object in a certain collection. So she restricts the search space by determining the search level to be 'Individual Collections' and selects this collection. As a consequence all presentations grouped in the collection are chosen to be in the 'List of ALL Selected Presentations'. Now, she remembers the exact presentation. Therefore, she chooses 'PPT presentations' as search level. This leads to a reset of the 'List of ALL Selected Presentations', but it opens the 'List of Presentations to choose from'. This list of presentations is restricted by the previous choice of collections, i.e. it contains just those presentations which are assigned to the selected collections. Loading the CPointNotes module adds another menu item on the CPoint menu bar that contains a list of services with regard to editor notes, also called \"ednotes\". Such ed-notes are comments for objects in the editing process and do not contain information for semantic annotation. It features groups for ed-notes like \"To Do List\" or \"Copyrights?\" that can be created and searched independently for a document (see the command buttons 'Prev in Group' and 'Next in Group' in Figure 26).\n\nIn this thesis I have shown that the interdependence of data and interaction quality suggests a fruitful collaboration of the fields of Knowledge Management and E-Learning using semantic technology. I used the conceptual Interaction Design approach with its base in the science for user-centered design to map out the concept of a \"Semantic Interaction Design\" that stresses a user's value judgments (the 'semantic turn') and does this with respect to the use of semantic data. For this I analyzed potential and idiosyncrasies of semantic data from the micro-perspective of users -focusing on the adoption of semantic data in educational scenarios.\n\nIn the introduction I posed a set of guiding research questions Q*, which I am ready to answer now: Q* If mere data are enhanced by 'semantics', what are the real benefits and sacrifices of their use? In particular, how can the structured quality of semantic data be exploited in educational scenarios and by whom?\n\nInterestingly, the very last question turned out to be the one which was the most influential for my work:\n\nBY WHOM can the specific qualities of semantic data be exploited? The answer is influenced by the observation that use and usefulness of semantics can be direct or indirect (p. 6). In the first case, the user exploits the enhancement of data by semantics, i.e. her space of possibilities is extended, in the latter initially the machine's is. But if the machine is thus empowered, a new set of questions arise naturally, e.g. the semantic currencies. When they are answered in turn, I argue that the specific qualities of semantic data enhance their machine-supported use by people. Moreover, I discussed the exploitation of semantic data in educational scenarios by Knowledge Management as 'captured content' and as a counterpoint by E-Learning as 'learning objects' which yielded different, but complementary perspectives on the same material.\n\nWHAT are the real benefits and sacrifices of using semantic data? These distinct exposures allowed to differentiate first-sight evaluations from real evaluations. For instance, at first glance from a Knowledge Management standpoint, the benefits of semantics in data consist in the achievement of machine-understandable data that allow elaborate servicesrecall Tim Berners-Lee's vision of the Semantic Web. Therefore, they enable much better fine-tuning of applications e.g. adaptability based on the separation of content and form. We can say that the benefits generally concern the potential of semantic data for enhanced human-computer interaction. But data-and interaction quality strongly interdepend. In particular, the real benefits and sacrifices of semantic data show up when looking at the actual use situation, i.e. the interaction process. Evaluated from here, above benefits trigger immediate sacrifices like 'responsibility hand-over' and 'modeling a user' which vigorously degrade benefits if not counteracted. As a consequence, I argued that Computer Science fields interested in semantic data have to better understand these (data-dependent) interaction processes, i.e. they need to identify users' micro-perspectives. Hence I took the microperspective standpoint with a focus on data-driven Knowledge Management and interactionbased E-Learning as salient poles for use of semantic data within educational scenarioswith interesting results. For instance, the number of sacrifices of using semantic data in the Knowledge Management-typical point of view is perceived as just a single one: a user's investment of time and energy into the creation of semantic data. But this perception is only fed by the macro-perspective -from the micro standpoint I showcased many problematic issues (framed as 'semantic currencies' in section 2) that turn up when 'semantics' enhance data and interaction: \"Semantic data are not just data!\".\n\nHOW can semantic data be exploited? This question brought about my suggestions for how to exploit semantic data, especially their machine-supported use by people. I argue that a specific design is needed because of the idiosyncrasies of semantic data, particularly a conceptual Interaction Design approach with a focus on a user's value landscape and hence her micro-perspective. Concretely, I propose addressing the semantic currencies with a Semantic Interaction Design methodology that has interaction at its heart, but additionally cares for the specific issues coming along with 'semantics' in Knowledge Management and E-Learning (section 3). In retrospect, the results of my work can be cast as an answer to the more specific question:\n\nEspecially: WHAT does the exploitation of semantic data look like in educational scenarios?\n\nIn particular, I distinguished the abstract level of determining the problem setting (pivotal for solution approaches) from the concrete level for drafting digital educational scenarios. For instance, I analyzed the common approach towards 'knowledge transfer' software based on semantic data (Figure 27 or section 2.1). On the abstract level the problems consist in the design of data-resp. interaction models that support decomposition of knowledge into content and its delivery. On the concrete level however, they relate to formalization and appropriation processes, particularly the task of capturing content and an end-users composing of knowledge. Thus my research contribution for the use of semantic data can best be described in terms of the four vertices marked as , , , and in Fig. 1 (reprinted on the right) and Fig. 27 above: one dimension is spanned by data and interaction quality, the other by their resp. abstract and concrete level. Emerging interdependencies were investigated and yielded extension opportunities for Knowledge Management as well as E-Learning technologiesfoot_8 : Abstract Data Quality: The Decomposition of Knowledge In order to be able to add 'semantics' to data, we conceptually dealt with knowledge from a Knowledge Management perspective. In particular, we had a closer look at the content/form distinction for mathematical knowledge and what it 'means' for the potential of semantic technology. Moreover, we suggested sensible data structure extensions concerning Communities of Practice for the decomposition of knowledge into 'content'. Another contribution consists in the analysis of E-Learning conditions for a semantic data format in Knowledge Management. [P14:KK04], [P15:KK05], [P16:KK06], [P03:KK08]\n\nTo go beyond the results reported here note that the request for a \"social life of information\" [BD00] and in particular the consequences for semantic data formats still seem to be without an adequate answer from a Knowledge Management standpoint despite Web 2.0 technologies and attitudes. The emergent phenomena with respect to social computing have to be elaborated on an abstract level .\n\nConcrete Data Quality: Capturing Content Once the data model is set, the question arises how to capture content. We developed the analogy between a content author's situation with the Prisoner's Dilemma, suggested author-tailored and semantic Added-Value services to dissolve the dilemma and introduced the notion of Invasive Technology. Additionally, we looked at the recently so successful Social Tagging systems considering tags as lightweight semantic data. From an E-Learning perspective we concluded that such semantic data represent reflective material or embodied conceptualizations. To capture semantic data in more work flows, we need more invasive technologies, e.g. for word processors, spreadsheets, desktop publishing systems, or Computer Aided Design packages. I already started to build another invasive, semantic editor in MS Word (\"CWord\") which builds on the code developed for CPoint and which ultimately will complement it as another source for content relevant for a specific educational context like a lecture. Naturally, other resources like the components in Open Office also have to be looked into as potential candidates for semantic enhancements. towards a user's process of composing knowledge (in contrast to an externally controlled composition of knowledge). To close the circle let us consider again the quote I started out with. The overall conclusion of this thesis consists in the cognition that software applications concerned with semantic data have to be designed with the interaction process in mind: \"There is no simple causal chain\" [Tur97,p. 46]. In particular, Knowledge Management and E-Learning designers should closely listen to each others requirements so that the final 'conversation' with the end-user is sensible. This means that the respective designers need to be informed by macro-and micro-perspectives to obtain a better understanding in the evolutionary process of building the value landscape with respect to the use of semantic data, i.e. raising an awareness about the framing of its resp. problem setting: \"We construct our technologies, and our technologies construct us and our times. Our times make us, we make our machines, our machines make our times.\" [ibid.].\n\nMoreover, an end-user should be enabled to follow the 'user as designer' notion. That means, the design needs to inform the user of the designer's underlying macro-and microperspectives so that she is enabled to perceive all of the software's (explicit and implicit) affordances. Then in turn a user's imagination might be stimulated and appropriation of software transforms from being a torturing task (like the reconstruction of a set of broken pieces) into an engaging process (like dynamic, on-the-spot composing of an entity for her needs, see Figure 29): \"We become the objects we look upon but they become what we make of them\" [ibid.]. 6 Acknowledgments \"Jedes Subjekt setzt sich konkret durch Entwürfe hindurch als eine Transzendenz; es erfüllt seine Freiheit nur in einem unaufhörlichen Übersteigen zu anderen Freiheiten, es gibt keine andere Rechtfertigung der gegenwärtigen Existenz als ihre Ausweitung in eine unendlich, geöffnete Zukunft. Jedesmal, wenn die Transzendenz in Immanenz verfällt, findet ein Absturz der Existenz in ein Ansichsein statt, der Freiheit in Faktizität; dieser Absturz ist ein moralisches Vergehen, wenn er vom Subjekt bejaht wird; [. . . ] \" Simone de Beauvoir in \"Das andere Geschlecht\" [dB49, p. 21]\n\nAll my life, I have been inspired by role models: they provided thought trajectories and challenges. Therefore, I like to start my list of acknowledgments with people, who either do not know me or are not aware of their relevance for my life. I took the readings of Simone de Beauvoir to heart that demonstrated what \"taking liberties\" affords. Mathematician Roswitha Helga Hotz was honored during an emeritus colloquium held for her husband as a woman who had the courage to start her dissertation in an unfamiliar field (Physics) after her youngest child had come of age. This directly appealed to me and is realized now in my dissertation. My supervisor Heidi Schelhowe is a living demonstration of the energy in pursuit of a goal one is capable of.\n\nMichael Lund, Isabel Zorn, and Iris Bockermann as well as other members of the working group \"dimeb (Digitale Medien in der Bildung)\" at University Bremen convinced me that transdisciplinary borders, particularly languages as practices of diverse communities, may be overcome -if all try hard enough. Thank you for enhancing my micro-perspective! Academically, I have to thank too many people to list individually, especially in the working groups \"dimeb\" and \"KWARC (Knowledge Adaptation and Reasoning for Content)\" at Jacobs University Bremen. But I do want to thank my co-authors individually. First to name is my main co-author Michael Kohlhase and my favorite partner in discussions: you opened new worlds of understanding (and revealed closed others). I also tremendously appreciated working together with Normen Müller, Michael Lund, Milena Reichel, and Heidi Schelhowe, each of which has a specific approach that s/he shared with me: Thank you! To my family, I have to apologize for many absences and neglections particularly with respect to Michael, Jana, Lukas, and Felix. You endured them heroically and this work was only possible because of your cooperation: It meant so much to me and I am very grateful for it. I do hope that you realize what personal growth you enabled!\n\nThroughout this work I will use to a greater extent mathematical examples and concepts as I thrive on my education as a mathematician. Please note though that their contributions are not restricted to mathematics. In particular, I consider mathematical knowledge only as an easier-to-deal-with prototype for general knowledge, since it is available in a pre-organized form.\n\nIn [P14:KK04] we called these author-tailored services 'Added-Value services', in hindsight [P02:KM08] the term 'value services' would have been more fitting (see argument on page 14).\n\nNote that the appreciation of a service as incentive or gratification depends on the individual user.\n\nhttp://kwarc.info/projects/CPoint/CPoint.html\n\nNote that PPT's own XML conversion feature translates into non-standard XML as it is geared towards PPT document presentation on the web and not sharing. I have not studied the new OOXML file format of Office 2007. Even though it is standardized by ECMA, it seems proprietary, non-semantic, and thus of limited use for sharing and interoperability.\n\nCPOINT AS OBJECT-TO-THINK-WITH\n\nThe label sizes in the image do not correspond to their original sizes, they are magnified for the readers convenience.\n\nhttp://kwarc.info/projects/CPoint/CPoint.html\n\nNote that I will use 'we' in the following list as parts of the results were achieved collaboratively."
}