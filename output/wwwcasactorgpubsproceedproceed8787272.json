{
    "title": "CREDIBILITY FOR CLASSIFICATION RATEMAKING VIA THE HIERARCHICAL NORMAL LINEAR MODEL",
    "publication_date": "1945",
    "authors": [
        {
            "full_name": "Stuart Klugman",
            "firstname": "Stuart",
            "lastname": "Klugman",
            "affiliations": []
        }
    ],
    "abstract": "In the past twenty years there has been ever increasing improvement in the techniques of classcfication ratemaking. Most of this has centered around improvements in credibility procedures and most of the improvements have been due to incorporating aspects of Bayesian analysis. In this paper, I attempt to take this trend to its (perhaps) final stage by developing a true Bayesian approach to the classification ratemaking credibility problem.\nThe opening section will provide the rationale for the Bayesian approach. I will argue that a hierarchical model with a noninformative prior is the most appropriate general framework. I will argue further that a normal model is a reasonable choice, and this model will provide results at least as good as those currently available. An indication of how the normality condition can be relaxed will also be presented.\nThe second section contains a general description and analysis of the hierarchical normal linear model (HNLM). Included are point estimation, estimation of the error in the estimator, and prediction intervals for future losses. The last two items are of special interest since current credibility procedures provide little insight with respect to variation.\nThe next two sections discuss the special case of the one-way model. This is the most common ratemaking model and is the simplest case of the HNLM. In Section 3, the formulas from Section 2 are evaluated for this model. In Section 4, two data sets are analyzed. TheJirst set provides an indication of the computational work required to use the HNLM. The second set provides a comparison of this method with two other ratemaking approaches.\nThe$nal section contains a discussion of the more complex models that can be handled with the HNLM.",
    "full_text": "The majority of this work was supported by a grant from the Society of Actuaries Research Development Fund through the Actuarial Education and Research Fund grants competition. An earlier version of the one-way model was presented at a NATO sponsored conference (Klugman [22]). Many conversations with Glenn Meyers and Gary Venter helped refine the arguments presented here. Two anonymous referees from the CAS provided a number of suggestions that improved the exposition. I thank the National Council on Compensation Insurance and Glenn Meyers for providing the two data sets and especially thank Kathy Hockenberry for writing the programs to do the analyses.\n\nThe historical basis for credibility procedures is long, varied, and generally considered to be one of the major actuarial contributions to statistical data analysis. Virtually from the beginning (Whitney [35] and Bailey [l]), the Bayesian and shrinkage nature of the problem was recognized. In a breakthrough paper, Btihlmann [6] placed the credibility problem in the framework of Baye-Sian decision analysis. I will begin by reviewing the Bayesian view and then discuss the four schools of Bayesian methodology that are prominent today. As part of this paper, I will argue that one of these methods is superior to the others. Next, I will argue that the normal model is appropriate even though we know that it does not accurately model insurance losses. This part closes with a suggestion for allowing for non-normal losses while retaining the advantages of normal theory. The final element of this section is a discussion of the noninformative prior.\n\nThe basic credibility problem for classification ratemaking can be posed as follows: The population can be separated into k groups, the various rating classes, Our objective is to estimate the mean loss per year generated by a randomly selected member of a particular group. Data is collected from a sample of members from each group. It is usually assumed that the observed losses are independent and that the variances of the observations are proportional to some measure of exposure. If this were all that were known, the most reasonable answer would be to use the sample mean from each group as an estimate of the population mean. Usually, however, we know more. In particular, when individual classes have abnormally good or bad experience, we tend to discount the experience when setting rates. This clearly makes good business sense and with the correct model makes good statistical sense.\n\nThe usual way to model this phenomenon is to treat the group means as a random sample from some probability distribution. This implies that experience from the other groups tells us something about the overall level of claims (the mean of this second level distribution), and therefore tells us something about the mean for the group in question. It also sets bounds on how much one can legitimately expect one class to differ from another. If more is known about the relationship among the groups, that knowledge can be incorporated into the second level distibution. Examples of this are presented in Section 5.\n\nThe model described in the previous paragraph is a standard Bayesian problem. We have a model given by the p.d.f. p(xlO,g) where x represents the data and (8,g) represents all unknown parameters. The parameters in 8 are the ones we want to estimate. The parameters in g are nuisance parameters, usually variances. In the above setting, 0 would be the group means. The prior (second level) p.d.f. p(O,g) represents our knowledge of (tl,g) before the data are collected. Since the Bayesian approach has now been widely accepted among actuaries (at least for this estimation problem), I will provide no further arguments to support that view. Interested readers who desire a wide ranging discussion of the merits of the Bayesian view are referred to Berger [2].\n\nGiven this setup, there are two ways to proceed. If the forms of the two distributions are known, the Bayes estimator is the posterior mean of Cl given the data X. Btihlmann [6] took a different approach. To avoid thinking about the distributions, he first restricted himself to estimators that are linear functions of the data. He then searched for the estimator that minimized the mean squared error. This mean would be taken over all possible values of x and 8. For his result it was essential that g be empty. That is, the model variance had to be known. Under this framework, it turned out that the estimator depended only upon the first two moments of the model and prior distributions. To many people, the word credibility is now reserved only for procedures that find linear estimators. In fact, Hewitt [14] compares a credibility estimator to a Bayes estimator (as I have defined it above). In this paper, the objective is to find the best estimator, and I see no reason to restrict attention to those that are linear functions of the data. I use the word credibility to describe any procedure that uses information (\"borrows strength\") from samples from different, but related, populations.\n\nA larger problem is the fact that the moments of the model and prior are rarely known, and therefore must be estimated. This has led to a number of schools of Bayesian thought. Having agreed to use a Bayesian procedure, the remaining task is to identify the best one.\n\nThere are at least four different approaches that are currently being used to solve the estimation problem. In this section I briefly outline them and then offer some opinions as to their respective merits.\n\nThis is the view that has already been mentioned. Here, the prior distribution must be elicited. This is very difficult to do in the insurance setting as one would have to be able to set out a distribution that describes the class-to-class variation in losses. Since we do not even know the means (determining them is the point of the exercise), it is unlikely that we know much about how the means vary. This problem can be resolved by removing the prior to a higher level of abstraction. This is done in the fourth school discussed in this subsection. To my knowledge, no one today is using the two level approach. At best, it is a starting point for the second method to be discussed here.\n\nThis method evolved as an attempt to resolve the problems created by the first method. Although they did not use the phrase \"empirical Bayes,\" Btihlmann and Straub [7] were the first to employ this method in the credibility setting. It remains popular, being advocated in more recent articles by the Insurance Services Office [16] and Meyers 1251. There is considerable evidence that it provides excellent solutions to the estimation problem.\n\nMany people do not consider empirical Bayes methods to be at all Bayesian. Also, there is considerable disagreement as to what the phrase \"empirical Bayes\" means. To avoid controversy, I will describe an estimation method that corresponds to the approach used in the papers cited above. It will be referred to as the EB approach and the reader can decide what that means. Begin with the density p(xlO,g), the first level density (or distribution, when talking about the random variable). The density p(B,glh) will be referred to as the second level density. Note the introduction of h. To the pure Bayesian, the parameters of the second level density must be known, and therefore do not need to be displayed. In reality, that is not true, so we add them to the formulation.\n\nIn brief, the EB idea is to first act as if g and h were known and find the Bayes estimate of 0. Next, use the data in some manner to estimate the nuisance parameters g and h and insert these estimates into the Bayes solution. The first thing to note is that upon doing so, we no longer have a Bayesian analysis. The second level distribution was supposed to represent prior opinion, yet here we are unable to establish this distribution until after we have seen the data. The usual justification is to show that as the sample size goes to infinity, the estimates of the second level distribution converge to what they ought to be if we had complete knowledge (which is what one has with an infinite sample size). A thorough discussion of these principles can be found in Norberg [30].\n\nAn alternative approach can yield the same solution as the EB approach. Assume again that the nuisance parameters are known and then search for the estimator that is linear in the data and minimizes the expected squared error.\n\nOnce again, substitute ad hoc estimates of the nuisance parameters into the solution. This has been called least-squares credibility.\n\nThere are three major objections to the EB approach. The first is that some external theory must be used to find estimators of the nuisance parameters. Since these parameters are usually variances, it is common to begin with sums of squares that look \"right\" and then to adjust them to create unbiased estimators of the various parameters. One drawback is that the resulting estimators (even in the simplest cases) can take on negative values. This does not make sense when one is trying to estimate a variance. The second objection is that EB theory gives no guidance as to the optimal choice of the estimator. All that is required is that they be consistent. The final objection is that for complex models, there may be no hope of finding useful sums of squares.\n\nA final problem with EB methodology is that it gives no insight into the sampling error of the estimator. The best it can do is evaluate the error when the variances are known. The additional error introduced by estimating the variances cannot be accounted for. Even if a good estimator of the nuisance parameters can be found (in which case, the method works quite well), the investigator will have no idea of the quality of the estimate. The previous statement that EB methods work well was in reference to alternative methods and does not mean that the results could be considered accurate. That can only be determined by some measure of sampling error. The next method is an attempt to rectify this problem without leaving the EB framework.\n\nTo see the difficulties in determining the variance of the estimator, we need to take a closer look at what we are trying to do. The general Bayes problem is to find E(0)x), the posterior mean given only the data. The EB approach uses the result E(6)x) = E[E(Bl.~,g,h)l. The interior expectation E(Blx,g,h) is just the pure Bayes solution with g and h known. The EB approach avoids taking the outer expectation and instead replaces g and h with their estimates. EB theory indicates that this is a reasonable thing to do. A measure of the quality of the result would be the posterior variance, Var(Blx). We have Var(0ln) = E[Var(B\\x,g,h)] -t-Var[E(BIx,g,h>]. It is apparent that merely inserting estimates of g and h in Var(Blx,g,h) will underestimate the desired variance. The second term reflects the additional variance due to the estimation of g and h. EB theory does not provide any ideas for estimating the second term.\n\nAn attempt to resolve this problem is the parametric empirical Bayes theory of Morris [27]. The key ingredient is to have some idea of the variability of the estimators of g and h. His theory requires not only the discovery of good estimators of g and h but also the ability to determine their sampling distributions. In simple cases (normal distribution, equal exposures), it is possible to show that the usual estimators have chi-square distributions. In slightly more complicated cases, the distribution is approximately chi-square. A detailed discussion of the distribution of some commonly used variance estimators is given in Klugman [21]. As should be apparent, there are considerable difficulties associated with putting this method into practice. One that is not apparent, and is often not mentioned in Morris's articles, is that to complete the calculation it is necessary to formulate a prior distribution for g and h. Morris uses p(g,h) = 1, but does not provide a justification for that choice. Other choices are supported by an argument that the resulting estimator of the credibility factor is unbiased, a surprising justification for a Bayesian.\n\nThe fourth model also requires prior distributions for g and h but proceeds in a more direct Bayesian manner. Before moving on, I should add one final criticism of the parametric empirical Bayes approach. Whatever errors in estimation are introduced cannot be reduced by improving the computational aspects of the method. The errors are due to lack of knowledge of the exact distribution of the estimators of the variances and no amount of computation can resolve that issue.\n\nWe have seen that the two EB schools are somewhat artificial attempts to resolve the problems of the pure Bayes method. This is mostly due to a lack of recognition of the real problem with the pure Bayes approach. The problem is that the second level distribution is not a prior distribution at all, but is part of the model. In the ratemaking setting, this distribution contains our knowledge of the relationships among the various rating classes. It is not our prior opinion about a particular class. The solution is to reformulate the model into three levels.\n\nLevel 1: p(.@,g)-Describes variations within each group. Level 2: p(8lp,h)-Describes variations among the groups. Level 3: p@,g,h)-A true prior distribution on the unknown parameters.\n\nThis is once again a pure Bayesian problem. As with any Bayesian analysis, a prior distribution must be established before any data is collected. By displacing the prior to a level further removed from the observations, the choice of the prior will have less influence on the final outcome. To repeat, level 2 describes an underlying (though not directly observable) physical process. Subjective beliefs enter only at the third level. The remaining problems are to select the prior distribution and to select the form of the p.d.f.'s for levels 1 and 2.\n\nAssuming the two problems just mentioned can be resolved, this would appear to be an ideal solution to the credibility problem. With the three densities in hand, it is just a matter of employing the probability calculus to obtain the posterior distribution of 0 given X. Any difficulties that will be encountered will be of a computational nature. Once the posterior distribution has been obtained, additional computation will yield the mean and variance. Another useful quantity is the predictive distribution, the p.d.f. (or the mean and variance) of the next observation from the group in question. This is once again obtained by an application of the probability calculus. The other methods do not provide this item.\n\nAn additional advantage of this approach is that the tools of Bayesian modeling and inference are all available. For example, one might want to compare various models for the level 2 distribution (e.g., cross-classification vs. one-way classification). Many of these tools are presently in the development stage, but more and more techniques are likely to become available in the future.\n\nAs mentioned in the previous subsection, it is necessary to specify the probability distributions for the three levels. For levels 1 and 2, multivariate normal models are an appropriate choice. At the end of the section, a suggestion for improving the process is proposed.\n\nIt is obvious that individual losses do not follow the normal distribution. Since losses are non-negative quantities, a distribution with support on the entire real line cannot be expected to be a good model. Furthermore, there is consid-erable evidence that the tails of loss distributions are much heavier than those of the normal distribution. See Hogg and Klugman [15] for a number of examples. One way to minimize the disparity is to work with loss ratios. The distribution at the second level will now reflect group to group variations in the departure from the expected losses. This will be more stable than the group to group variation in the absolute level of losses. In addition, loss ratios are likely to have identical unconditional distributions. That is, if you were given a list of risk classes and a list of loss ratios, you would be unable to do better than chance in attempting to match them up. The loss ratios are likely to be dependent. Knowing that the loss ratio for one class is high increases the chances that the others are also. The multivariate normal model is one of the few multivariate models that allows for dependence in a manner that is easy to construct and interpret.\n\nDespite the fact that the observations are not normally distributed, there are a number of good reasons for employing the normal model. The first, though least appealing, justification is computational convenience. Although the algebra is tedious, as demonstrated in Section 2, a number of results can be obtained analytically. The remaining numerical work will be simple, at least relative to that required for non-normal models. It is likely that as our numerical capabilities increase, this argument for normality will lose its validity. For the present, the following quote due to Novick and Jackson [31] is appropriate.\n\n\"Surely it is better to get some results using a model which is only approximately relevant than to sit twiddling one's thumbs in front of a model which is felt to be more accurate but which one is unable to manipulate.\"\n\nThe second justification for normality is related to the link between the normal model and linear credibility. It was mentioned above that in a particular simple model, the linear least squares solution depended only upon the first two moments. It turns out that the Bayes solution for the same model with normal distributions is identical. Therefore, at least in this case, normality and linear least squares are equivalent. It has been shown that, in general, any model that is a member of the linear exponential family of distributions will produce the same result as the linear least squares solution (Ericson 191 and Jewel1 [17]). There has been speculation (Goel [ 111) that the linear exponential family contains all the distributions with this property. So, to a certain extent, those who are willing to accept linear solutions should be equally comfortable with models from the linear exponential family. As far as choosing the normal distribution as the member to use, a second argument is needed. Most current practitioners estimate the variances using sums of squares. These estimates are unbiased for use. In Section 4, an indication of how one might \"verify\" the choice of model and prior is presented.\n\nThree approaches can be taken to specifyingp(p,g,h).\n\nThe first is to always use p(p+g,h) = 1, Morris [27] uses this prior in obtaining his parametric empirical Bayes results. The first thing to note is that since the support of (p,g,h) is usually unbounded in at least one direction, this prior is not a proper probability distribution. Box and Tiao [3] argue that this is acceptable. Suppose l..~ is the average loss ratio over all rating classes. We can be virtually certain that this value is between 0 and 10. A uniform distribution over this interval combined with one that tails off slowly outside this interval would reflect the fact that very little is known about the true average loss ratio. Inferences that we would make using this prior would differ very little from those made using p(p) = 1 for --CO < lo < m. Two features of this approach should be noted. First, there is no guarantee that the posterior distribution of 81~ will exist. This would make it impossible to determine the posterior mean or variance. Second, the posterior mode is identical to the maximum likelihood estimator (after integrating out all nuisance parameters). In general, when this prior admits a solution it is quite reasonable.\n\nThe second school of thought is to find a general way of obtaining prior distributions that reflect minimal prior knowledge. Words such as \"noninformative\" or \"reference\" are often attached to such priors. The goal of research in this area is to find a way to automatically generate the noninformative prior for a given distribution. The fact that there is still disagreement on the appropriate reference prior for the probability of success in a sequence of Bernoulli trials (Geisser [lo]) indicates how much work remains in this area. In the simple univariate case, Box and Tiao [3] support the prior p(g) = l/g when g is the variance. An extension is given by Tiao and Zellner [33] who argue that if g is a covariance matrix, the appropriate prior density is the inverse of its determinant.\n\nThe third belief is that only proper densities (those that integrate to 1) should be allowed for the prior distribution. Proponents of this approach insist that everyone has a prior distribution and it is just a matter of care and effort to bring it out. This makes excellent theoretical sense but is very difficult to implement. It is even more difficult to convince someone else that your opinion, as expressed by your prior distribution, is valid.\n\nI have elected to take the middle ground. For credibility problems, the reciprocal prior for variances seems to be an appropriate choice for the prior density. This prior appears to be more \"balanced\" than the uniform one. Since the support is the interval from zero to infinity, we should expect that our prior opinion is equally apportioned between points near zero and those near infinity. The prior l/g does this as it bounds an infinite area over all regions of the form (0,~) and (a,~). The uniform prior puts infinite probability only on the latter region. That is, it seems biased towards larger values of the variance. In Section 3, some brief attention will be given to a proper prior, so those who have one can still employ the methods to be discussed.\n\nAll of the ideas presented in this section other than the use of normality are summarized in the following quote (Berger [2]):\n\n\"We would indeed argue that noninformative prior Bayesian analysis is the single most powerful method of statistical analysis, in the sense of being the ad hoc method most likely to yield a sensible answer for a given investment of effort.\" (author's italics)\n\nIn this section, the algebraic manipulations required to evaluate the three level hierarchical model are performed. Attention will be restricted to linear versions of the model. This is done mostly for computational convenience.\n\nBefore beginning the manipulations, a few notational items will be presented. Scalars will be represented by lower case letters. Vectors will be represented by bold face characters. Matrices will be represented by upper case letters. In classical statistics it is common to use upper case symbols to represent random variables. In a Bayesian analysis the various quantities are sometimes random and are sometimes fixed, so no attempt is made to use notation to identify random quantities. For example, in the model, the data are random and the parameters are fixed, but in the posterior, the parameters are random and the data are fixed. At times, the distribution of some parameters conditioned on others is needed. When examining a density function, the way to tell the fixed quantities from the random ones is to look at the left hand side. For example, p(B/y,G,H) indicates that in the function which follows, 8 is the random quantity and is the variable in the density, while y, G, and H are fixed quantities. The density is for the indicated random variable, conditioned on the specific values given. When the two sides are separated by a proportionality symbol (m), the constant of proportionality may depend upon the conditional items. The constant can always be found by integrating the function with respect to the random elements.\n\nThe linear version of the three level Bayesian model is usually attributed to Lindley and Smith [24]. The three levels are:\n\nyl%G -NAW) @' x 11, %0-f -WW,,,H) (k x 11, and (r. -N(W) (z x l),\n\nwhere p and C are known and A and B (also known) are of full rank. A special case, and the only one considered here, is obtained by letting C-i + 0. This is equivalent to setting p(p) 0~ 1, the widely accepted noninformative prior for the mean. It is not necessary in this case to make any statement about p. In most applications the covariance matrices G and H will not be known. It is then necessary to specify a prior distribution for them. Let p(G,H) be the density for this prior distribution.\n\nThe standard credibility problem is to make inferences about 8, the expected losses (or loss ratios) for the various groups under consideration. The matrix A reflects the nature of the data collected. For example, there may be data from various years for each group. The second level indicates any relationships between the groups. One particular version of this model is analyzed in Sections 3 and 4; examples of other models are presented in Section 5. In any event, the objective of all the manipulations in this section is to obtain the posterior distribution and moments of 9 given the data y. Of less interest are the posterior distributions of G and H.\n\nThe first useful relationship is a matrix equation that is true for any symmetric non-singular matrix G; it will be used for completing the square.\n\nThe second item relates to the multivariate normal density. In general, the multivariate normal p.d.f. for a random variable with mean p and covariance C (a positive definite matrix) is\n\nwhere ICI denotes the determinant of the matrix C. This implies that in general\n\nThe final item is concerned with finding conditional densities. The general problem is the following: Letf(a,, . . a&, . . ., b,) be proportional to the conditional density of AL, . . _, A, given BI = br, . . ., B, = b,, (n may be 0). To find the conditional density of Al, . . ., A, given B1 = bl, . . ., BJ, = bh (where g 5 m and h I n and at least one of the inequalities is strict) evaluate J _ _ . Jf(u,, . . am/b,, . . ., b,)du,+l . . . da, and then drop all terms that involve only bl, . . ,, 6,. (This latter step can be done first; additional terms can be eliminated after the integration.) The resulting function will be proportional to the desired density.\n\nA related fact is that a conditional density is proportional to the conditional density in which some of the quantities on the left hand side of the \"I\" are moved to the right hand side. For example, f(ul, u2Jbl, b2) is proportional to the conditional density of A, given AZ = ~2, BI = bl, B2 = b2. Any factors that depend only on uz can be deleted. While the theory behind the above development is not germane to a Bayesian analysis, it is comforting to note that a Bayesian analysis often produces results that match those from classical theory. The quantities fi and p will appear often in the analysis that follows, but will arise from a different algebraic procedure.\n\nThe Joint Density of (y,0, p,G,H)\n\nThe joint density involving all of the quantities from the three levels is the ideal place to begin. The last concept presented in Section 2.2 indicates that it is also the conditional density of any subset of the five variables given the remaining ones. The density is given by pCvle,G)p(O(~,H)p(l~l)p(G,H) which is proportional to (recalling that p(p) 0: 1)\n\nIt would be pleasant to proceed directly to the density of 0 given y. However, it is not possible to obtain this density analytically. Instead, begin by obtaining those conditional densities that are reasonably easy to derive. This is done in the next section. In the following section these densities are used to obtain the desired result.\n\nThe following subsections contain the derivations of a number of important conditional densities. The results are summarized in Section 2.5.6. Readers who are uninterested in the derivations can skip to that point. Since the second term does not depend on 8 it can be dropped. The final result is\n\nBy inspection it is immediately apparent that t$,G,H -N(VA'G-'y,V).\n\nLet fi = VA'G-ly be the conditional mean. It can be rewritten as\n\nThis is the customary weighted average common in a Bayesian analysis. It is also a (linear) credibility formula. In fact, this is the result that arises from an EB analysis. As discussed in Section 1, the point of departure is the treatment of the unknown G and H.\n\nThis calculation is included mostly for completeness. It is not used in any subsequent work. The procedure is exactly the same as that used above, only now integrate out 8 instead of I.L. The result is ply,G,H -N(@,[B'(A + H)-'B]-').\n\nTo find this density, integrate or. out of the joint density. The difference between this calculation and the one in Section 2. The second and third terms in the exponent cancel. From Section 2.3,\n\nComplete the square to obtain\n\nIntegrate p out of the density in Section 2.5.4 to obtain\n\nIn the second term of the exponent write 6 in terms of y. The two terms in the exponent are the within and between sums of squares, respectively. Both depend on the unknown variances, G and H. While it is once again comforting to note that frequentist quantities have appeared in the Bayesian development, we should keep in mind that these quantities have no special meaning. The determinants can be rewritten as\n\nThe two numerator terms can form the basis for a prior distribution on (G,H). This is somewhat consistent with the ideas presented in Box and Tiao [3] and in Tiao and Zellner [33].\n\nThe important matrices and distributions from this section are repeated for convenience:\n\nAs introduced in Section 1.2.2, the EB approach begins by finding the posterior mean of 8 given the covariance terms G and H. In the HNLM this is 6. External estimates are then found for G and H. In this section, two general approaches to finding such estimates are introduced.\n\nThe first method uses the posterior density p(G,H!y). Either the mean or the mode could be used as the estimate. The mean is superior in that it is guaranteed to be in the interior of the parameter space. The mode is often easier to compute, but may be on a boundary. Either estimate usually requires a numerical evaluation, Specific formulas for a simple model are presented in Section 3.8.\n\nThe second method is an iterative technique. Begin with a preliminary estimate of 8, say 6. Then in p(B,G,Hb) hold 8 fixed at its current value and find the values of G and H that maximize this density. Obtain a revised estimate of 0 by evaluating 6 at the values of G and H just obtained. Repeat this procedure until 6,G, and H stabilize. It is not entirely clear what the results mean, but the procedure is similar to that recommended by Morris [27]. Computationally, this tends to be the simplest approach, as the maximization can often be done analytically. This is demonstrated for a simple model in Section 3.5. If an analytical approach is not possible, an all-purpose maximization method like that of Nelder and Mead [29] is likely to provide the answer.\n\nRecall that one of the drawbacks of the usual EB method is its inability to produce variance estimates. In Section 3.8, it is shown how this can be done when G and H are estimated by the posterior mean. The first density in the integrand is a multivariate normal density and was obtained in Section 2.5.1. The second density was obtained in Section 2.5.5 up to a constant of proportionality. We must obtain that constant in order to insert the exact density in the integral above. That can be found by integrating the expression found in Section 2.5.5 with respect to both G and H. These two integrals are of equal difficulty and usually must be done numerically. The degree of difficulty will depend on the form of the covariance matrices G and H and the prior density p(G,H). It will be seen in Section 3 that in a specific case the problem can be analytically reduced to a one-dimensional numerical integration. Some excellent procedures for performing multidimensional numerical integration are given in Smith, Skene, Shaw, Naylor, and Dransfield [321.\n\nIn most applications, the vector 0 will be of a reasonably high dimension, certainly greater than two. It is unlikely that much insight will be gained by examining the posterior density. The remainder of this section is devoted to obtaining various summary quantities. This will conclude the development of the general hierarchical normal linear model.\n\nOne way to obtain this quantity would be to evaluate the following integral: Jfhp(~/y)dfJ.\n\nGiven the fact that a numerical step is necessary to yield each evaluation of the integrand, the cost of performing this integration is likely to be quite high, Instead, employ the following result (this notion was introduced in Section 1.2.  where Vii is the ith diagonal element of the matrix V introduced in Section 2.5.1.\n\nThis univariate density could be plotted to provide insight about a particular group mean. An approximate integration needs to be performed to get each point from the posterior density. The formula is P(eib> = SP(eiJG,H,y>p(G,Hlv)dcdH.\n\nThe first density is a univariate normal density with mean 6, and variance Vii.\n\nIf this calculation appears to be too time-consuming, the posterior distribution may be approximated by a normal distribution with moments as given in Sections 2.7.1 and 2.7.2. This result is given in Berger [2] and is a Bayesian version of the central limit theorem. The same result applies in the following section.\n\nIn the insurance setting it may be more useful to get information about the losses in a future period than to estimate the class mean. Such a calculation would incorporate both the uncertainty with respect to the group mean and the uncertainty about the experience of next year's insureds.\n\nIn general, consider a new observation, x -N(AxO,Cx) where C, will depend in some way on the elements of G. A typical example would have A, be a 1 X k vector of zeros with a one in the i\" column. This would make x (a scalar) an observation from the i\" group. The matrix C, would be a scalar of the form o*/P where o2 is the variance from the original model and P is a measure of exposure for the year to come. The expectation can be found using the formula in Section 2.7.1 since each element of the vector of expectations can be found individually. The covariance requires evaluation of CoV(Bi,Bj(y). This can be done from Jv;jp(G,H!y)dGdH. The (ij)'h term of the expected value is evaluated as S(Cx)q~(G,Hly)dGdff.\n\nIn this section a specific hierarchical model is investigated. It is similar to the model treated by Biihlmann and Straub [7] in their EB analysis and is appropriate when there are k identically distributed groups and the goal is the simultaneous estimation of their means. The three levels of the one-way model are Level I-yijlt3i,u2 -N(0i,U2/Pij) i = 1, . ) k j = 1) . . . ) t~i The random variables at each level are conditionally independent and Pij is some measure of exposure. The usual situation is that yij is the average loss (or loss ratio) in year j for class i. This differs from the Btihlmann-Straub model in just one respect. In their model, the level one variances were allowed to differ from class to class. Their result, however, uses only the average of these variances. That is, at no point is this variability taken into account. An indication of how one could truly account for unequal variances is given in Section 5.\n\nTo use the formulas of the previous section it is necessary to identify the various matrices and vectors. Begin by letting y be the N X 1 vector of the observations where N = Xni. Arrange the observations so yl,, . . , yr,,, appears first, followed by ~21, . . . , yzrz2. and so forth. The matrix A is N X k and contains only zeros and ones. In the first column, the ones are in the first nl rows. In the second column, the ones are in rows ni + 1 through ni + n2, and so forth. The vector 8 is k X 1 and contains the unknown group means, 01, . . . ) Ok. The covariance matrix G is diagonal with diagonal elements running from a'/Pii in the upper left corner to o*/P knk in the lower right comer. At the second level, B is a k X 1 vector consisting entirely of ones. Let 1 indicate such a vector. The vector or. is a scalar and so will be written k. The covariance matrix H is diagonal with all elements equal to r2, that is, H = T~Z~.\n\nThe exposition will proceed in four steps. The first is a development of a pair of useful matrix relationships. They will aid in the evaluation of the determinants and inverses. Next, prior distributions for c? and r2 are introduced. The third step is to obtain the conditional densities. The final step is to perform the integrations. Derivations of these results can be found in Graybill [ 121 (Theorem 8.9.3).\n\nIn this section, three noninformative priors and one proper prior will be introduced. Two of the noninformative priors are based on a general theory that can be used in any setting of the HNLM. The third one is particular to the oneway model.\n\nThe easiest one to describe is the naive version of a noninformative prior. It is p(G,H) = 1. In the one-way model the only random elements are o* and 72, so the actual prior in this case is P(u',T~) = 1. This prior is used by Morris [28]. While it is convenient for computational purposes, there are good theoretical reasons (Box and Tiao [3]) for not using it. The essence of the argument is that this prior puts too much weight on large values of the parameters. On the other hand, it is often the case that this improper prior will yield a proper posterior (something that must always be checked when using a noninformative prior). Also, the posterior mode is the maximum likelihood estimate. This should give comfort to those who are troubled by Bayesian methods. Call this prior 1.\n\nThe second prior is based on the arguments of Box and Tiao [3] for the balanced model. The balanced model is the special case where nl = . . = izk and PII = . . . = Pknt. The first requirement is common in insurance studies, as n, often is the number of years of observation for the ith class. However, it is extremely unlikely that the exposures will be equal for all years and all classes. In any event, in the one-way balanced model the reasonable noninformative prior is p(02,r2) CC (~~)-'(a' + PTZT~)-' where P is the common value of the PC and n is the common value of the ni. This prior is taken after Box and Tiao [3] and is related to the Fisher information about G and H. In the above expression, pdim(G) refers to the number of distinct parameters in the matrix G while dim(G) is the number of rows in G.\n\nFor the one-way model, P(u*,T~) CC (u2)-'[lI(u* + P~T~)]-\"~. In the balanced case this prior is identical to prior 2. Call it prior 3.\n\nThe fourth and final prior is an attempt to offer a proper distribution. As such, it requires that the investigator have a genuine opinion about the variances. When seeking a proper prior, mathematical convenience is always a high priority. At the very least, the family of proper priors should include a sufficiently large variety of possibilities so as to give the investigator a chance of finding a representative prior. The natural choice for variances is the inverse gamma distribution. The general form of the density is p(x) K x-\"exp( -h/x).\n\nFor it to be a proper distribution we must have 1, > 1 and h > 0. The limiting case of 1, = 1 and h = 0 is similar to prior 2. Since prior 1 is equivalent to v = 0 and A = 0 we see how far from being a proper distribution prior 1 is. The inverse gamma prior will be referred to as prior 4. A specific version appropriate for the one-way model will be given later.\n\nThis section contains all the details of the evaluation of the formulas in Section 2 in the special case of the one-way model. Prior 4 uses two independent inverse chi-square random variables. Prior 1 is a special case. In the next section the four priors will be written with one general formula.\n\nIt turns out that calculations are much easier to carry out with a transformation of u* and\n\nRecall the two-step iterative procedure from Section 2.6. For the one-way model, the first step is to find the values of cx and 6 that maximize p(8,01,6/y) with 4 replacing 0. This density was obtained in the previous section. Let A surprising observation is that for prior 4 the estimate does not depend on ~2. The solution with prior 1 is the ratio of the appropriate sums of squares but, unlike the usual EB estimate, it can never be negative. It is, however, possible to get a value of zero.  The integral for g must be done numerically. An approach that is not necessarily the most efficient but is sure to work is to use separate numerical integrations on the intervals [O,l], [1,2], [2,4], [4,8], . . . until the contribution from the latest interval is sufficiently small. An iterative Gaussian integration converges fairly quickly. Any numerical analysis text (e.g., Burden, Faires, and Reynolds [Xl) is likely to prove useful.\n\nWhen doing numerical integration, it is important to know in advance if the integral will be finite. For the integral above it is sufficient to look at the balanced case. After removing some constants that depend only upon n and P, the integrand becomes h@)(l + nP6)-(k-')'2[C, + AZ/i5 + &(I + nP8)]--(N+q-3)'2 with cl and c2 being positive constants that depend only on the data. The four priors can be generalized to h(6) = Sdu2(1 -t-nBPh where v2 = vz for prior 4 and is zero otherwise and h is 1 for priors 2 and 3 and zero otherwise. It is necessary to verify the conditions under which the integral will exist as both 6 + 0 and 6 -+ ~0. For the first case, the essential part of the integrand is ij-yc3 + h2/q-'N+9-w2, For existence, either hZ>OandN+q-2v2>10r A2 = 0 and (u2 > 1 or v2 = 0) must hold. This condition is always satisfied for priors 1 through 3. For the second case, the tail behavior is governed by\n\nand so the integral will exist if k -1 + 2h + 2v2 > 2. This reduces to k > 3 for prior 1, k > 1 for priors 2 and 3, and k > 3 -2~2 for prior 4. Keep in mind that both conditions need to be satisfied. Rather than repeat these arguments for the integrals that follow, the existence results are summarized in a table at the end of this section.\n\nReturning to the estimation problem, the first quantity to compute is\n\nThe next, and most useful quantity, is\n\nThe next quantity of interest is Var(B,ly). From Section 2.7.2, two integrals are needed. The first one is similar to the one above. It is\n\nThe second one is Jviip(o,GJy)doA. With regard to cx (see Section 3.4), vii contributes a multiplicative constant of (Y and so the integral with respect to (y is similar to the one done in Section 3.6.\n\nTABLE 1 CRITERIA FOR THE EXISTENCE OF THE INTEGRALS To do all the calculations listed in the previous section requires a large number of approximate integrations. It would be helpful if those that are needed for each of the k groups individually could be avoided. A compromise that is reminiscent of EB methodology is presented here.\n\nTo proceed it is necessary to obtain a general result for the mean and variance of a function of a random variable. To do this begin with a general random variable X and a function g(x). Use the Taylor series expansion about 5 = E(X) to write WX)I + EMS) + (X -Sk'(81 = g(t) and Vark(X)1 G VarMO + (X -E>s'@l = k'(5)12Var(X).\n\nGenerally, for these approximations to be reasonable, the random variable X should in some sense be the average of a fairly large number of observations and the function g(x) should be thrice differentiable around 5. Almost any advanced text on mathematical statistics will contain theorems that make the above results precise. The random variable under consideration here is 6/y and its density will converge in the same manner in which a sample mean converges as the sample size, N, increases.\n\nTo evaluate E(fli(y) = E(w,& + (1 -Wi)ib), let S/y play the role of X and so 5 = E(6jy) = 6, a quantity obtained in Section 3.7. Then let Rj = P&(1 + P$). Finally, E(O& * @j& + (1 -~Cj)fi demonstrate the evaluation of prediction intervals. In both cases, the results will be compared to those obtained from the usual EB formulas.\n\nThe data were supplied by the National Council on Compensation Insurance (NCCI) and comprise observed frequencies from 7 years on 133 rating groups in 36 states. To make this data set somewhat manageable, the years were combined to yield the following: yij = relative frequency in state j from group i Pq = Payroll in state j from group i. i=l,..., k = 133 j = 1, . . . , FZ~.\n\nThe number of states per group (ni) is not always 36 as some states had no exposures for some groups. The total number of observations was N = 4,572, indicating that 216 cells had no exposure.\n\nThe objective is to estimate 8i, the relative frequency of claims from insureds in rating group i. While the payrolls were adjusted for inflation, no attempt was made to adjust for any trend in the number of claims. As a final note, only claims resulting in permanent partial disability were included. It is important to recognize that the purpose of these illustrations is not to recommend a specific ratemaking procedure for workers' compensation insurance, but rather to illustrate the calculations using the formulas of Section 3. In particular, one might check the possibility that a cross-classified (state by group) model better describes the process.\n\nFor comparison, the formulas recommended by Biihlmann and Straub [7] were evaluated. These are the conventional EB formulas and produce the following results:\n\nwhere b = EPidilP and P = CPi.\n\nWhen a negative value is obtained for r2, the convention is to use zero. This produces credibility weights of zero and so the grand mean is used as the estimate for each of the group means. As has been mentioned before, this method does not allow for evaluation of the quality of the estimates.\n\nThe key function in all the integrations isA6) as displayed in Section 3.7. The only item that involves the index j is Z(y,--&)' and it depends only on the data and so is constant. The number of observations per group will not affect the computation, other than the evaluation of a single sum of squares. All of the sums that involve 6 have k = 133 terms. A second item is the size and shape of the integrand. The function fl6) is well-behaved, being small at 8 = 0 and zero as 6 --$ 30, and having a single mode. However, because a variety of constants were removed from this density, it turned out that the value at the mode was very large. To see just how large, a variety of values of ln(A6)) were computed. In the actual calculations that followed, I worked with fl6)/exp(4,650) as the maximal value was near exp (4,650). Since all expressions are the ratio of two integrals involving this function, the adjustments cancel. Finally, it should be noted that f(6) was calculated by first obtaining the logarithm of each of its constituent factors, adding them, subtracting 4,650, and then exponentiating the result. This avoided any overflow or underflow problems in the intermediate calculations.\n\nIn Table 2, the results for the first three priors are displayed. Note that while the posterior mean of ~~ is indeed small (so zero was not an unreasonable estimate), it is large relative to a2/Pi, and so zero was not a reasonable choice for the credibility weight. This led to results that were considerably different from those obtained by the Btihlmann-Straub formula. Values of p. and the zi were found by solving the following system of k equations: E(eib) + z&i + (1 -~i)(i where p = Ez~~JCZ~.\n\nThe zi then take on the role of credibility factors. These do not automatically arise in a Bayesian framework, and except for the fact that actuaries are accustomed to seeing this quantity, there is no reason to compute it. The three classes displayed were the ones with the smallest, median, and largest values of 8i, respectively.\n\nIt is not surprising that the three priors produced virtually identical results. The large amount of data overwhelms all of these priors. In addition, I computed the standard deviation of (r2!y). Under prior 2 it is 0.000416. The mean of 0.003087 is over seven standard deviations above zero, indicating that the Biihlmann-Straub estimate is extremely unlikely to be valid.\n\nThe same items were evaluated using the approximations from Section 3.8. This was done only for prior 2, since the results will be similar for the others. The results are displayed in the last column of Table 2. In this case, the approximation performed well.\n\nTABLE 2 ESTIMATES BY INTEGRATION IN THE ONE-WAY MODEL Prior 1 Prior2 E(~/Y) .0006613 .0006498 E(~Y) 4.755 4.753 W%) .003143 .003087 Inferred p .07488 .07487 Class 107 (Auditors, Accountants, Draftsman) Pi 102,471 102,471 e, .002762 .002762 E(O;;; .9852 .9849 .003830 .003848 SD(Oijy) .006763 .006761 Class 68 (Explosives and Ammunition Mfg.) PI 3,018 3,018 6, .06262 .06262 E(Oiii .6638 .6599 .06674 .06679 SW~IY) .03237 .03227 Class 89 (Stevedore) PZ 11,275 11,275 6, .3895 .3895 E(Oi{i .8800 .8782 .3518 .3512 SW& .01980 .01979 CPU (sec.) 14.92 15.23 Cost ($) 3.91 3.97 Prior3 .0006503 4.753 .003089 .07487 EBStyle d .0006498 a .07488 102,471 102,471 .002762 .002762 .9850 cj .9852 .003847 .003829 .00676 1 .006762 3,018 3,018 .06262 .06262 .6600 Pi% .6623 .06678 .06676 .03227 .03234 11,275 11,275 .3895 .3895 .8783 K& .8799 .3512 .3517 .01979 .01980 15.11 12.49 3.94 3.50\n\nThe computation was done on an IBM 4381 computer. The time did not include that used for setting up the data set (computing and arranging the values of yii and Pii), so this can be viewed as the increase in cost of the Bayesian method over the Biihlmann-Straub formula (which is essentially free).\n\nIn addition, the iterative algorithm (Section 3.5) was employed with prior 2. Eight iterations were required for convergence. The results were 8 = 0.0005936, 6' = 4.625, and a = 0.07479. The results compare favorably with those obtained by integration.\n\nThis data set was taken from Meyers [25]. He provided loss ratios for three years of experience in 319 rating classes in the state of Michigan. In addition, the premium volume was given for each class/year; they will be used as the Pij as in the Meyers paper. In that paper he used the Btihlmann-Straub formulas to obtain the credibility estimates. In view of the success from the previous section, I only computed estimates based on prior 2 and the EB approximation. The results were (the column labelled EB contains the results from the\n\nMeyers paper): EB HNLM cr2 92,374 101,650 T2 0.019237 0.019762 P 0.5822 0.5799 K = 02h2 4,801,900 5,143,710\n\nIt is not surprising that the results are similar. This also indicates that the Btihlmann-Straub formulas are indeed based on a hidden assumption of normality.\n\nOne of the most useful features of the Meyers data set is that it also provided the premiums and actual losses for the year following the three years of experience. This admits an evaluation of the predictive ability of the various procedures. I will begin the evaluation by duplicating the two tests performed by Meyers. In performing the tests, the expected losses based on the estimated loss ratios were adjusted to make the total expected losses equal to the actual losses. This is legitimate, since both credibility procedures were formulated to indicate relativities, not the absolute level of future losses. To do so would require trend factors to be incorporated into the analysis. An indication of how this might be done within the HNLM is given in the next section.\n\nThe first test is to measure the squared error of the predicted versus the observed losses. Bayes procedures (of any kind) should do well since the objective is to minimize squared error. The formula is XPi(Ai/Ei -1)*/k where Ai is the observed losses, Ei is the expected losses, and P; is the premium. Also available for this test were the losses expected according to the rates promulgated by the NCCI. In addition, the weighted average relative error of the predictions was computed. The formula is CPi(A,/Ei -ll/XPi.\n\nThe results were: Mean squared errors Mean relative errors NCCI 298,063 0.26776 EB 289,651 0.26396 HNLM 287,416 0.26368\n\nThe second test was invented by Meyers [25]. He called it the \"Underwriting Test.\" The idea is to consider an insurer with established rates and a new entrant into the market. The new entrant uses his own method to determine premiums. He then offers insurance only to applicants in those rating classes for which his calculations produce rates less than those of the established insurer. He then charges a slightly lower premium than the established insurer and gets all of this business. If the new entrant's ratemaking methods are superior, he will expect a profit from his actions. Assuming differences only in relativities, but not in overall level, the established insurer will lose the same amount that the new entrant gains. A formalization of this process has Ai as the actual losses, Ei (for established) as the established insurer's expected losses, and N, (for new) as the new entrant's expected losses. The profit and loss ratio, respectively, for the new entrant will be C(Ei -Ai) and CA;/CEi, where all sums are taken over those classes for which N, < Ei. The comparisons among the three estimators are presented in Table 3. In these classes HNLM scores a big \"win\" over EB, although the predictions are indistinguishable. Also note that in class 8033, HNLM defeats EB but loses to NCCI. It happened that there were no similar cases producing great gains under EB with a premium only slightly better than HNLM. The similarity of the expected losses should not have produced such a large overall difference between EB and HNLM. I attribute this to the method itself.\n\nRecall that one of the stated advantages of HNLM is that it also produces prediction intervals for the future observations. This was done for the 319 classes. The standard deviations of the predictions were computed according to the approximation in Section 3.8. The formula is Var(Bi\\y) e (di -0.5'799)2(Pi)2(2.030 X 10-15)/(1 + 1.957 X 10-7P34 + (0.01993 + 3.842 X 10e9Pi)/(l + 1.944 X 10-7Pi)2.\n\nIf all is well, the standardized actual losses (actual minus predicted divided by the standard deviation) should follow a normal distribution with mean zero and variance one. To see if that is so, two plots were prepared. Figure 1 shows a histogram of the 319 standardized losses. It is apparent that there is more skewness present than one would expect from a normal distribution. The chisquare goodness-of-fit test statistic using 20 intervals is 71.55. With 17 degrees of freedom there is clearly a lack of fit. Figure 2 is a plot of the standardized errors against the expected losses. This can be used to check for serial correlations and constant variances. The former is not a problem and that is confirmed by performing a sign test. There are 153 sign changes out of 318 opportunities, clearly close to the expected number of 159. There does appear to be a problem\n\nHistogram from 319 predictions 50 1 with the variances. For small predicted losses, the points are much too concentrated about the horizontal axis. This would lead us to suspect that we are overstating the variances in this range. One way to allow for this would be to adopt the unequal variance model of Section 5.1.\n\nWith these problems in mind, is there any value in attempting to predict these values? I believe there is. First of all, as was stated in Section 1, an inadequate model is almost always better than none at all. Secondly, we have some idea of the shortcomings, and could make some ad hoc corrections in the future.\n\nAs an illustration of the benefit of knowing the prediction errors, consider the following analysis which is done in the spirit of the \"Underwriting Test.\" Identify all classes for which the HNLM predicted loss exceeds the NCCI predicted loss by at least k standard deviations. Do not offer insurance to these classes. For k = 1 there is only one class, number 4420. In thousands the -----1 800000 le6\n\npredictions were 4,329 and 3,427. The actual loss turned out to be 3,855. The point here is that this analysis can help identify classes in which the current rate levels are out of line, perhaps inspiring an investigation to see if something unusual has happened, either to the insureds in that class, or to the data in the process of recording it.\n\nA final comment is in order. The above analysis leads us to believe that the normal model is not appropriate for these losses. In most settings we would not have the actual losses available in order to check this out. Can this be done with the original data? Box [4], [5] suggests the following approach to modelchecking. In the general Bayesian setting, let X be the marginal distribution of the observations. Its density is computed from f(x) = @$(9)fltl)& where, as usual, &]0) is the model density and f(O) is the prior density. If the model and prior are reasonable, the observed data x should, in some sense, be a \"typical\" observation from this density. While Box suggests a specific test, I will just display the standardized observations. First Level Std. Obs.\n\nHistogram from 957 values Figure 3 In particular, I will restrict attention only to the assumption of normality and will condition on the other aspects of the model such as constant variance. I will also condition on the estimated values of the variances. In the general HNLM, the distributions of interest are In the particular case of the one-way model, these distributions become Figures 3, 4, and 5 display the histograms for the three sets of observations. In each case the appropriate values Cyii or 6i) were standardized according to the indicated means and variances. If the normal model was correct, the histograms should correspond to the standard normal distribution. An examination of the figures indicates that normality might indeed hold at the first level, but definitely does not at the second level. As a result, it is clear that the overall model should not be normal and that is indicated by the histogram.\n\nDoes the discovery of non-normality invalidate all the work that has been done? I believe the answer is no. We are at least as well off as one who used the EB methodology and we have the additional knowledge that we do not have the optimal solution. It is now a matter of deciding if the extra effort of analyzing a non-normal model is justified. Perhaps the ideas suggested in Section 1. 5. OTHER HIERARCHICAL LINEAR MODELS\n\nIn this section I will present a number of other models that fit the framework set out in Section 2. No attempt will be made to analyze these models and, in particular, no attempt will be made to assess the computational difficulties of evaluating these models. Unless there is an indication to the contrary it should be assumed that all of the random variables at a given level of the model are conditionally independent.\n\nThe process variance within each class may differ from class to class. From year to year within one class it is still assumed that variances are proportional to some exposure measure. The first two levels of the model are Noninformative priors would then be placed on p,, r2, and UT, , . . , a:. It is easy to see what the EB approach to this model would be. Each (T? would be estimated from data in the i\" class. This is not in the spirit of credibility analysis where we would expect that information from the other classes can improve the estimation of a particular a:. A model that would do this would have an additional component at Level 2 such as cdlu,X -Inverse gamma(v,X).\n\nNoninformative priors would then be placed on l.~, TV, u, and A.\n\nSuppose it is possible that the class mean 8i varies from year to year, but not in any predictable manner. A model for this would be This is similar to a model proposed in Meyers [26]. It is not possible to derive EB estimates of the three variance terms as the within sum of squares is all that is available to estimate both u2 and y2. There is, however, a least squares approach based on the relationship of the variance in one group to its exposure that can yield estimates of the three parameters. A detailed HNLM analysis of this model is presented in Klugman [23]. The major problem is the evaluation of a two dimensional integral.\n\nIn this paper, the word hierarchical applies to all the models. In credibility work this term has been reserved for the case where the k classes can be divided into g groups, where the i* group would have rni classes in it (ml + . . . + mg = k). Begin with a three level model: Noninformative priors would be required for p, 72, y*, and u2. Levels 2 and 3 may be combined to form a single distribution. However, when conditioned only on p, the t3;/ are no longer independent. EB formulas for this model and the one in Section 5.4 are given in Venter [34].\n\nSuppose each rating class is identified by two variables, such as sex and age, or state and occupation. An additive model, with the possibility of error, can be expressed with three levels: The matrices F, and G, are known while A and B require prior distributions. The first level is the process distribution which explains how the observations relate to the underlying parameters. The second level is the state distribution which explains how the parameters change over time.\n\nAs an example, consider the linear trend model from Section 5.4. In this setting it would look like Level 1-Y$3U -N(0U,cr2/Pjj) Level 2-0,10i,j-1 -N(pi + O,j-l,T').\n\nIt is not exactly the same, as level 2 implies that there are some disturbances that let the progression of means depart from strict linearity. The parameter (pi in Section 5.5 is unchanging over time. Prior distributions would be needed for &,o (to get the system started) and for pi, r2, and u2. This model is very similar to the Kalman filter. An excellent non-Bayesian application of this model to loss reserving is found in deJong and Zehnwirth [ 181. A discussion of its relationship to the usual credibility models is given in deJong and Zehnwirth [19].\n\nThe intent of this paper was to introduce the hierarchical normal linear model as a tool for classification ratemaking. This model has three advantages over the EB approach. First, methods for estimating the variances do not have to be created on a case-by-case basis. Instead, the estimates fall naturally out of the analysis. Second, estimates of estimation and prediction error are available. Finally, model-checking and model-selection procedures can be employed. The latter was not discussed in this paper, but methods do exist for identifying the most appropriate model when there are several to choose from (for example, a one-way vs. a cross-classified analysis). See Klugman 1231 for an application.\n\nOf course, this approach also introduces difficulties of its own. Foremost among them are the intensive computations needed to perform the analysis. In addition, the derivation of formulas for specific models can be very time consuming (although once obtained they can be used over and over). These prob-lems are really another advantage of the HNLM approach; they are all technical in nature and are certain to be solved if there is sufficient interest in doing so.\n\nThe major area for future work (other than grinding out the solution to the many models of interest) is the relaxation of the normality assumption. There is overwhelming evidence that insurance data are not normal and so methods to accommodate that fact are most desirable. I envision two ways to attack this problem. One is to create methods that are robust against general departures from normality. To do this, the t-distribution or a mixture of normal distributions could be used in the model. Another way would be to find methods that are superior under specific distribution assumptions that are likely to correspond to insurance experience. In any case, considerable sensitivity testing should be done to any recommended formula."
}