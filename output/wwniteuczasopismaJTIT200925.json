{
    "title": "Percolation Driven Flooding for Energy Efficient Routing in Dense Sensor Networks",
    "publication_date": "2002",
    "authors": [
        {
            "full_name": "Gergely Vakulya",
            "firstname": "Gergely",
            "lastname": "Vakulya",
            "affiliations": []
        },
        {
            "full_name": "Gyula Simon",
            "firstname": "Gyula",
            "lastname": "Simon",
            "affiliations": []
        }
    ],
    "abstract": "Simple flooding algorithms are widely used in ad hoc sensor networks either for information dissemination or as building blocks of more sophisticated routing protocols. In this paper a percolation driven probabilistic flooding algorithm is proposed, which provides large message delivery ratio with small number of sent messages, compared to traditional flooding. To control the number of sent messages the proposed algorithm uses locally available information only, thus induces negligible overhead on network traffic. The performance of the algorithm is analyzed and the theoretical results are verified through simulation examples.",
    "full_text": "Energy efficiency is a key question in wireless sensor networks made of large number of inexpensive nodes with limited energy reserves and restrained processing and communication capabilities. Such networks have been used in a wide range of applications with various goals, using different platforms and technology [1]- [4]. Independently of the nature of the application, however, certain middleware services are always present, one of them being routing. Limited resources on the sensor nodes require that routing protocols be simple, energy-conserving and robust. Depending on the actual deployment scenario and available hardware, several ad hoc routing algorithms have been proposed. Location aware routing schemes, e.g., greedy perimeter stateless routing [5], location aided routing [6], distance routing effect algorithm [7] use position information of the nodes obtained from global positioning system (GPS) or other localization services. Other data centric schemes do not use location information, e.g., directed diffusion [8], ad hoc on demand distance vector routing (AODV) [9], dynamic source routing [10], temporarily ordered routing [11], or zone routing [12], just to name a few. To allow sensor networks better cope with scaling, hierarchical protocols were designed, e.g., low energy adaptive clustering hierarchy (LEACH) [13] and threshold sensitive energy efficient sensor network protocol (TEEN) [14].\n\nThe simplest routing protocol is flooding [15], which is a useful simple (but inefficient) protocol in itself, and also is a building block for several more sophisticated algorithms, e.g., directed flood routing [16], controlled flood routing [17], tiny ad hoc routing [18], or AODV [9], which is used in Zigbee's routing protocol [19].\n\nThe basic flooding protocol is the following: 1. Start. When a source node intends to send a message to all other nodes in the network, it broadcasts the message to its neighbors.\n\nRelay. When a node receives a message for the first time, it rebroadcasts the same message to its neighbors. All other copies of the same message, received later, will be discarded by the recipient node.\n\nFlooding naturally can be used in its basic form for networkwide dissemination of information. In this context the important performance criteria of the algorithm are delivery ratio, latency, and the energy efficiency. Flooding is also used for route discovery [9]. Route discovery protocols find a route from a source node to a destination node in two phases. First the network is flooded by a discovery request message originated from the source, and then a reply message is sent back from the destination to the source node. In this way the network learns the path from the source to the destination and this route will be used in the subsequent communication sessions. In this context the quality -primarily the length -of the discovered route is an important performance criterion as well.\n\nApart from its simplicity the main advantage of the flood routing protocol lies in its robustness: the implicit redundancy built in the algorithm provides resistance against high degree of message loss and node failures as well. The drawback of the algorithm is the large number of packets transmitted in the network, referred to as broadcast storm: whether it is necessary or not, each node will retransmit each message. In a dense network the unnecessarily high number of messages can cause frequent collisions (and thus performance degradation) and it wastes network energy as well [20].\n\nTo reduce the number of routing messages, probabilistic variations of the flood routing were proposed. The main idea behind these algorithms is that a node randomly decides whether to forward a message or not. Several variants of random flood algorithms were proposed. According to the simplest protocol, a node will rebroadcast a message after its first reception with probability p and discards it with probability 1 -p (clearly, p = 1 results in basic flooding).\n\nTest results verify what intuition suggests: higher p values provide higher network coverage than small p values. Moreover, in sufficiently large and sufficiently dense random networks there is an interesting bimodal behavior of the algorithm: if p > p c , where the critical probability p c depends on the network topology, the message reaches practically all nodes in the network, otherwise only a small portion of the network receives the message. Thus the optimal choice clearly would be p = p c . Modified algorithms try to further increase the performance by various modifications; e.g., premature message death can be avoided by varying p as a function of hop-distance from the source: nodes close to the source rebroadcast messages with higher probability, while distant nodes use smaller p. A comprehensive study on random flooding algorithms can be found in [15]. The common problem with these algorithms is that the design parameters (probabilities) depend on actual network layout, no automatic or adaptive solution is known. Especially in a network with varying node density the probabilistic algorithms tuned to work reliably will be suboptimal.\n\nMore sophisticated adaptive schemes can successfully handle networks with varying properties as well. Location based algorithms use node positions to optimize retransmissions [21], graph-based algorithms utilize connectivity information up to 2-hop neighbors to construct a dominating set in a distributed way [22]. Other heuristic rules, e.g., message counters and neighbor coverage schemes were also proposed [20]. These schemes can adapt automatically to changing topology at the price of higher complexity.\n\nSensor networks are often modeled by the Poisson-Boolean model. In this model nodes are scattered on the plane by a Poisson point process with constant density λ , and each node has a fixed communication radius r (disc model). Two nodes can communicate with each other if their distance is less than r. The Poisson point process applies reasonably well to controlled, but still random deployments. The simplistic disc model, however, is far from reality, where communication range is not circularly symmetric, but rather has an anisotropic shape. Speaking of connectivity, however, the disc model has an important property: it can be considered the worst case model, since other transmission models provide easier percolation under similar circumstances [23].\n\nPercolation theory [24] has important impact on wireless communication networks. Results in continuum percolation theory show that if the density of transmitting stations (or alternatively, their communication power) in a Poisson-Boolean network is greater than a critical value λ c then an unbounded connected component is formed with probability one (the network percolates) [23], [25]. On the other hand, if λ < λ c then all connected components are fi- nite with probability one. Thus percolation is an important property of a network if long distance multi-hop communication is required.\n\nIn this paper a new percolation inspired solution will be proposed based on probabilistic flood routing algorithms.\n\nThe new algorithm adaptively sets the rebroadcast probability based on the network topology, using locally available neighborhood information only. The proposed algorithm provides high performance with a low number of messages and can adapt its behavior dynamically to the network properties. The proposed algorithm is only marginally more complex than the basic flood routing thus it can be successfully used in applications with limited resources as well.\n\nThe rest of the paper is organized as follows. In Section 2 related works will be summarized. In Section 3 the proposed algorithm will be introduced. An important property of the algorithm will be proven based on percolation theory: every message broadcast in the network will reach infinite number of nodes almost surely. In practice this property ensures that the message reaches almost all of the nodes in a finite-sized network. The performance evaluation of the algorithm through simulation can be found in Section 4.\n\nConclusions are drawn in Section 5.\n\nResults of percolation theory have been applied to wireless networks, forming the theory of continuum percolation [26]. More realistic network models, i.e., unreliable communication links and anisotropic radiation patterns were studied in [23]. An important result of [23] shows that the debatable disc model actually provides a conservative estimate of the network connectivity: percolation in networks with disc communication shapes is more difficult than in networks with any other convex communication shape with the same surface (\"discs are hardest\" conjecture). This property justifies the usage of the disc model when connectivity issues are studied.\n\nResults from percolation theory have been infiltrating various recent sensor networking algorithms. As a practical application of this phenomenon, the distributed minimumlink-degree rule was proposed to counterbalance local spatial inhomogeneities in ad hoc networks [25]. To provide network wide connectivity, the transmission powers of the nodes are tuned so that each node has at least a minimum number of neighbors.\n\nIn [27] a distributed energy saving mechanism is used by switching the nodes in the network on and off, while providing percolation in the network. The simple rule for the algorithm is to keep the active time ratio of the nodes larger than λ /λ c . This algorithm does not scale well since each node in the network has to know the global parameter λ . The idea was further elaborated in [28], where a practically useable distributed algorithm was proposed for the control of the active time ratio. In this algorithm only the local density, namely the number of neighbors is used. For a node with degree k the active ratio η k is defined as follows:\n\n, where φ is a density independent design parameter. A similar idea will be used in the proposed flooding algorithm.\n\nThe idea behind the percolation driven flood routing is the following: use probabilistic flooding and set the retransmission probability adaptively at every node so that with high probability the message reaches almost all of the nodes in the network. The algorithm is the following:\n\n1. The source node broadcasts the message with probability one.\n\n2. After the first reception of a message node n rebroadcasts it with probability p n , and discards it with probability 1 -p n . Copies of the same message received multiple times are discarded with probability one. Probability p n is defined as\n\nwhere: K n is the degree of node n (i.e., the number of neighbors of node n), and the K min design parameter is the required minimum number of neighbors. Two nodes are neighbors if they can hear each other (i.e., a symmetric link exists between them).\n\nThe algorithm requires neighbor discovery to determine the number of neighbors. Each node can gather this local information by transmitting and receiving hello messages. In practical situations parameter K min is chosen around 7, as will be justified by the simulation examples.\n\nTheorem 1: If in the infinite random network, generated by a Poisson-Boolean process with density λ and communication radius r, there exists design parameter K min < ∞ independent of λ , such that when node n with degree K n transmits a message with probability according to Eq. ( 1), then each message reaches infinite number of nodes with probability one.\n\nProof : Let us denote the graph representing the network by G(λ , r, 1) where λ is the density, r is the communication radius, and the third parameter is a probabilistic value used in the generator process (used and discussed later) and is simply set to one at the moment. Using scaling, we define another process with density λ ′ = λ r 2 and communication radius 1. This process yields the same graph G(λ ′ , 1, 1). Now let us use the following Theorem 2 [28].\n\nTheorem 2: Given G(λ ′ , 1, 1) with λ ′ > λ ′ c , there exists 2 < ϕ < ∞, independent of λ , such that when each node with degree k is active with probability:\n\n)) is percolated. The proof of Theorem 2 can be found in [28].\n\nIf in the network we decide before a particular message is broadcasted which nodes will retransmit the message (when they receive it) by activating/deactivating nodes with probability (1) we get G(λ ′ , 1, p(.)). In this way we construct a virtual network for each message and we can apply Theorem 2 to it. Thus there exist K min = ϕ < ∞ so that G percolates. From this it follows that the message will reach infinite number of nodes [24].\n\nIn real-world finite-size networks Theorem 1 means that with an appropriate choice of K min each message reaches practically all nodes in the network. Further properties of the algorithm will be analyzed through simulation examples.\n\nIn this section the performance of the algorithm will be evaluated through simulation results. First the performance metrics and the network/communication models will be introduced, followed by the simulation environment and the simulation scenarios. Finally, the results of the simulations will be presented to illustrate the behavior of the proposed algorithm.\n\nThe main performance criteria of broadcast algorithms are the message delivery ratio and the number of sent messages.\n\nIf the number of nodes in the network is N and the number of nodes receiving the message is N rec then the coverage ratio is defined as C r = N rec N . Clearly, C r close to 1 is required for a good quality of service. Another quality metric is the total number of sent packets M while a message is propagated in the network. Trivially, for basic flood routing M = N, if C r = 1. We expect lower number of messages and still high delivery ratio for a good performance algorithm. For easier comparison, we will use the normalized number of messages defined as\n\nIf flooding is used for route discovery, an important performance metric is the length L of the discovered route. Other possible metrics are message delay/latency, length of flood period, and number of collisions. The actual low level details, e.g., implementation, hardware and media access control (MAC) layer properties are not investigated in detail in this paper, but their effect is examined through high level parameters they have impact on, i.e., C r , M norm , and L will be studied.\n\nTo model random deployment of nodes or possibly mobile networks, in the simulations nodes are placed at random, according to a uniform distribution on a two-dimensional area. Finite communication distances are represented by the disc model, where a communication link is assumed to exist between two nodes if they are less than the communication radius apart. According to the results published in [24], this -otherwise rather simple and idealistic -model can be considered a worst case model. Our communication channel model does not deal with specific details of the physical layer or the MAC layer; rather we use a probabilistic model: a message can reach a neighbor within the communication radius with probability p rec < 1. This high-level model represents message losses due to collision, fading, or other disturbances as well.\n\nThe model does not distinguish between individual phenomena but rather incorporates the different sources in one parameter, thus some aspects of reality (e.g., inter-message dependencies) are neglected, but the model is faithful enough to provide useful and easy means for testing.\n\nTo perform high-speed simulation, a simulator was written in C to validate the efficiency of the proposed algorithm. The program places nodes randomly according to the different test scenarios considered. The number of nodes N, the communication radius r, and K min are input parameters.\n\nIn each simulation, a new placement is generated, and a source node starts transmission. The simulator returns the size of connected component containing the source node, the number of nodes receiving the packet, and the total number of messages. Optionally, the software can visualize the topology of the network, the active data paths, and the nodes' reception status.\n\nIn the test we used 3 different scenarios to model sensor network setups. The first scenario is a random uniform distribution (with constant density), while in the second   scenario we used three regions; in each region nodes were placed with uniform random distribution, but with different densities. In the third scenario the densitiy is increasing towards the centre, thus there is continuous change and extreme deviation in local density. Typical examples for the test networks can be seen in Figs. 1, 2 and 3.\n\nTo test coverage ratio C r and the number of messages M we placed 3000 nodes in a square-shaped area, as shown in Fig. 1.\n\nWe varied the communication radius to provide different network densities: the average number of neighbors K was set to 10, 30, and 100. The K min value varied from 1 to 20 in 0.25 steps. We run the simulation 100 times for each communication radius, K min , and p rec values, thus each point in the subsequent graphs is the average of 100 experiments. Figure 4 presents coverage versus K min , while p rec was set to constant 1 (error-free communication). The for different network densities are similar: when K min is low (K min < 3 ), the message delivery ratio is very low. If K min is increased, the coverage is increasing quickly, the critical value being around K min ≈ 4.5. Coverage reaches 90% at K min ≈ 5. If K min > 7, practically all nodes in the network receive the message. Figure 4 illustrates that percolation driven flood can indeed be used in networks with different densities: K min is independent of the actual network density. In Fig. 5 the normalized number of sent messages M norm is shown versus K min , for the previous experiments. In case of the conventional flood routing M norm = 1, because all nodes relay the received message. As Fig. 4 illustrates, K min > 7 gives almost perfect delivery ratio. Around this value K min ≈ 7 the total number of messages is greatly decreased, as presented in Fig. 5. Depending on the node density, in the tests the total number of messages were reduced by 30..95%, higher reduction rates belonging to higher densities. Clearly, in networks, where the average node degree K is only slightly higher than K min the number of messages can be decreased only moderately, while in dense networks a much lower number of messages is enough to provide good delivery ratio, as shown in Fig. 5. According to Figs. 4 and 5, in dense networks the percolation driven flood algorithm can effectively reduce the of message while good coverage. Figure 6 illustrates the effect of unreliable communication links. In the experiment constant density (K = 30) was used, while the p rec reception probability was varied between 0.3 and 1. The figure shows that the algorithm can provide high coverage even in the presence of bad quality communication links, but as intuitively expected, higher K min is necessary as the communication channel degrades.\n\nFigure 7 presents the associated number of messages for the unreliable communication experiment. Percolation driven flood routing algorithm is capable of handling varying network densities. To test this property we used the scenario illustrated in Fig. 2, where three areas with different densities are present. Clearly, a constant retransmission probability would either be suboptimal (set to provide sufficient coverage in the less dense area as well) or would not provide good coverage in the sparse regions of the network.  8 illustrates the effectiveness of the percolation driven flood showing the coverage versus K min when the densities of regions from left to right were set to [10, 20, 40], [30, 60, 120], and [100, 200, 400]; and p rec = 1. The behavior is quite similar to that of the first scenario (constant density): percolation happens around the same K min ≈ 5, and practically full coverage can be provided if K min > 7. The associated total numbers of messages are shown in Fig. 9, where the gain with respect to the basic flooding algorithm is apparent. In Fig. 10 the number of coverage versus K min is illustrated in the circular scenario. In the experiments the number of neighbors varied between [120..4], [220..20] and [525..50], respectively. The figure clearly presents the excellent performance of the algorithm even if there are extreme differences in the number of neighbors of each node. The ideal value for K min is around 7, in this case the algorithm gives  Fig. 11. Number of total messages versus K min for different network densities in the circular scenario, p rec = 1. the algorithm gives a gain of 55-95%, with respect to the original flooding. The length L of the discovered route is important when flooding is used for route discovery. To test the properties of percolation driven flooding in this respect we measured hop distances discovered by flooding and percolation driven flooding, for different K min values. In the tests the uniform distribution (constant density) scenario was used with K = 35 and p rec = 1. The source node was placed to the center of the field and hop distances to all other nodes were measured. The histogram created from averaging 100 independent experiments is illustrated in Fig. 12. The distribution is linear for small hop distances. The explanation is shown in Fig. 13 for the ideal case: messages are propagated in belt-shaped increments. The areas of the subsequent belts increase linearly, thus the number of nodes in each belt is linearly increasing. If the node density is smaller, the number of nodes in each belt is smaller, thus the slope of the histogram will be smaller. According to Fig. 12, smaller K min also causes decrease in the slope: e.g., for K min = 9, the slope is 40% smaller than in the basic flooding case. This means that the detected route is 40% longer than that of basic flooding. The declining part at higher hop distances due to the finite size of the network: the messages reach the network  perimeter and eventually die. The tail of the distribution presents the maximum hop distance from the source, which is in the square setup ideally √ 2 times the hop distance measured at the maximum of the histogram. The effect of unreliable links is shown in Fig. 14. The histogram shows the average of 100 experiments conducted in the uniform test scenario with K = 35, and p rec = 0.4..1.\n\nAs intuitively expected, unreliable links cause smaller slope, i.e., higher hop distances. For example, link quality p rec = 0.7 results in the increase of hop distance by 18%, while for the rather unreliable link quality p rec = 0.4 the increase was as high as 63%.\n\nAs simulation examples show, percolation driven flooding results in considerably higher hop distances than basic flood routing when small K min parameters are used. Naturally, low link quality also causes larger hop distances. This effect may be an undesired side effect in route discovery protocols, thus in these applications basic flooding may be more advantageous.\n\nA percolation driven flood routing algorithm was proposed, which uses only locally available neighborhood information to reduce broadcast storm. The algorithm is able to massively reduce the number of messages in the network and maintaining high delivery ratio at the same time. Theoretical results prove the usefulness of the algorithm: it is able to provide high coverage, if the network density is high enough. Simulation tests were performed to validate the performance of the algorithm. The proposed algorithm reduced the total number of messages in the network by 30-95%, depending on the network density, while the coverage was almost 100%, with appropriate choice of K min (K min > 7). The percolation driven flood algorithm is adaptive to changing node density thus provides high coverage with low number of messages in all scenarios. The algorithm is robust in the presence of unreliable links as well."
}