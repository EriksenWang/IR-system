{
    "title": "List of work not included in the thesis A LUNAR over Bluetooth",
    "publication_date": "2001",
    "authors": [
        {
            "full_name": "Olof Rensfelt",
            "firstname": "Olof",
            "lastname": "Rensfelt",
            "affiliations": []
        },
        {
            "full_name": "Richard Gold",
            "firstname": "Richard",
            "lastname": "Gold",
            "affiliations": []
        },
        {
            "full_name": "Lars-Åke Larzon",
            "firstname": "Lars-Åke",
            "lastname": "Larzon",
            "affiliations": []
        }
    ],
    "abstract": "Overlay networks is a popular method to deploy new functionality which does not currently exist in the Internet. Such networks often use the peerto-peer principle where users are both servers as well as clients at the same time. We evaluate how overlay networks performs in a mix of strong and weak peers. The overlay system of study in this thesis is Bamboo, which is based on a distributed hash table (DHT).\nFor the performance evaluation we use both simulations in NS-2 and emulations in the testbed PlanetLab. One of our contributions is a NS-2 implementation of the Bamboo DHT. To simulate nodes joining and leaving, NS-2 is modified to be aware of the identity of overlay nodes.\nTo control experiments on PlanetLab we designed Vendetta. Vendetta is both a tool to visualize network events and a tool to control the individual peer-to-peer nodes on the physical machines. PlanetLab does not support bandwidth limitations which is needed to emulate weak nodes. Therefore we designed a lightweight connectivity tool called Dtour.\nBoth the NS-2 and PlanetLab experiments indicate that a system like Bamboo can handle as much as 50 % weak nodes and still serve requests. Although, the lookup latency and the number of successful lookups suffer with the increased network dynamics. I would like to thank my supervisor Lars-Åke Larzon. He has always been very helpful and supportive and makes it fun to go to work. I would also like to thank my secondary supervisor Per Gunningberg, not only for all the help with the thesis but also for creating such a creative work environment in the communication research group (CoRe).\nI have very much enjoyed working with Sven Westergren and without him this thesis would have been very different. His PlanetLab skills have been invaluable and I am very grateful to him. I would also like to thank Peter Drugge for all his work on Vendetta and Magnus Rundlöf for implementing Dtour. It has been great fun working with all of them.\nI would also like to thank Arnold Pears for his feedback on the thesis as well as the other CoRe group members. They have helped much through discussions and feedback. So I would like to thank Christian Rohner, Erik Nordström, Oskar Wibling, Laura Feeney, Thabotharan Kathiravelu, Ioana Rodhe, and Fredrik Bjurefors.\nPast members who I would also like to thank for helping me when I was a new student are Henrik Lundgren and Richard Gold.",
    "full_text": "Paper A: I implemented the overlay system in NS-2 and performed the simulations. I also analyzed the data and was the main author of the report.\n\nPaper B: I participated in the design process and implemented the C-client of Vendetta. Vendetta was implemented as a Master thesis which I supervised.\n\nPaper C: I had a big role in the design of doing connectivity modeling in a Pre-loaded library and I implemented the generic filter support. I worked both on the simulations and the PlanetLab experiments and I was the main author of the paper.\n\nChapter 1\n\nAs new networking technologies constantly evolve, we need to gain new understanding of how to evaluate them. A dominating trend in the Internet is that a wider spectra of devices are connected. The change from a network environment consisting of mainly desktop machines to a network with mobile users, cellphones, and new phenomena like peer to peer networks stretches the capabilities of the current design of the Internet. Weak Internet nodes become more common, strong nodes become stronger; and network heterogeneity increases as access technologies range from low-bandwidth wireless networks to GigaBit Ethernet. It is not only the nodes that are changing but also how nodes communicate. New communication models appear for nodes being both clients and routers in ad hoc networks, intermittent connectivity in delay tolerant networks, data centric communication in sensor networks, and the use of overlay systems to provide indirection points.\n\nNew applications need new functionality in the Internet. The deployment of new functionality is a very slow process. The Overlay concept to add new functionality avoids this by adding new functionality between the application and the existing Internet.\n\nMany overlay services are peer to peer networks, which creates traffic patterns that were not foreseen during the design of the Internet.\n\nThis new functionality, combined with the increasing heterogeneity, creates a need for new ways to evaluate the performance of the system. Although much of the experience gathered from evaluating applications and protocols in a fixed network are still valuable, they do not cover how to study the impact of node mobility, or how a system handles nodes joining and leaving the network in an unpredictable manner.\n\nThe main question explored in this thesis is whether it is possible to have nodes with limited connectivity, for example cell phones, as members of applications based on distributed hash tables (DHTs) and how a mix of different nodes would influence the performance of such a system. The work is motivated by the increasing number of 3G enabled devices, and the availability of flat rate pricing for such devices, which makes it more likely that users would use a bandwidth intensive application. The contribution is an increased understanding of how a DHT performs in a heterogeneous environment. Different methods are used to explore the impact of heterogeneity. Another novelty is our design of the evaluation tools, including the extension of previous tools for this environment.\n\nPrevious work on the DHT Pastry in heterogeneous environments [4] suggests that management traffic could be a problem because it causes the mobile nodes access links to get congested. The network management was redesigned in the follow-up DHT to Pastry, called Bamboo [22]. It performs updates periodically rather than when changes occur. One advantage is that it can handle higher network dynamics and avoids feed-back loops.\n\nOur first evaluation approach uses simulations to allow us to configure link parameters like bandwidth and delay. We use the NS-2 simulator [9], the \"standard\" simulator in the research community. To study a DHT in simulation means that the DHT has to be re-implemented from scratch which is a very time consuming task (paper A). The simulations of Bamboo indicate that mobile nodes can participate in the DHT, but that the complexity of the system makes it impossible to simulate long enough scenarios to make any firmer claims. The most valuable outcome of the simulations was the understanding of how to design scenarios with thin nodes and mobility which place stress on the system. The insight that the complexity of the problem made simulations hard to perform was also valuable.\n\nOur second approach was to perform experiments on PlanetLab [6]. As a part of this work, our tool Vendetta was developed to manage and analyze the experiments (Paper B). Vendetta consists of two parts, the client and the monitor. The client runs on every node that participates in the experiment and controls the application that is evaluated. The monitor is a central control and visualization application. When the application runs, the client captures the output from it and continuously parses it for predefined log entries. When a log entry is found, the client performs a configurable action. Such actions can for example be to stop the application and send a message to the monitor. The monitor receives such messages from the nodes in the experiment and can visualize them in a 3-D canvas. The contribution of Vendetta is not the individual functions but how they are incorporated into one powerful tool and Vendetta has proved very valuable for understanding unexpected behavior in Bamboo.\n\nA reason for using simulations in paper A was the need to limit link capacities and a need to have complete control of the network topologies. When you do experiments on a testbed like PlanetLab, you do not have to worry about the accuracy of the network model as the physical network you use is the Internet. To create a heterogeneous environment on PlanetLab, the bandwidth of some of the participating nodes needs to be limited. Unfortunately that functionality is not currently offered by PlanetLab. That need prompted us to design the Dtour tool. Dtour is a connectivity emulation system residing in user-space, implemented as a shared library. When an evaluated application uses system calls to send and receive traffic, the system calls are redirected to Dtour. There the traffic is either dropped or forwarded after being filtered. The bandwidth limitation is implemented using a token bucket that the packets need to pass before going into the network stack. The design is very lightweight with no needs to modify the operating system.\n\nAn overlay network is a logical network built on top of an existing network infrastructure such as the Internet. The strength of overlay networks is that you do not need to modify anything in the existing Internet to deploy them, as you only use the Internet as a transport service between nodes in the logical network. Overlay networks are often used to provide functionality which are missing in the existing Internet, for example to support mobility [23], media services [12] or virtual private networks (VPNs). It is also a fast way to deploy new services compared to incorporating them in the Internet design.\n\nIn networking terms, an overlay network uses the Internet as a globally distributed link layer, as a \"link\" between two nodes. When an overlay node sends traffic to another overlay node, it looks like the traffic is sent directly to the other node, although it may be sent through many physical machines. Many overlay networks offer a lookup service to their users. A Lookup consists of a query from a user that gets a response from the network.\n\nPeer to peer (p2p) networks are overlay networks where all participating nodes have the same initial functionality. In a p2p system, all nodes are peers in the sense of having equivalent roles and responsibilities. The p2p model is in contrast to the classic client-server model used for example in web services, where different nodes have clear roles in the system. All decision made within a pure p2p network are distributed, since there is no central decision point. Some p2p networks do however select supernodes which are nodes with high performance and stable network connections [8]. Such nodes are typically used to perform network management tasks. Even though supernodes might seem a contradiction to the p2p philosophy, all nodes still have the possibility (or risk) of being chosen to become a supernode.\n\nOverlay networks can be classified to be either structured or unstructured. An unstructured overlay network builds a random graph between the participating nodes and uses algorithms like random walk [29] or flooding to distribute queries through the network. If the flooding is not complete, there is no guarantee that an answer will be found. Unstructured overlay networks have good scalability properties because a node only needs to know a few other nodes in the network. For that reason, unstructured p2p networks are often used for file sharing [11] where an user wants to find a certain file, not all copies of it.\n\nStructured overlays assign keys to data items and have a mapping function that map a key onto a node in the overlay. Having such a mapping function makes it possible to have efficient lookups of the data as every node in the network knows where to forward requests. It also makes it possible for a node to insert certain data into the overlay, and for another node to be sure to retrieve it at a later time. There are of course times when a structured overlay can not return values previously inserted but that is an error state, not as in unstructured overlays in which it can occur because of incomplete flooding. To decrease the risk of failed lookups due to nodes leaving the network, many structured overlay networks use replication of data among multiple nodes.\n\nStructured overlays are mainly implemented DHTs. A DHT offers a storage service to its users in which data can be inserted and later retrieved from anywhere in the network. A DHT handles key-value pairs where the key often is a hash of the value. A key-value pair might be a name and a telephone number, and in that case the telephone number could be retrieved by using the name. Such a service is a useful building block when designing distributed systems. Examples of where DHTs are used are in Azureus [1] to find torrent files, as building blocks in systems supporting mobility [23], and in grid computing [3]. Four different proposals for DHTs were published in year 2001. They are Chord [24], Pastry [21], Tapestry [28], and CAN [16]. Their algorithmic background is consistent hashing [13], which has the property that when adding or removing bins in a hash table, a limited amount of keys need to be moved between bins. If the number of bins in an ordinary hash table is changed, a majority of the keys needs to change bin. The hash function is expected to distribute keys evenly over the key space. Consistent hashing was initially used to do load balancing among web servers but it is also a good way to partition the key space.\n\nChord, Pastry, Tapestry, and Bamboo [18] organize keys and nodes in a circular key space using SHA-1 [7] as the hash function. CAN uses a more complex key space, a n-dimensional Cartesian coordinate space on a multitorus. The 2-dimensional CAN key space is presented in figure 1.5.\n\nThe keys in a DHT are flat identifiers, meaning that they do not hold any hierarchical information. This is in contrast to an IPv4 address that is tightly coupled to a physical location in the Internet, where the location can be derived from the hierarchy of the address. Because of this difference, DHTs are useful building blocks in systems that want to differentiate between location and user identity. Such systems can enable transparent mobility because a user can keep her ID as she moves around in the physical network.\n\nThere are also approaches where keys are given an hierarchical meaning. For example, in a global geonotes system where geographical location is part of the key [25]. The DHT is then made aware of the hierarchy of to increase efficiency. Others have built data structures on top of DHTs [5], which goes well with the idea of having a DHT as a service [19,2] for nodes not participating in the overlay.\n\nThe separation between physical location and logical position in the overlay network makes DHTs robust against network disturbances. It is highly unlikely that two adjacent overlay nodes are located close to each other in the underlying physical network, thereby risking being affected by the same local network outages. However, if the underlying network gets partitioned a DHT might be divided into two different networks. Therefore most DHTs have network merging functionality.\n\nWhen data items are inserted into a DHT, they are given an identifier value in the network by the hash function. The hash function is applied to the data or meta-data and the result is called a key. The set of all possible keys is called the key space and is dependent on the hash function used. Because of the properties of hash functions, every data item has one unique place in the key space, and that place can be located by any member of the network. The most common hash function used in DHTs today is SHA-1 which distribute data among 2 160 bins. An one dimensional key space can be thought of as a ring into which values are put, which in the case of SHA-1 are all values between 0 and 2 160 -1 (figure 1.2). This thesis concentrates on ring-based DHTs.\n\nWhen values are inserted into the key space, you need to decide which node should be responsible for what values. To do that, nodes are also put into the key space, often by applying the same hash function to the port and IP address. We will use the term node ID to indicate the place where a node is put into the key space. In Chord for instance, a node is responsible for all values between its node ID and the node ID of the next node in key space while in some other DHTs the node with the numerically closest node ID to the key is responsible for attached data [21,18].\n\nThe purpose of a DHT is to offer a service for handling key-value pairs. To insert data is called a PUT and to later retrieve it is called a GET (figure 1.3). When a user inserts data to, or requests data from, a DHT, messages need to be forwarded between the DHT nodes for the PUT or GET to reach the responsible node. To find the responsible node is called a lookup which can be done in two different ways. The first way is that the initiating node asks a node for a pointer to the next suitable node in order to forward the lookup. When an answer arrives, the initiating node issues a new request to the node pointed to by the previous node, and the process continues until the right node is found. The second way is that an initiating node sends Figure 1.4: Different approaches for lookups in a DHT a request to another node to do a lookup, and if the receiving node is not the responsible node, the receiving node forwards the request through the DHT (figure 1.4). The first approach, iterative routing, gives the initiating node control of the lookup which circumvents the potential problem of a malicious node dropping requests. The second approach, recursive routing, on the other hand performs lookups with lower latency and also tackles the problem of non-transitive connectivity [10].\n\nAs DHTs should work in dynamic network environments, nodes need to communicate with each other in order to know what nodes are still members of the network. The nodes a certain node communicate with directly are called neighbors. The status of neighbor nodes is often tested using some kind of echo-reply communication. The interval with which a neighbor is contacted affects how fast network dynamics the system can handle. This is a relevant setting in our scenario when mobile users are expected to join and leave at a high rate.\n\nA design decision when creating a DHT is how a node should select neighbors. The crude approach is to have all nodes communicating with all other nodes in the DHT. Such an approach is feasible for small networks, and will give good lookup performance as all values can be reached with only one request, or with complexity O(1). However, such an approach becomes inefficient when the number of nodes participating in the network grows large, and it is therefore common to also use more advanced methods when selecting neighbors.\n\nIn DHTs that use a circular key space, it is common to let nodes keep track of certain number of nodes before and after their node ID. The set of such nodes are sometimes referred to as a leafset (figure 1.2\n\n). The leafset ensures that messages can be passed between any two nodes in a stable network, using other nodes. To only use a leafset causes a high lookup path length of O(n), where n is the number of participating nodes. Therefore, although sufficient to ensure correct lookups, it is not efficient at handling lookups in big networks. To increase performance, nodes can keep information about other nodes far away in key space. That information is often called a routing table (figure 1.2).\n\nThere are certain metrics commonly used when evaluating the efficiency of DHTs. An obvious metric is how timely the DHT is in terms of servicing requests, often called lookup latency. Lookup latency and successful lookups are two metrics with which to evaluate DHT services. However, to only consider lookups overlooks the internal processes causing delays and failures.\n\nTo see how efficiently an overlay uses network resources, you often measure the ratio between physical network hops and overlay hops, the overlay stretch.\n\nA high overlay stretch increases the risk of something going wrong during the lookup and can therefore lead to a decrease in successful lookups.\n\nThe price of providing an overlay service can also be evaluated. It mainly consists of the traffic that needs to be sent between nodes regardless of whether requests are served or not.\n\nWith overlay services becoming more widely deployed, the need to evaluate the performance of such systems have increased. Experimental evaluation tools include simulators, emulators, and testbeds, while theoretical methods involve statistics and formal methods. The theoretical methods are beyond the scope of this thesis, so only the experimental methods will be discussed here.\n\nThe most used method to evaluate overlay systems is by simulation. Simulations can be very useful, as you have complete control of the environment in which you evaluate an application. You often need to implement an application or a protocol specificly for a simulator to evaluate it. That might be good, as you can make simplifications that makes the model less complex. Such a simplified implementation might make it possible to simulate a bigger network or longer scenarios. Simulation does not typically run in real time, so it is also feasible to study quite long scenarios in a short time if the complexity is kept low.\n\nWhen a scenario is designed, there are a multitude of configurable parameters which need to be assigned. Parameters might control network topology, network dynamics, and timers in the evaluated application or protocol implementations. There are numerous tools for creating network topologies according to Internet models, but still, you always need to estimate the relevance of the created topologies for your experiments.\n\nThe simulation environment makes it possible to quickly change the topologies and even model configurations which are rare in real life. This also means that the evaluation results are directly dependent on the accuracy of the models, e.g. of the network topology and the application behavior. It can make it hard to draw general conclusions outside the the assumptions of the models and the parameters used.\n\nThe most common network simulator within the academic research community is NS-2 [9], which is an event-driven packet level simulator. By simulating every packets path through a network topology, links and queues can be simulated. Such a detailed network model is needed when evaluating transport protocols. However, such high detail in simulations is computationally expensive which makes it time consuming to evaluate large scale network configurations.\n\nAnother approach to simulate big networks is to have a network model only modeling delays. It is fairly simple to implement an event driven simulator with such a network model, so many researchers implement their own. Such simulators are often used to verify functionality rather than to evaluate performance. This is because they can not model bandwidth or packet loss caused by full network queues. The praxis that developers and researchers of overlay systems implement their own simulator unfortunately makes it hard to compare different results, implementations, and algorithms.\n\nOur choice to implement Bamboo in NS-2 was based on the need to set link parameters to model wireless access links. If we only wanted to model high network dynamics caused by mobile users a simpler simulator could have been used.\n\nIn emulation, parts of the real system and models are combined. The modeled part is used for different reasons. It could replace a complicated part which is difficult to provide, such as a large network. It could be used to provide a repeatable environment, such as a radio network which otherwise has an unpredictable component. In both examples, parameters for the model could be systematically changed during an experiment. With an network emulator we mean that the actual application is used and that the emulator provides the same interfaces as the real network. Still, a designer of a network emulator needs to design a communication scenario for the emulator.\n\nEmulation has been used a lot within wireless network research due to the ability to model mobility as connectivity changes [27]. For research on systems in wired network there are hardware network emulators available, as well as publicly available emulation testbeds like Emulab [26]. Emulab supports both wireless and wired experiments as well as mobility.\n\nRecently there has been work done that aim at using measured network phenomena in an emulated network, where the properties of the emulated network is affected by measurements from a real network [20].\n\nTo evaluate how a networked application would behave in a real deployment, real experiments is the most valuable method as it is common that an application behaves unexpectedly when it is exposed to real network dynamics. Real experiments are often hard to perform due to coordination problems, time synchronization, and hardware. Unlike in simulation and some emulation, you do not have a global clock to time your measurements with. There are tools available to support real life experiments that typically helps in choreographing node behavior, synchronizing experiment start, and later gather logfiles and other data [15].\n\nA fundamental property of real experiments is the varying environment between experiments. It might be that the background noise varies over time when doing wireless measurements, or cross traffic in the Internet when doing overlay experiments. On one hand such variations reflect what a real deployed system would have to cope with, so results gathered under such circumstances are highly relevant. On the other hand, if the variations are high, they can cause results to be hard to compare or reproduce.\n\nIn overlay network research, where you typically want to evaluate systems with many nodes spread over the whole Internet, real experiments are costly. Some companies have testbeds that can be used to perform experiments [5] but the most commonly used testbed is PlanetLab [6]. PlanetLab is a cooperation between mainly research institutions that provides a global testbed. The testbed currently consists of 777 machines at 378 physical locations around the world. The users of the testbed can get shell accounts on the machines. To have access to that many machines distributed over the world enables many interesting experiments, but it also creates new problems; for example clock skew, machine crashing, and other experiments competing for resources.\n\nRunning experiments on PlanetLab involves problems that you do not have in simulation or emulation. First you need to distribute software and possibly different configuration files to all the participating nodes, and unlike in simulation, it is a time consuming task. When the nodes have the right software installed the experiment needs to start on all nodes synchronized, which is pretty hard to achieve on PlanetLab. While the experiment is running, it is nice to be able to monitor how it proceeds, but it is often hard to get a good picture of the experiment by looking at logfiles at different nodes.\n\nCurrently there is no way offered by the PlanetLab testbed to control network specifics like intermittent connectivity or limited bandwidth. Being able to control such properties of the nodes in a experiment can be valuable when evaluation overlay systems that should function in other network environments than the fixed Internet.\n\nTo evaluate a DHT running on the Internet with low bandwidth nodes participating we designed Dtour. It is a lightweight connectivity emulation library which allows us to emulate weak access links on PlanetLab.\n\nThis thesis consists of the following papers. Paper A: A bandwidth study of a DHT in a heterogeneous environment Olof Rensfelt and Lars-Åke Larzon Uppsala University Technical report no: 2007-017 This technical report documents the work of implementing a version of the Bamboo DHT to NS-2. It describes how NS-2 was modified to better handle node churn, as well as how the heterogeneous scenario was modeled. It also presents simulation results indicating that mobile phones might actually work as full members of a DHT. The choice to use NS-2 might in retrospect be questioned, as it turned out it was extremely time consuming to reimplement the system. However, the experience about what scenarios to study and how to model them have showed themselves to be very valuable when doing PlanetLab experiments. Paper B: Vendetta -A Tool for Flexible Monitoring and Management of Distributed Testbeds Olof Rensfelt, Lars-Åke Larzon and Sven Westergren In the proceeding of TridentCom 2007, May , Orlando\n\nIn this paper, the Vendetta monitoring and management tool is described. Vendetta is a tool both used to interactively control experiments as well as visualize events that occur in an overlay network. The system consists of two parts -first a small piece of software running on every node in a testbed called the client and second, a monitor where an experiment can be set up, monitored, and controlled. A main contribution is the framework to handle logfile parsing during experiments, which in combination with the generic event queue allows a great amount of flexibility when controlling experiments.\n\nPaper C: Evaluating a DHT in a heterogeneous environment Olof Rensfelt, Sven Westergren and Lars-Åke Larzon Submitted for publication Using both the NS-2 implementation of a DHT as well as real experiments on PlanetLab, the impact of weak nodes to a DHT was evaluated. To model heterogeneity on PlanetLab, a lightweight emulation library called Dtour was designed. Dtour decides whether packet should be forwarded or dropped. It is implemented by catching system calls like send() and sendto() and a filter mechanism. The packets are sent through a token bucket filter to limit bandwidth. The results show that there is a good match between simulation results and PlanetLab measurements and that a DHT like Bamboo can actually cope quite well with bandwidth limited nodes and high churn rates. However, the DHT enters an oscillating state which needs to be addressed. The oscillation does not occur when nodes without bandwidth limitation churn in the same pattern as bandwidth limited nodes in the oscillating experiment.\n\nNeither does it occur when bandwidth limited nodes participate in the network without churning so the combination of churn and bandwidth limitations seems problematic.\n\nSince PlanetLab allowed us to evaluate long scenarios compared to simulations, we were able to observe strange behavior that did not appear in simulation.\n\nThe experiments running on PlanetLab matches the simulations from pa-perA. The only difference is that there is no extra delay on weak nodes access links on PlanetLab. Nodes are either weak or strong where weak nodes models mobile terminals and the strong nodes models nodes with broadband connection. The weak nodes are bandwidth limited according to measurements from an commercial available 3G service where the uplink was measured to 384 kb/s and the downlink to 64 kb/s [14]. While the strong nodes stay connected to the network for the duration of the experiment, weak nodes join and leave with short intervals. The network size is kept fixed by letting a new node join as soon as a node leaves. When nodes join and leave are modeled by a Poisson process which creates exponentially distributed connection times with the mean of 3 minutes.\n\nIn figure 1.6 we present performance over time for the DHT. All experiments with weak nodes show significant variations in mean lookup latency over the time of the experiment. In figure 1.6(a) it is clear that the latency slowly oscillate with an about five hours period.  Because the addition of bandwidth limitations lead to the oscillation, we expected to see an increase in dropped packets during the latency peaks. However, when we studied the drop rate we found it rather decreases indicating that nodes decrease their sending rates. From figure 1.6(c), we can also see that the mean bandwidth used is below 4 kB/s for combined received and sent data, which is about half the upstream limit of 64 kb/s.\n\nThe impact of churn on a DHT can be substantial [18]. It does not only cause failed lookups due to nodes leaving while forwarding a lookup, but it also causes routing tables to be non optimal. Non optimal routing tables will cause higher lookup latencies in the DHT. Churn can also create an increase in management traffic when newly joined nodes need to synchronize neighbor information and stored data. Such traffic might congest links between nodes.\n\nTo investigate the impact of churn, we ran experiments without any bandwidth limitation, but let 30% of the nodes churn like weak nodes. Except for the lack of bandwidth limitation, the setup was identical to previous experiments. In the measurements from this experiment, we observed some variations in latency in the first few hours but the latency stabilized.\n\nThe results from the experiment indicate that the churn is not solely to blame. We also ran an experiment with 30% weak nodes that did not churn and obtained similar results. This experiment performed at a stable level throughout the entire experiment, without any significant variations in latency or success ratio. This result also reduces the risk that a programming error in for example Dtour is causing the oscillations.\n\nSince neither churn nor bandwidth limitation on its own caused the network to crash we are lead to believe that the cause must be the combination of the two. Our current hypothesis is that the congestion mechanism implemented on top of UDP is causing the oscillation. The main reason for this suspicion is the decrease in total traffic sent and received during the latency peaks seen in figure 1.6(c). We find it interesting that added dynamics with a 3 minute mean interval can cause dynamics on +5 hour time scales. Because the congestion mechanism reacts to dropped packets, we would like to find out what packets are dropped when during the experiment. Unfortunately it seems hard to make Dtour aware of what packets are dropped because it is not a well defined protocol but serialized objects that are sent between Java machines. We do not currently know how to solve that problem.\n\nAs we have worked on evaluating a DHT in heterogeneous networks we have found a need to improve the available tools since both simulation and testbeds have limitations. We have extended existing tools during our work, both by modifying NS-2 and implementing an emulation library which can be used on PlanetLab.\n\nOur results both from simulation and PlanetLab experiments indicate that a DHT could work with a high percentage of bandwidth limited nodes. Even if the time they are attached to the network is short. The performance obviously suffers, but the system is able to satisfy requests even under extreme conditions.\n\nOn a longer time scale there are two main directions that seem interesting to pursue. First it would be very interesting to see if Dtour and Vendetta could be used to evaluate other networks than overlay networks. It seems likely that Vendetta could also be very useful for managing sensor network testbeds. A tool like Dtour might also be useful when evaluating DTN solutions. Extending Dtour with functionality to delay traffic would also enable other interesting uses.\n\nPaper A: A bandwidth study of a DHT in a heterogeneous environment\n\nIn the design of distributed applications, there has been a strong trend during the last decade to use the Internet mainly for connectivity and build an overlay network with its own node identifier space on top of IP. This effectively deals with problems that could otherwise occur due to dynamics in IP address allocations. By not using the IP address as an identifier for the service itself, the service can continue to function as long as Internet connectivity is maintained even if IP addresses change over time.\n\nA common approach to introduce a new identifier space is to use distributed hash tables (DHTs). A DHT is a distributed data structure that functions much like an ordinary hash table, except that the key space is distributed over several nodes rather than kept together at one single node. When querying a value in a DHT, the query is routed to the node that maintains the corresponding part of the key space. Nodes continually exchange data to keep track of how the responsibility is divided among them. Most DHTs include some degree of replication to deal with nodes that may disappear without prior notice.\n\nMost existing evaluations of DHTs are done using simulators written for that specific purpose to enable proper simulation of the data structure itself with a focus on how queries are carried out. Modeling of the communication between nodes that collaborate in a DHT tend to be simplistic at bestsometimes it is assumed that all messages sent are instantaneously received by the receiver. While this assumption can be argued to be a reasonable simplification in an environment with fast computers communicating over a fixed Internet connection with high bandwidth, it does not hold for more heterogeneous network environments.\n\nIn this report, we document the NS-2 implementation of a Bamboo [11] like DHT and present simulation results on how it behaves under such conditions. The main contribution is that the implementation allows a more detailed networking model, compared to simpler simulators.\n\nA recent trend is to use overlays to deploy functionality that the existing network infrastructure does not provide. An overlay network is a logical network built on top of the existing network with its own addressing scheme using the Internet as a link layer. Building an overlay network makes it possible to deploy functionality not available in the current Internet architecture, or to create services that are provided by the users of the service. A user pro-vided service is for instance BitTorrent where a group of users cooperate to provide efficient mass-distribution of large files. In some sense the users pay for the service by participating in the overlay. The common use of overlays is as the communication module of an application, sometimes referred to as BYOI (Bring Your Own Infrastructure). BYOI has proved very successful in file-sharing applications and in some sense in VoIP with Skype. Parts of the research community are however suggesting to have overlays as services for multiple applications to share, provided by companies [1]. The benefit would be that the price of management overhead can be shared among more participants. Also, such a service could be assumed to be more stable than a service where only the users collaborate to provide the overlay. Another benefit, or draw back, depending on conviction, is that a payment system needs to be added to the system because users no longer pay for the service by participating in it.\n\nA common service provided by overlay networks is a lookup service handling flat identifiers with a ordinary query-response semantic. Such a service is often implemented using DHTs (Distributed Hash Tables) [9,16,13,18,11] . A DHT allows you to insert values connected to keys much like ordinary hash tables. A key is typically a hash of the value stored or alternatively a hash of some meta data of the value. When the key is inserted it is routed through the overlay network until it reaches the node that is responsible for storing the key. The key can later be used to retrieve the value from the DHT.\n\nThe flat address structure often used in overlays, and especially DHTs, is appealing for cases when you want addressing differentiated from your physical location in the network. Such a differentiation can for instance be a building block in systems supporting mobile nodes [15] where identifiers should remain the same regardless of the location of the node.\n\nDespite the flat address space structure on the DHT level, it is still possible to add some form of hierarchy in the application. E.g in [17] we embed the geographic location of information in the key itself. Other have also built hierarchy on top of a DHT [3].\n\nBamboo is a DHT implementation first presented in [11]. It is referred to as a third generation DHT, where lessons learned from previous systems have Figure 2.1: The routing table. The white nodes are the middle white node's leafset if the leafset size is configured according to l=3. The dotted arcs show the routing table entries been incorporated in the design. The Bamboo implementation has proved stable when used in OpenDHT [12] ,where it serves a system with good uptime.\n\nTo continue the earlier studies [2] of how an overlay network behaves in a heterogeneous environment, we chose to implement a DHT in NS-2 [5]. We believe that a lot of the problems seen in [2] is addressed with Bamboo. For example, the problems encountered with Pastry in heterogeneous networks were mainly caused by management traffic congesting nodes, and a new approach to management traffic were presented in [11].\n\nBamboo uses the routing logic of Pastry but has more developed mechanisms for maintaining the network structure in a dynamic environment. A big part of network dynamics is that nodes leave and new nodes join the network, which is called churn. Bamboo maintains two sets of neighbor information in each node (figure 2.1). The leafset consists of successors and predecessors that are the numerically closest in key space. When routing a query, it is forwarded to a node which has the key in its leafset. Using the leafset is enough to ensure correct lookups. However if only the leafset was used when doing lookups, a lookup complexity of log(n) is all that could be achieved. To improve the lookup complexity, a routing table is used. The routing table is populated with nodes that share a common prefix, and routing table lookups are ordinary longest prefix matching.\n\nThe major difference between Pastry and Bamboo is how they handle management traffic. In Pastry, management is initiated when a network change is detected, while in Bamboo all management are periodic regardless of network status. The approach to use periodic updates has been showed to be beneficial during churn [11] since it does not cause management traffic bursts during congestion. Such traffic bursts can further increase network disturbances.\n\nThe Bamboo system has been evaluated both in simulation and as a deployed system on PlanetLab [4]. However the evaluations have not taken bandwidth or other node specifics into account, only network delay. This is not a major problem if you want to evaluate scalability and lookup delays in noncongested networks. The nodes in PlanetLab are typically very strong machines on academic or other types of very stable, high bandwidth networks, and therefor they are not suited for studying the scenario we are investigating.\n\nIn order for a DHT to be able to serve requests and maintain a consistent network view among its nodes, it needs to perform network maintenance. This maintenance consists of network messages sent between nodes. In this section we will describe the different types of maintenance performed by Bamboo. Periodic management traffic occurs in all layers of the Bamboo system (figure 2.2). In the data transfer layer, ping messages are used to measure RTTs (Round Trip Times) to peers. Routing table and leafset information are exchanged and databases are synchronized. We have used [11,10] as design documents as well as the Java source code from [6].\n\nThe most basic management traffic type is to make sure that you can still reach your one-hop neighbors in the overlay. This is normally done with an echo/reply type of communication. In Pastry it is called probes, and other systems have the same function with different names. The messages sent are not ICMP pings but UDP echo and reply packets. The major design decisions regarding neighbor pings are the interval which is used to ping and the number of unanswered pings that should cause a node to treat a neighbor as unreachable or, as in Bamboo, as possibly down. In Bamboo the neighbor pings are also used to maintain a RTT estimate used for retransmission timeout calculations.\n\nThe reason why UDP is the preferred transport protocol in Bamboo is that the overhead of connection oriented communication does not justify the benefits of reliable transfer. A DHT also has a non symmetric nature regarding neighbor knowledge between nodes, meaning that the fact that node A has node B in its neighbor set does not necessarily mean that node B's neighbor set include node A. Because of this asymmetry the number of nodes that know a certain node will increase with the network size. If TCP is used as the transport protocol, the state that a node needs to keep increases significantly as TCP needs both the receiving and the sending nodes to keep state information. A DHT could benefit from using a transport protocol with properties like DCCP [7] as mentioned in [11]. DCCP offers a UDP-like, nonreliable datagram transfer with congestion control.\n\nChanges in node leafsets are propagated using an epidemic approach. Every node periodically chooses a random node from its leafset and performs a leafset push followed by a leafset pull in response. Both messages involve sending the complete leafset to the synchronizing node where the information is incorporated. It is important to both push and pull leafsets. Otherwise there might arise situations where nodes are missed in the leafsets of its neighbors [10].\n\nWhen a node has another node in its routing table, those two nodes per definition share one level. The local routing table updates are used to exchange the node information in that level. If a node gets information about other nodes that fits into the routing table it probes the nodes to test reachability and to get a RTT estimate. If a node is reachable and fits into an empty field in the routing table, it gets added. If the matching routing table entry is occupied, the node with the lowest latency is chosen. Other optimization schemes could be considered, such as optimizing for uptime, but optimizing for latency is the most common approach used. Having an optimized routing table does not influence lookup correctness, only lookup latency.\n\nLocal routing table updates can only improve routing table levels that are not empty. To improve that, you need to exchange routing table information with nodes that you do not yet know of. To find such nodes the routing functionality of Bamboo is used. To optimize a certain routing table entry, a lookup is made for a key which shares prefix with that entry. If a suitable node exists in the network the request will be routed to it, and that nodes is a candidate for the routing entry. Unlike with local updates, global updates can be used to optimize a specific routing table entry.\n\nWhen data is stored in the DHT using the PUT command, the data is routed through the DHT to the node primarily responsible for storing the data. When the responsible node gets the data, it caches it within its leafset at 'desired replicas' neighbors in each direction. The caching does not occur immediately, but is performed by the periodic replication functionality described below. The value 'desired replicas' is a configure parameter, and with the default settings there are 7 copies of the data within the system. When nodes disappear or joins, the subset of nodes that should store a certain value changes. Therefore there is a need for a mechanism to try to restore the distributed storage to the wanted state. The default setting of 'desired replicas', and the resulting 7 copies of each data units within the system, causes demands for storage space. If all nodes have equal amounts of keys to store, every node needs to store seven times that amount.\n\nThe first maintenance operation made is that a node periodically picks a random node in its leafset and synchronizes the stored keys with it. A synchronization operation starts with a node picking a node to synchronize with and requests a synchronization. The other node calculates the set among its stored keys that it believes should also be stored at the initiating node and send those keys and the hash values of the data. The other node receives the keys and hash values, and matches them to what it has stored. If a certain data unit received is not already stored it requests that data unit from the initiating node.\n\nThe second maintenance operation performed by the data storage layer is to move values that are not longer within a nodes storage range. If a node has such a value stored, it performs a new PUT to the place it should be stored before deleting it.\n\nWe have implemented a DHT in NS-2 [5] and, in what we believe to be the relevant properties, made it as similar to Bamboo as we could. However, since we did not run the Java code in simulation, differences might exist that we are not fully aware of. We will state the known differences when describing the different parts of the system. During the implementation work we have used the technical report [14] as a reference as well as the source code, and later the doctoral thesis [10] when it became available. In the following text we will refer to our implementation as Bamboo-NS2, and the original implementation as Bamboo.\n\nThe NS2 implementation consists of multiple modules that are constructed to fit the design of NS2, rather than the design of Bamboo (figure 2.2). There are however many similarities between which modules Bamboo and Bamboo-NS2 are divided into.\n\nTo be able to simulate big networks, we needed to make some simplifications. One simulation specific method is that we have the possibility to build the overlay network before the actual simulation starts. We will refer to this as building the network offline. In section 2.5 we will further discuss how this influences the evaluation.\n\nAs previously mentioned we have not implemented storage of real data in order to save memory, and instead of a faked hash value we use a globally unique id on every data item that exists in the DHT. Since we have control of all data that is inserted into the DHT, we believe this to be a valid approach.\n\nWhen we started to simulate churn, we ran into some problems with memory leaks trying to free NS-2 objects. This lead us to reuse the same NS-agents with multiple overlay nodes. First we tried to have multiple NS-nodes for each overlay node, so that when an overlay node went down and a 'new' overlay node came up, it came up on a different NS-node. The reason that we did not simply use the same NS-node for the new overlay node is because of the node information about the old node that is still in the system. This would cause a new node to receive traffic meant for an old node which would take up link bandwidth. We call this kind of traffic 'stale traffic'. We did not want to filter out traffic to no longer active nodes at the sending node, because in a real life deployment there is no way of knowing whether a node is active or not. The approach with multiple NS-nodes meant that we needed to simulate much bigger networks since many more physical nodes than overlay nodes where needed. Even when we used three physical nodes per overlay node stale traffic still turned up at newly joined nodes. Therefore we needed to find an other method of getting rid of stale traffic.\n\nThe second method involved giving every overlay node another globally unique id (GID), apart from its overlay address, and introducing a directly indexed lookup table with connection status. Then we modified the NS2 routing function to compare next hop IP from the routing logic to the end destination IP of the packet, and if they are equal it makes a status lookup to see if the destination overlay node is active. If it is not active the packet is simply dropped after it has been logged as stale traffic, and will therefore not stress the last hop link of a new node.\n\nThe packet handler at a Bamboo-NS2 node consists of a list of known neighbors. Bamboo implements reliable transfer on top of UDP, using acknowledgments which are also used for RTT measurements. If traffic is not flowing between nodes, periodic probes are sent to keep the estimated RTT accurate. In Bamboo-NS2 we use the NS-2 class agent, which we connect between nodes. Agents are closest matched by UDP sockets in Bamboo. To keep the memory usage low, we connect agents dynamically when needed. We encountered problems when we tried to free memory after the agents were not needed anymore. A workaround was to implement an agent pool, which we could request agents from in order to reuse them. An agent pair is only used to send data one way, because there where implementation benefits from having all traffic to a node go through one agent. We call the sender-side agents bamboo send agent, because they are of a different class compared to the receiving side type described in 2.4.4.\n\nWe did not use cumulative acknowledgments since we did not want to keep state at the receiver for every node that communicates with us. We do however need to keep a bamboo send agent for each node we communicate with, so the benefit of not using accumulative acknowledgments is limited. In a real deployment, the approach would be more beneficial.\n\nThe Bamboo-NS2 router consists of three modules; The routing table, the leafset, and the routing logic. The routing table consists of information about nodes spread over the key space, as well as functions to maintain and lookup node information. When we use the term node information, we refer to a structure which apart from a key value also consists of information of the network connection point of the node.\n\nThe leafset consists of ordered node information about the numerically closest nodes in key space which are the white nodes in figure 2.1, and functions to insert and remove nodes from the list. As previously mentioned, the routing table works like in Pastry. The routing table and leafset are used by the routing logic to lookup the next hop node when a key is looked up. When a routing request of a key is made to the routing logic, it first checks whether that key falls within the leafset. If the key is within the leafset, the numerically closest node is found, and the nodes information is returned as the next hop. If the looked up key is not within the leafset, a request to the routing table is made, which returns the closest node outside the leafset. If no such node exists, the next hop node is the numerically closest node of the two leafset nodes that are furthers away, and then the information about the closest node is return by the routing logic.\n\nThe Bamboo-NS2 Agent is both the listening agent in NS-2 as well as the interface to the TCL scripts used to run simulations. It is the connection details for the listening agent which is spread through the network for other nodes to connect to.\n\nFrom the TCL script that defines the simulation, the behavior of the Bamboo-NS2 node can be controlled. You can set the word and key length, make PUTs and GETS, connect and disconnect etc. It is in the listening agents recv() function that all incoming traffic to a node enters. If a new packet is an acknowledgment, the packet handler is called to remove the acknowledged packet from its buffer, as well as to calculate a RTT estimate. If the packet is not an acknowledgment the packet handler acknowledges the packet and checks whether it is a new packet or not. If it is a old packet or a PING the only action taken is the acknowledgment. If it is new packet, it is sent to the router to calculate the next hop and generate a new packet to send. If the next hop returned by the router is not null and not the node itself, the agent sends the new packet to the next hop node with the help of the packet handler module.\n\nWhen a Bamboo-NS2 node is connected to a NS-2 network node, and it has joined the overlay network using the join command to the agent, PUTs and GETs can be issued to the agent from the TCL script. A PUT takes a key, an id, and the data size as arguments. The key is where the value is stored, the id is instead of an hash of the data, and the size is how big the data is. No actual data is put into the system but the size field is used to set the correct size of network packets during simulation, and the id is used to distinguish between different values. The GET command takes the key value requested and records the time. If a GET matches multiple values in the DHT only one is returned. This is not how Bamboo behaves; Bamboo would return values together with a pointer. The pointer can be used to retrieve the remaining values that matches the GET with repetitive GETs.\n\nTo support different measurements of the system, two different GET behaviors are implemented. The first is the one resembling Bamboo with keys stored and cached, as is later described in the section on the data storing. The second is a special GET where you lookup exact nodes in the network to evaluate the pure routing functionality of the system without the noise of key management.\n\nThe data storing module in our system does not implement all the functionality present in Bamboo. The synchronization between nodes is initialized by a node when it sends a list of its keys to another node. The receiving node builds a list of the keys in the received message it does not have, and sends that list to request those keys. Keys in the systems have a TTL, but that is a function we do not use during our tests. A good study of the storage problem is [10].\n\nIn Bamboo an improved synchronization method is used. It is based on Merkle trees [8] and it involves building a tree of hash values over the stored key values. The best case for this method is when the nodes are completely synchronized, which will result in the need to exchange one hash value to determine that. According to [10] the worst case of the Merkle tree approach is only O(n), were n is then number of keys. However, there is no evaluation of the time aspect of synchronization.\n\nBamboo uses a concept of possibly down nodes. That is nodes that have not responded to 4 succeeding pings. The set of possibly down nodes are still periodically pinged with a greater period and are considered unreachable. If a node in the set answers to ping it becomes a known neighbor again. A big advantage of this is that it can rejoin a partitioned overlay network. If for instance the connection between two continents is cut off, two different overlay networks will be formed and their knowledge of each other will fade away with 4 succeeding pings. With the addition of possibly down nodes, that you keep trying to reach for a long time, the partition of the network can be healed. We have not implemented support for treating nodes as possibly down in our implementation, since we have not been interested in studying the influence of the intermediate network on the overlay, only to study the influence of connection technologies.\n\nOur implementation does not handle multiple PUTs to the same key in the same way as Bamboo does. However, we believe that for the sake of evaluating the performance in heterogeneous environments, the benefit from such a complete implementation is limited, compared to the need for it in a deployed system.\n\nTo evaluate the system in heterogeneous environments, we have set up scenarios in NS-2. In this section we will first describe the simulation setup, then describe our evaluations of the impact of different network variables and present the results of each evaluation.\n\nThe physical network layout used in our simulations is modeled with the nodes in clusters connected with very high bandwidth links with long delays (figure 2.3). The reason not to use a more advanced topology created by a topology generator is that we wanted to keep the variables influencing the simulation as static as possible. By using the same delay on the links between the clusters, we have had an easier task to realize their influence on the total delay on for example lookups. We used very high bandwidth links between clusters so that they would not introduce packet loss, but only delay. Overlay nodes are not connected to the cluster nodes, only to the NS-nodes, with links into the cluster nodes. The node characteristics are set at the last hop link into the cluster. Strong link Weak link Link between clusters downlink 10 Mb/s 384 Kb/s 100 Gb/s uplink 10 Mb/s 64 Kb/s 100 Gb/s delay 5 ms 115 ms 50 ms\n\nTable 2.1: Physical network specifications\n\nThe overlay is built \"offline\" in order to have a fixed, well known starting state. During the building of the network every node has knowledge about every other node. This will create a network where a node has as many nodes as possible in its routing table. The routing table is however not optimized for proximity since RTT measurements are not done before the simulation starts. When the simulation starts, a node pings all the nodes it has in its routing table and leafset. This causes an initial burst of traffic that needs to be taken into account. We decided to not collect data until the system was stable.\n\nThe fact that we build the network offline indicates that we start the simulation in an unrealistic state. The main factor of initial instability is that nodes need to ping their neighbors in order to calculate RTTs. To study how long it takes for the system to stabilize we periodically performed GETs on a stable system and then plotted a moving average of the lookup times. The stabilization time is of course dependent on management traffic settings, as well as overlay network behavior, but we decided to use the settings from [14] since they were tweaked for a system under churn(table 2.2). From figure 2.4 we decided that that the initial 80 seconds of the simulation should be considered start up time. We also looked at the simulation runs with churn and we concluded that 80 seconds still seemed to catch the initial turbulence (figure 2.5).\n\nMaking measurements of a DHT is not as straightforward as it might first seem. The first problem is that the measurement traffic influences the systems performance by adding extra load. It is on the other hand not a realistic scenario to have a DHT without lookups that influence performance. There are two directions we could have taken with the lookups. One way is to try to model store and lookup traffic realistically, for instance by using a stochas- tic process that causes bursts in network utilization. However, the bursts would make analysis of lookup delays harder as it would be hard to compare two different samples in time. It would be hard because of the difference in measurement environment. We choose to use a periodic probing scheme to simplify the analysis of the data.\n\nIn the tests performed on Bamboo in [11,10], a majority procedure was used to decide if a lookup was successful or not. 10 nodes requested the same key, and if they received different answers the minority was considered to be wrong. Since we have global knowledge in simulation we have used single lookups. Another problem with deciding on success or failure is whether to use a timeout. If a timeout is used you will remove information about how lookup times are distributed and move it to the failure statistics. With a very long timeout you will get a high success ratio but a higher mean lookup time. In [12] 60 minutes is used, but we have set a timeout of 60 seconds, since we do not believe that a lookup that exceeds a minute can be considered a success.\n\nThe next decision to make about the lookup is whether you should lookup nodes or keys. If you make lookups aiming at nodes, you do not need to introduce the extra complexity of a data storing system. On the other hand you need to make sure that the requested node is available for lookups during the right time. If you do not, you might decide that a lookup has failed when there is no way of success. Having to take transit lookups into account when\n\ntechnical report OpenDHT Neighbor ping period 4 20 Leafset maintenance 5 10 Local routing table maintenance 5 10 Global routing table maintenance 10 20 Data storing maintenance 10 1 Table 2.2: Management traffic periods in seconds simulating churn complicates matters. Therefore we use the data storing to allow us to simulate churn more freely and we make lookups for keys rather than nodes.\n\nSimulations were made with management traffic according to [11], where churn was targeted, as well as with settings matching the ones used in the deployed DHT service OpenDHT [12]. During simulation, 10 of the strong nodes were used as bootstrap nodes. The first scenario used is that nodes are distributed over 3 clusters as seen in figure 2.3. The links between the clusters are modeled as having extreme high bandwidth but with a intercontinental delay. The nodes are connected to one of the clusters with a link that either is a 10Mb/s, low delay link (strong node) or a link with specifications according to measurements made of 3G connectivity (weak node). The weak nodes has a down link bandwidth of 384 Kb/s, an uplink bandwidth of 64 Kb/s and a link delay of 110 ms. Weak nodes are uniformly distributed over the network.\n\nWith the choice of NS-2, we sacrificed the possibility to study large networks (more than approximately 500 nodes), but it does allow us to simulate link bandwidth and link queue drops.\n\nThere are multiple variables that influence the characteristics of the network. In a dynamic overlay, these variables will change over time, but in order to study how they influence network performance we have kept them fixed during the course of one simulation. We will present the simulation setup and result for each network variable.\n\n100 200 300 400 500 0 0.2 0.4 0.6 0.8 1 1.2 Nodes Delay [s] 0 0.3 (a) Lookup delay 100 200 300 400 500 0 0.5 1 1.5 2 2.5 3 nodes Mean lookup path length 0 0.3 (b) Mean lookup path length 100 200 300 400 500 0.95 0.96 0.97 0.98 0.99 1 1.01 nodes success ratio 0 0.3 (c) Success ratio\n\nThe network size is the number of participating nodes at a measured time. How the size of a DHT impacts performance is evaluated previously, both in simulation and on testbeds using emulation [13]. We only study how size influences the network up to 500 nodes. The reason for varying the size in our initial simulations is to justify our decision to use a fixed network size of 500 nodes when studying bandwidth usage. The lookup path length complexity of O(log(n)) ensures good scalability properties. The results from the simulations are presented in figure 2.6 where we use lookup delay, lookup path length, and lookup success ratio as measures of the systems performance.\n\nFrom figure 2.6(a) we can conclude that the added weak nodes, and the resulting churn, affects the lookup times much more than the size of the network. The lookup delay for networks with only strong nodes and no churn is only marginally affected by size, which might seem non-intuitive when figure 2.6(b) shows a increase in lookup path length. We believe it to be caused by the routing table being optimized for communication latency in combination with how we model the core network. When communication within a cluster is very cheap compared to between clusters, and when the routing tables are optimized for network proximity, an extra overlay hop might not increase the total lookup delay significantly. For instance in the simulation with 500 nodes, where the nodes are randomly distributed among the three clusters, every node should have more than three candidates for each top level routing table entry. With three candidates per top level entry, on average one of them should be in the same cluster and thus chosen when the routing table is optimized.\n\nWhen weak nodes are introduced a small increase in lookup delay can be seen (figure 2.6(a)) but when the size of the network reaches 300 nodes it levels out. We believe it to be caused by the same mechanism as in the case of a static network. The information of the weak nodes does not spread\n\n0 0.2 0.4 0.6 0.8 1 0 0.5 1 1.5 2 2.5 3 Ratio weak nodes Lookup delay [s] (a) Lookup delay 0 0.2 0.4 0.6 0.8 1 0 0.5 1 1.5 2 2.5 3 Ratio weak nodes Mean lookup path length (b) Mean lookup path length 0 0.2 0.4 0.6 0.8 1 0.95 0.96 0.97 0.98 0.99 1 Ratio weak nodes Success ratio (c) Success ratio 100 200 300 400 500 0 0.05 0.1 0.15 0.2 0.25 Network size Percentage stale traffic (a) Stale traffic vs. size 0 0.2 0.4 0.6 0.8 1 0 0.05 0.1 0.15 0.2 0.25 Ratio weak nodes Percentage stale traffic (b) Stale traffic vs. ratio of weak nodes through the network fast enough to make a big impact, and even when the information reaches other nodes it is unlikely that a weak node is the best candidate in a routing table . \n\nIn figure 2.6(b) we can see that having weak nodes in the network increases the mean lookup path length. Since the latency and bandwidth of links should not influence lookup path length, the difference is probably caused by the introduction of churn in the network. Churn causes routing tables to be non optimal, which should cause increased lookup path lengths.\n\nFinally we we can see from figure 2.6(c) that the success ratio of lookups is constant 100 % for a static network which is what should be expected in a noncongested network. We use a low request rate so the network is not congested during these experiments.\n\nA common assumption, both in simulation and in real world tests, is that all nodes are created equal. That assumption does not follow the trend of networks where the heterogeneity increases. We choose to make a simplification of network heterogeneity by introducing what we call weak and strong nodes. A weak node is modeled from a UMTS cellphone, as such phones are probably the first mobile devices that it makes sense to have as members in an overlay. The strong nodes are modeled from desktop computers with broadband connections. We use the term ratio to describe how many percent of the nodes that are weak.\n\nIn these simulations we keep network size fixed and vary the ratio of weak nodes. More weak nodes does not only lead to more weak links but also to a more dynamic network. A more dynamic network increases the risk of lookups being lost in transit. Failed lookups have two different reasons. First a lookup can be lost if a node leaves the network while the lookup is routed through it. Second a lookup fails if it reaches the destination node when that node has recently joined and the destination nodes data storage has not yet been synchronized. Bamboo has caching optimizations but we have not implemented them because we believe that they hide the true performance in an experimental evaluation. Nevertheless they make complete sense in a deployed system.\n\nAs we can see in figure 2.7(c) all lookups succeed when no weak nodes are present in the network. This is expected because it means that the network is static. It seems that the success rate has a close to linear relation to the ratio of weak nodes which is promising.\n\nRegarding lookup delays (figure 2.7(a)) there is a weak tendency of nonlinearity in the results, which we have also seen in other simulations. We believe that with a small amount of weak nodes in the network, the weak nodes are unlikely to end up in routing tables, but as the ratio increases more weak nodes starts to forward traffic.\n\nIn figure 2.7(c) we can see that even for 50 % weak nodes the succes ratio is well over 95 % which seems quite good, considered the introduced churn.\n\nA system that is distributed over the Internet will experience churn. The churn can be caused by many different things like network problems, node crashes or nodes that join and leave in a controlled fashion. We only simulate single nodes going up and down.\n\nWhenever a node leaves the network it leaves silently, meaning that all state is left in the network. Only having silent leaves is the worst case scenario, but it is also how Bamboo handles leaves. When nodes leaves silently the node information related to those nodes will continue to spread throughout the network for some time. It will however fade out when nodes that receive the information unsuccessfully tries to ping the dead node. The ping traffic to dead nodes, as well as neighbors that try to perform maintenance with dead nodes, cause what we call stale traffic within the network. We define stale traffic as traffic that is destined for a node that is no longer a member of the network. In figure 2.8(a) the percentage of stale traffic is plotted against the size of the network. The figure shows that the percentage of stale traffic is not increasing with the size of the network. There might have been an increase in stale traffic if nodes did not try to ping neighbors before adding them to leafsets and routing tables, but since they do, information about down nodes are not redistributed through the network.\n\nWe have simulated networks with churn and different ratios of weak nodes. The size of the network in the simulations presented here is at most 500 nodes, which is close to the upper limit of what it is feasible to simulate with the methods and tools we have chosen. Even if larger networks would be interesting to study we believe that 500 nodes is enough to study the performance of the system, since an initial deployment of a DHT might for instance be on PlanetLab with some 200 nodes.\n\nWeak nodes come and go in the system while strong nodes are static. How long a weak node is connected is determined by a Poisson process. The interarrival times model a mean online period of three minutes. Three minutes is a very short period of time but as we model cell phones used by mobile users, we believe that it is unlikely with many weak nodes that are online for extended time periods.\n\nIn figure 2.9 we present a visualization of the results gathered during three simulation rounds with different ratios of weak nodes. Each column of plots presents information about one run. All negative values are weak nodes and all positive values are strong nodes. The top plot shows the mean bandwidth utilization of all nodes in the simulation. The nodes are sorted on the utilization, and a completely even distribution of used bandwidth would look like a horizontal line. By studying the columns, some relations can be seen. When all nodes are strong the distribution is almost even but with weak nodes that introduce churn the distribution becomes less even. We can see that there are two major clusters of weak nodes at the extremes of utilized bandwidth. From studying the uptime plot we can see that the nodes that use the least bandwidth are nodes that have an uptime less than the maximum uptime. This means that those nodes have joined during the simulation and we believe that the reason for them to have a smaller load is that the information about them has not yet spread through the system, which could be very beneficial for a heterogeneous system. Weak nodes are typically connected shorter periods and could then get a smaller workload. The other extreme are the weak nodes that have the highest bandwidth utilization and the uptime plot gives us the information that they have typically been online for a very short period of time. From the bottom plot we can make the observation that the ratio between received and sent bytes are very close to zero which indicates that these nodes have just gone online, sent initial probes but that they have not yet received much response.\n\nBecause of the nature of a Poisson process some very short uptimes will occur, but extremely short uptime of nodes is not very realistic. A node might join the network in order to make a request and then leave, but we believe it to be unlikely that a node will take the cost of sending probes without getting the benefit of the response. To minimize the effect of the very short lived nodes in our analysis we added the condition that a node must have an uptime greater than 5 seconds to be presented, and then plotted the same data as in figure 2.9 in figure 2.10.\n\nIn figure 2.10(e) we still see a cluster of nodes that does not seem to be influenced by the extra condition on uptime.\n\nWhen we realized that a DHT's management traffic could pose a problem in heterogeneous environments [2], we realized that to evaluate how big the problem was , we needed to be able to simulate bandwidth. The most common approach when simulating DHTs is to only simulate a static delay between nodes. Such a network model will not introduce packet drops, reordering or delay variations due to congestion in the network. To be able to model a more dynamic network, we needed a more expressive simulator. Our choice of NS-2 was based on the fact that it is a de facto standard within the community, and also that it has good supporting tools like topology generators etc.\n\nThe choice to simulate a DHT in NS-2 showed time consuming. Implementing the DHT functionality from scratch was needed to be able to make simplifications, and simplifications where needed to be able to study somewhat large networks. As memory is a constraint in simulations, we needed to implement some extra support for dynamic allocation of simulation resources. Our experience is that NS-2 is not a tool that fits simulations of large, dynamic, overlay networks well. For example, we have not been able to simulate more than 60 minutes of real time when simulating a 200 nodes network with 30 % weak nodes on a 2 GB Ram machine. Fortunately, that time has been enough for the networks to stabilize so we have been able to get data from stable networks. The way we set up the networks offline 2.5.2 also shortened stabilization times compared to real experiments.\n\nIn conclusion we think that more bandwidth studies of DHTs are needed as they are becoming more common as building blocks in distributed systems. The simulation approach can be valuable but a simulator which is less detailed compared to NS-2 and more complex than the delay only simulators would be a good tool for such analysis.\n\nThe last decade has shown an increasing need of distributed testbeds to support networking research. One reason for this is that experimental results from an actual deployment -if only in a limited testbed -can reveal phenomena that never will appear in a simulator. An important factor is the increased availability of platforms on which distributed testbeds can be deployed. Emulab [6] and Planet-lab [2] are two of the most well-used examples for both experiments and deployment of new services.\n\nWhen creating a distributed testbed, focus tend to be on actual functionality of the software to be deployed, with less efforts to provide a good user interface for monitoring and management features. Not uncommonly, code distribution, experiment synchronization, data gathering and similar tasks are done using customized scripts specific to the testbed. As functional as these script-based solutions can be, they are not always very user-friendly. Complete data collection for post-mortem analysis is also common in distributed testbeds. This approach typically provide researchers with large amounts of data on which data mining is done using third-party software.\n\nWe introduce Vendetta, which provides a flexible platform for developing user-friendly monitoring and management functionality in distributed testbeds. It is primarily aimed at medium-sized testbeds used in smaller projects with limited resources and incentives to develop testbed-specific software with similar functionality. Vendetta is designed to be flexible yet powerful by allowing users to create testbed-specific modules that work like plugins. The testbed-specific parts of Vendetta consists of two configuration files plus Java code to collect and parse data. It is also possible to construct a graphical canvas to illustrate testbed-specific details in a more intuitive way. The software includes highly customizable functionality for basic monitoring and management tasks that can be used to produce a powerful testbedspecific monitoring/management solution at a relatively low programming effort. This paper is organized as follows. In the following section, we present the design and features of Vendetta in more detail. After that, we will present a case where Vendetta is used for monitoring and management of a distributed hash table (DHT) testbed running on PlanetLab. After a discussion about the features and limitations with a platform like Vendetta, the paper is concluded with ongoing development efforts and planned future work.\n\nVendetta is not a testbed in itself. Instead, it is a tool that can be used to monitor and manage an existing testbed. In order to use Vendetta together with a distributed testbed, two requirements must be met. First, the testbed should consist of several nodes that can be accessed via the network. This should be put in contrast to testbeds where data is collected post-mortem which means that nodes must not be available at all times. Second, it must be possible to run a Java client on each node that needs to be monitored and/or managedfoot_0 .\n\nThe Vendetta software has two main components -the monitor and the node client. The monitor is an interactive program that presents a graphical user interface to the user, running on the users machine, while the node client is a small program that runs on all nodes in the testbed. Communication between the monitor and node clients can use either TCP or UDP as transport protocol, depending on how time-critical and error-tolerant the information to be sent is.\n\nThe node client runs at all nodes in the distributed testbed and can be started from the monitor with a SSH connection.It acts as a middle-man between the testbed components running at the node and the monitor. The applications are started as processes and the process output is parsed by the node client. A benefit of this is that applications can be evaluated even if they are closed source, but in that case, the resolution of the monitoring is limited to the amount of log information the application outputs. The node client collects log data from the testbed and stores it in a local log. Requested log data are sent to the monitor for presentation. It is possible for the monitor to instruct the node client to filter its log data before sending it to the monitor. However, the node client still stores all log events locally so that it can be used to do post-mortem analysis of the networks behavior. To support management tasks, the node client can also receive messages from the monitor, for example messages telling it to start or stop the monitored application.\n\nIt is important to note that the node client does not make any assumptions about the testbed that it serves. Instead, it act as a kind of messagepassing middle-man between the nodes in the testbed and the monitor. A benefit of having a middle-man software is that if the monitored application dies during an experiment, the node client can restart it and notify the the monitor. How data is collected, filtered and communicated is defined in a configuration file read by the node client when it starts up. By specifying another configuration file, the node client can be reconfigured to interact with another type of testbed. In section 3.3, we present an example of what a configuration file for the node client can look like in a specific testbed.\n\nThe monitor is the program that interacts with the testbed and present information to the user. The GUI is divided into three areas. First is the node list that is read from a node file at startup. The node list contains all nodes that is part of the testbed that you want to monitor. The second area is for commands that you can send to one or many nodes. Typically the commands are implemented as buttons that might have a form for extra arguments. However some commands are chosen with drop down menus, which is very effective when you know that the possible options are limited. The third area is the canvas area. This area uses most of the screen to visualize the events received from node clients. The number of, and which, graphical canvases displayed in the canvas area is defined in the monitor configuration file. As canvases are dynamically loaded at runtime, it is not necessary to recompile Vendetta to support a new canvas.\n\nWhen the monitor is initiated, it waits for node clients to report . As nodes clients are heard from, their status is updated in the GUI and the user can start to interact with them. If the node client software is not running on the remote nodes, the user can use the GUI to initiate the client software. Nodes report to the monitor by sending alive messages. The alive messages, or rather the lack of them, are also used to determine if a node is unavailable. Because the monitor waits for alive messages and node clients are not expecting the monitor to respond, the monitor is not a single point of failure. If the monitor would crash or be closed down, e.g. during longer tests, the user only needs to restart the monitor to regain control of the testbed.\n\nTo reduce the amount of data sent over the network, the monitor can instruct node clients to use a specific filter. It can also request all logged data from one or more node clients to get a more complete picture. The user can decide whether a certain type of log event should be sent using UDP or TCP. Typically periodic node state would be sent using UDP whilst important but rare events are sent using TCP. The node client also buffers log events before sending them to reduce the number of datagrams the monitor need to handle. The buffer is either sent when full, or when a configurable amount of time has passed.\n\nEvents can be monitored in several different ways. Live monitoring gives an overview of what is currently going on in the testbed. As all received data is saved, it is also possible to pause the live monitoring, rewind and replay interesting sequences. When not monitoring events live, the visualization speed can be changed to either fast forward past uninteresting event sequences, or slowly replay the interesting ones.\n\nThere are two ways to send commands from the monitor to the node client, either as a pre-defined command that is parsed by a Java class on the receiving node, or as SSH commands. The Java commands are in clear text and not tested for origin, therefor we do not accept anything other than requests of predefined data units. A button can trigger shell commands over SSH if that flexibility is needed. If a user needs complete control over a remote node, she can start a terminal over SSH to the remote node client by clicking on the selected node and request a terminal. This is substantially easier than logging in on the remote node in a more conventional way.\n\nA playout buffer is used when monitoring live. The length of the playout buffer is configurable, rather than adaptive. Having an adaptive playout buffer might improve the live experience but has not been prioritized as we believe the replay function is what primarily will be used for analysis pur- poses.Beside the monitoring features, the monitor can also perform maintenance tasks. Examples include starting up and shutting down testbed nodes, executing remote commands at several nodes, sending command scripts to choreograph the behavior of nodes, and more. Most management tasks appear as clickable command buttons.\n\nA key principle in the design of Vendetta is flexibility. It is possible to configure both the monitor and node clients to fit into almost any distributed testbed where the participating nodes can run Java programs. In fact, the entire behavior of Vendetta is defined in two configuration files -one for the monitor and one for the node clients. At the monitor, the configuration file defines what elements to display at the screen, what actions to associate with GUI elements such as buttons, and what nodes to communicate with. In the node client, the configuration file defines how to collect and parse data to be sent to the monitor. In section 3.3, we will present more details on how we have used Vendetta to interact with a experimental DHT deployment and what the corresponding configuration files might look like. This will give an indication of the flexibility offered.\n\nNot all behavior can be defined in configuration files. For some purposes, it may be needed to write a Java class that performs some specialized task. One example of this is the PlanetLab module included in Vendetta.\n\nPlanetLab [2] is a distributed testbed consisting of several hundred nodes around the world, available through ssh remote login. Running experiments on the PlanetLab include keeping track of what nodes are active, deploying compiled code to selected nodes, coordinating node actions, collecting results and more. Throughout the years, users of the PlanetLab have developed and redeveloped scripts and tools for these tasks.\n\nMotivated with the popularity of the PlanetLab, Vendetta includes modules for monitoring and management of testbeds based on PlanetLab. For visualization purposes, a canvas showing the geographic locations of available nodes can be used to show node events. By associating an event at a node with a graphical representation, it becomes possible to create more intuitive ways to parse data. As an example illustrated in figure 3.2, query paths in a distributed system can be visualized on the globe, making it obvious when inefficiencies occur due to long communication paths.\n\nFor management purposes, Vendetta includes a module that ease code deployment on the Planet-lab. Nodes to monitor or manage are selected from a list or by clicking on the canvas. By clicking on a button in the GUI, a custom command can be executed at selected nodes. Filters to use, graphical canvas associations, commands to execute and events to monitor are all specified in the configuration file that defines the testbed behavior.\n\nIn this section, we will present a case where Vendetta is used together with a experimental DHT deployment running on the PlanetLab. We will present the testbed-specific components needed, configuration scripts and also discuss what can be achieved when using Vendetta in this way. The purpose of this section is not to present the testbed in itself, but to give an indication of how Vendetta can be configured to interact with it.\n\nThe testbed in our case is used to study a DHT-based overlay network with support for range queries. As we have implemented range queries in the DHT routing mechanisms rather than in an application on top of it, it is interesting to be able to study if actual DHT performance is affected when adding support for range queries. The software used in the DHT testbed is a modified version of Bamboo [4], which is used by OpenDHT [5] that also runs on the PlanetLab. In the process of evaluating our DHT testbed, we need to be able to do the following:\n\n• Distribute a new version of the DHT code to all nodes in the testbed.\n\nThis feature is frequently used during the programming phase when new versions of the code needs to be deployed to all nodes before testing it.\n\n• Dynamically add and remove nodes to/from the DHT. This feature is used during the experiments to mimic churn in the system.\n\n• Study the routing tables and leaf sets at a given node at a specific time.\n\nAs convergence times of routing tables and leaf sets are interesting metrics in churn-prone systems, we want to be able to monitor changes in the routing tables and leaf sets.\n\n• Study query paths at different times. This is used to evaluate the performance in a churn-prone system where query paths might change on relatively short time scales.\n\nThe first two tasks are standard management tasks that are addressed by adding command buttons to the GUI. When a button is clicked, the command associated with it is forwarded to selected client nodes where it is executed. The labels of the command buttons as well as the associated commands are defined in the monitor configuration file. This means, that adding a new command button is usually a matter of adding 4-5 lines of text to the configuration file without having to recompile anything.\n\nThe latter two tasks also requires commands to be sent to request current routing tables and track queries respectively. However, this information is something that is much more intuitive to display in a graphical canvas rather than letting it appear as log messages. For this reason, we have chosen to implement a special canvas that illustrate some testbed-specific events.\n\nOur DHT canvas consists of a DHT ring, which is the common way to represent a one-dimensional address space for DHT networks. On this ring, all nodes in the testbed are represented as points. shows the key distribution among nodes in the DHT ring. In the figure, it is clear that keys are not very evenly distributed among participating nodes at the observed time -something that would have been hard to catch by simply studying log files. By continuing to watch the canvas over time, it will show that keys are eventually spread out over a larger node set. The canvas makes it easy to get a first understanding about the convergence times for the key distribution. All these presentation modes are specific to our testbed and included in the DHT canvas. The globe canvas, as seen in figure 3.2, shows the physical location of the nodes in the testbed, as well as chosen network events. A nice feature of the globe canvas is that the user, by clicking, can choose nodes according to their location. It is also very convenient to highlight a node in the overlay using the DHT ring canvas, and directly get the physical location visualized.\n\nWhen Vendetta is initiated, it reads its configuration file and present the GUI shown in figure 3.1. The part of the configuration file that decides the look of the monitor is presented in figure 3.4. To be able to set the background color has been very convenient when we have changed between using monitors and projectors to present the visualization.\n\nThe <CANVAS> tag is used to specify what canvases to use in the GUI -the DHT ring canvas and the globe canvas that have already been introduced. The canvas area is shared equally between the canvases so removing the DHT canvas from the configuration file would leave all of the top of the screen to the globe. The bottom left part of the GUI includes a node table, with the columns defined in the <TABLECOLUMN> tag in the configuration file, as well as a status message window.\n\nThe <NODECMD>, <OVERLAYCMD> and <LOGFILTER> tags are used to define the management commands available in the GUI. When the configuration is parsed by Vendetta, it creates the command panels shown in figures 3.8,3.6, and 3.10.\n\nThe configuration of the node commands needed in our case study is presented in figure 3.5. We have defined node commands to be commands that would be needed for most testbeds, but which are not directly affecting the tested application. Node commands can for example start or stop the remote node client, update the software running on the remote nodes, or flush the remote message queue. From the configuration file we can see that updating the node client software is done using a local script. The script simply contains a rsync command. In the configuration of commands the user can use <NODE HOSTNAME> which will be replaced with the hostname of the chosen node. If multiple nodes are chosen it will result in a number of parallel calls. A limited amount of parallel calls are allowed, so if too many nodes are called at once the calls are buffered to not overload the monitor. To start the node client, a remote shell command is done using SSH. When the node client is started, TCP can be used to send commands. For example the command to clear the node clients message buffer is using TCP.\n\nIn the next tab, there are the commands that directly affects the tested application. In our case it is to start and stop the Bamboo overlay on a node, to request the routing table, or to request the leaf set. The configuration needed to create the buttons are presented in figure 3.7. From the configuration file we can see that all overlay commands are sent using TCP to the node client. At the node client messages are parsed by the corresponding Java classes. We use commands over TCP rather than SSH because the SSH daemon on the PlanetLab nodes can have large response times. Response times of up to 20 seconds is not uncommon and when we want to control node behavior in\n\n<NODECMD> label=Start VClient type=EXEC msg=ssh uu_bamboo@<NODE_HOSTNAME> daemonize.pl pandora.kicks-ass.org:4444 </NODECMD> <NODECMD> label=Kill Monitor Node type=TCP msg=CTRL_NODE_DOWN_REQ </NODECMD> <NODECMD> label=pkill java type=EXEC msg=ssh uu_bamboo@<NODE_HOSTNAME> pkill java </NODECMD> <NODECMD> label=Update VClient type=EXEC msg=TIMEOUT=60000 ./scripts/updateclient <NODE_HOSTNAME> </NODECMD> <NODECMD> label=Update Bamboo type=EXEC msg=TIMEOUT=600000 ./scripts/updatebamboo <NODE_HOSTNAME> </NODECMD> <NODECMD> label=Run Node Script type=TCP msg=CTRL_SCRIPT </NODECMD> <NODECMD> label=Clear Node Queue type=TCP msg=CTRL_CLEAR_QUEUE </NODECMD> Figure 3.5: Configuration of node client commands Figure 3.6: Node commands panel <OVERLAYCMD> label=Start Bamboo type=TCP msg=CTRL_NET_UP_REQ args=run-java args=bamboo.lss.DustDevil args=node.cfg </OVERLAYCMD> <OVERLAYCMD> label=Kill Bamboo type=TCP msg=CTRL_NET_DOWN_REQ </OVERLAYCMD> <OVERLAYCMD> label=Request RT type=TCP msg=CTRL_RT_REQ </OVERLAYCMD> <OVERLAYCMD> label=Request Leafset type=TCP msg=CTRL_LS_REQ </OVERLAYCMD>\n\nThe configuration file also defines the possible log events, shown in figure 3.9. The log events can be either sent to the monitor using UDP or TCP, or only logged locally by the node client. In the corresponding panel, shown in figure 3.10, there are drop down boxes containing the three different log event actions. When initiated they do not show the current state of the log event, because the default behavior is defined in the node clients configuration file.\n\nIf the user change the behavior with the drop down menus they will then show the current state.\n\nUsing the monitor is relatively easy. It is possible to select one or more testbed nodes to work with by clicking on one of the canvases or highlighting them in the list of available nodes. When this is done, one can use the overlay command panel to let the selected nodes go up or down or request routing table information from them. Using the node commands panel, it is possible to run arbitrary commands at the selected nodes, push out new code, clear caches or ask for a specific script to be used. When scripting commands, it is possible to add timing information to ensure that the commands are executed at exactly the right time. From the log filter commands panel, it is possible to specify what information the node clients on the selected nodes should forward to the monitor. <VENDETTA> 123.123.123.123:1234 </VENDETTA> <PING_INTERVAL> 15000 </PING_INTERVAL> <UDP_TIMEOUT> 8000 </UDP_TIMEOUT> <LOGPARSER> vclient.overlays.bamboo.BambooParser </LOGPARSER> <LOGEVENT> type=LE_STORED_KEYS method=stored_keys regexp=^DataManagerTest.stored:.* net=none </LOGEVENT> <LOGEVENT> type=LE_GET_REPLY method=get_reply regexp=.* INFO bamboo.dht.Dht: upcall for get (range )?req key=.* net=tcp </LOGEVENT> <LOGEVENT> type=LE_GET_RECEIVED method=get_received regexp=.* INFO bamboo.dht.Dht: got new recur get (range )?resp key=.* net=tcp </LOGEVENT> <LOGEVENT> type=LE_GET_ITERATIVE_QUERY method=get_iterative_query regexp=.* INFO bamboo.dht.Dht: sending iterative get req key=.* target .* net=tcp </LOGEVENT> <LOGEVENT> type=LE_GET_DONE method=get_done regexp=.* INFO bamboo.dht.Dht: iterative get req key=.* done$ net=tcp </LOGEVENT> <LOGEVENT> type=LE_PUT_STARTED method=put_started regexp=.* INFO bamboo.dht.Dht: got putreq: .* net=tcp </LOGEVENT> <LOGEVENT> type=LE_GOT_KEY_FROM_ROOT method=got_key_from_root regexp=.* INFO bamboo.dmgr.DataManager: got key=.* from root .* net=none </LOGEVENT> <LOGEVENT> type=CTRL_NET_UP method=joined_overlay regexp=.* INFO bamboo.router.Router: Joined through gateway .* net=tcp </LOGEVENT> <LOGEVENT> type=LE_ASSIGNED_OGUID method=assigned_oguid regexp=.* INFO bamboo.router.Router: Bamboo node .* has guid .* net=no </LOGEVENT>\n\nThe configuration file for each node in the testbed is shown in figure 3.11. In our DHT testbed, we have chosen to use the same configuration file for all node clients. However, it is of course possible to use different configuration files at different clients to give them slightly different functionality.\n\nIn the beginning of the configuration file, some basic parameters like the location of the monitor and timeout settings are defined. The <LOGPARSER> tag defines the name of a local Java method that is used to collect data from the testbed.\n\nThe rest of the configuration file defines different log events that the node client should be able to identify and report back to the monitor. These are defined using standard regular expressions on log messages generated by our tested application.\n\nWith the DHT canvas and the configuration scripts in place, we can start up Vendetta with the GUI shown in figure 3.1. Using the general commands panel, we initiate the node client on all nodes in the testbed via a ssh command. With this done, we can choose what nodes to include in our DHT ring by first highlighting them and then clicking the start bamboo command button in the overlay commands panel. As nodes join the DHT ring, they will show up as dots in the DHT canvas.\n\nTo study the routing table of a node, we highlight that node either in the DHT ring or in the table. When we click the request RT command button in the overlay command panel, the canvas will be updated to show the routing table as illustrated in figure 3.3(b). We can also filter data from a set of nodes by first highlighting them and then specifying a filter from the log filter panel. From the node commands panel, we can choose to add or remove nodes to the DHT ring, push out new configuration scripts, instruct nodes to restart the node client and more.\n\nEvents that match our specified filter are displayed as they occur in the GUI. When something interesting is discovered, we can pause the live monitoring, rewind and replay. This feature makes it possible to study observed phenomena more in detail.\n\nIn our DHT tests, we started off with traditional script-based tools for basic data collection and management tasks. Although this worked quite well, it was far from user-friendly. It was also time-consuming to dig through the collected data to locate what we were interested in. Pretty soon, we realized that a visualization tool would be useful. Doing a quick survey, we could not find a suitable tool that was easy to adapt to our testbed. We did find tools for visualization [3,1], and we did find PlanetLab specific scripts to help deploy code and collecting data.\n\nBut we did not find an integrated solution to support through application development, code deployment, data collection, and data analysis. Hence, we decided to develop the Vendetta tool and to make it as flexible as possible to support others in the same situation.\n\nAfter having used Vendetta for months, it is clear that it greatly improves the efficiency of our experiments. Being able to easily control selected nodes while monitoring them in real-time gives a better idea about what is going wrong, and where things are broken when they break. In the initial phase of our work on DHT, being able to visualize the overlay message paths helped us find non trivial bugs. Adding new commands to the GUI is the matter of adding a few extra lines to the configuration file and then restarting Vendetta.\n\nAlthough we have mainly talked about Vendetta in the context of distributed testbeds, it can also be used for other purposes. By running the node client on the same machine as the monitor, it is possible to visually monitor local events.\n\nThe main advantage with Vendetta in comparison to similar framework is the integration of a GUI supporting 3D canvases to visualize testbed events with flexible monitoring and management tasks. Being able to rewind, fast forward and replay certain parts of an experiment is useful when trying to understand some weird phenomena that was observed. For management purposes, the ability to send command scripts to node clients with information about what to do when means that we can choreograph node behavior with great precision.\n\nHowever, there are of course limitations to what Vendetta can do. During the work, we have identified several usages for the framework that will require some new features. Most of these will however reside in testbed-specific code.\n\nThe Vendetta framework is very much work in progress. Albeit the positive experiences from working with it, we have identified features that will improve its flexibility even more. Examples of such features include (in no particular order):\n\n• The possibility to control connectivity parameters between nodes in the testbed. Being able to change bandwidth, delay, jitter, error rates and other properties of the communication path between two nodes introduce new possibilities when it comes to studying more challenged usage scenarios.\n\n• Combining the logging and command scripting features, i.e., making it possible to produce command scripts that will reproduce observed phenomena in the logs. This can be useful for reproducing the same situation over different set of nodes.\n\n• More canvases, e.g., a dynamic topology graph. This could be useful for wireless testbeds where the network topology can change frequently in the presence of high mobility.\n\nWe have developed a C version of the node client to ease deployment at smaller systems, e.g., a Linux-based basestation like the Linksys WRT54GL, but this code is not yet tested in real experiments. All code will be made available to the research community under an open-source license. The C version is planned to be used in a wireless testbed deployment.\n\nWe also plan to try Vendetta on Emulab as the specifications of Emulab makes us confident that it will work smoothly without modifications.\n\nThe increasing mobility of, and heterogeneity among, Internet endpoints introduce new challenges for network services that need to be investigated. While the fastest access networks gets faster, slower networks with transmission speeds at kbits/s are still around. As it is becoming more popular to use mobile terminals such as cell phones and PDAs to connect to the Internet, services must be able to cope with the limitations in system resources that applies to such devices. Connectivity may not only have low bandwidth, but can also be expensive, intermittent, asymmetric etc. New services that are deployed should be able to handle this wide range of network properties to some extent.\n\nDuring the last few years, a range of p2p-style distributed services using some sort of distributed hash table (DHT) have appeared. Within the networking community, several proposals for DHT data structures have been proposed and evaluated, e.g., [8,15,13,1,14,10]. However, most evaluations tend to be based on the implicit assumption of fairly powerful end nodes connected to the network using stable, high-bandwidth access networks.\n\nBefore you design new systems to cope with the changing network environment it is valuable to understand how current designs actually perform under such conditions. Therefore we have studied how Bamboo [4] -which is a widely used DHT within the research community -performs in a mixed environment with mobile nodes.\n\nWe performed our initial studies using a NS-2 [3] implementation of a DHT following the design of Bamboo. To add random behavior of a real network, which is hard to model when setting up simulation scenarios, we also ran experiments on PlanetLab [7], emulating mobility and bandwidth limitations with a lightweight network emulation tool.\n\nThe main contribution is an initial understanding of how a DHT would perform if mobile phones would particpate as full members. Part of our contribution is also our method to evaluate an application layer network in a heterogeneous environment using PlanetLab.\n\nAs we were able to run experiments for much longer time than possible in the simulation-based studies, we observed reproducible phenomenas not seen before during our simulations: after a few hours, the lookup times started to increase significantly . When investigating the cause of the observed phenomena closer, we have found that it is not solely node churn or bandwidth limitations that caused it, but rather a combination of them.\n\nThis paper is organized as follows. First, we present related work and then the scenarios studied are outlined in section 4.3. Then the measured performance of the DHT is presented. The paper is concluded with a discus-\n\nStrong link Weak link Link between clusters downlink 10 Mb/s 384 Kb/s 100 Gb/s uplink 10 Mb/s 64 Kb/s 100 Gb/s delay 5 ms 115 ms 50 ms Table 4.1: Properties of access links in simulation sion on the results and methodology used.\n\nIt is common to use simulations to evaluate the performance of DHTs [6], although many evaluations are done without taking bandwidth into account.\n\nInstead large networks are evaluated by only modeling network delay. To use NS-2 to simulate DHTs is not as common, although there are other implementations [17]. Evaluations of DHTs that take bandwidth and network queues into account are instead often made using emulation [10]. By using emulation you sacrifice the ability to study very large networks, but it does allow you to model queue drops. To our knowledge, emulation testbeds have not been used to study the effect of heterogeneity to DHTs. We are neither aware of any work similar to our approach to use PlanetLab as an emulation testbed but our approach does have similarities to flexlab [12].\n\nA limitation of our approach to simulations was the complexity of the system which limited the time scales we could study. This limitation motivated an investigation whether the results even from short simulations were generally applicable. Our real-network experiments were conducted on PlanetLab [7] with complementary emulation of connectivity and bandwidth variations.\n\nThe scenario for our experiment is a heterogeneous network with a mix of stationary computers with broadband connections, and 3G-type cellphones. We will call them strong and weak nodes respectively. As weak nodes model mobile users, they join and leave the network often and in an unpredictable manner. Such behavior is called churn, and puts stress on distributed systems like DHTs. The churn in our scenario is modeled using a Poisson process which creates a mean sessiontime of 3 minutes. This is an extreme amount of churn, but cellphones can not be expected to participate in a DHT much In addition to bandwidth, delay and other network-related properties, each node in a DHT also has a number of properties that are directly related to the DHT service, i.e., timeouts, how often management data is sent etc.. We used the parameters described in table 4.2 for both our simulations and real experiments.\n\nWe have performed simulations using our own NS-2 implementation of a Bamboo-like DHT. The details about the implementation and our approach to simulations can be found in a technical report [9].\n\nTo compare with our PlanetLab experiments later described, we simulated networks with a size of 170 nodes as that was the number of nodes we could use on PlanetLab. Each run is 10 minutes and have different ratios of weak nodes ranging from 20 to 50%. 10 strong nodes were used as bootstrap nodes which new nodes connected to when joining the network. Nodes in the DHT were evenly distributed over a physical network modeled with three clusters of nodes, connected by very high bandwidth links with high delay (figure 4.1). The clusters represents different continents, while the links connecting the clusters have high delays and high bandwidth to model backbone transcontinental connections. For simulations, we limited the bandwidth and delay of access links according to table 4.1. These values are chosen based on results from a previous study of a commercially available 3G service [5].\n\nTo limit the time needed to simulate, the overlay network is built offline. This means that we let all nodes have complete knowledge about what nodes participate in the DHT. This allows all nodes to have as populated routing tables and leafsets as possible from the beginning of the simulation -the only restriction is that nodes can not optimize for network latencies as they have not yet measured it. To start the network in such an optimal state is of course unrealistic, but as simulations have showed that a network started in an ordinary fashion will eventually reach a similar stable state, we believe it to be a acceptable method. With our approach Bamboo will stabilize within 80 seconds from when it starts . We want to reduce the stabilization time as much as possible as we will filter out that period of time later on -by doing this optimization we will thus get a larger amount of useful data from the same amount of simulated time. As the time scales we can simulate are limited, this makes a significant difference.\n\nAfter the DHT has stabilized, each node performs a GET operation every 10 seconds to measure the performance of the network. For each GET, we measure whether the operation is successful and if so, how long it took. For the simulations, each node holds about 10 keys that can be requested by other nodes through GET operations. This is a smaller amount of keys compared to the PlanetLab experiments, where we could insert more keys per node.\n\nThe distributed testbed PlanetLab [2] has become a popular approach to evaluate new distributed services and systems. PlanetLab is a collection of 700+ Linux machines spread over the world on which researchers can get accounts to run application-level experiments.\n\nUnfortunately it is hard to create the heterogeneous network environments we want to study with PlanetLab nodes, as you do not have privileges to modify the network stack. For this purpose, we have developed a lightweight connectivity emulation library called Dtour.\n\nThe Dtour design is based on our need to filter an unmodified application in user space to mimic network dynamics as perceived by the application. That need is met by implementing a layer between the application and the network stack (figure 4.2). All system calls that involve outgoing network traffic goes through Dtour where it is filtered. Dtour might drop packets due to for example emulated bandwidth limitations or loss models.\n\nThe design of Dtour is deliberately kept simple. All functionality is implemented in a dynamically loaded library without any active threads or daemons. This means that we do all filtering and state updates when a library function is called. If we instead let a separate thread handle the filtering, we could do state updates continuously, but it would increase the complexity of Dtour. Some operating systems offer the possibility to have shared libraries loaded before the normal system libraries.The library functions in libdtour.so are an entry point into the Dtour system.\n\nCurrently, only outgoing traffic is filtered in Dtour, so the strong nodes filter traffic destined to weak nodes. The path from a strong to a weak node is limited to 384 kbits/s and all outgoing traffic from a weak node goes through a 64 kbits/s bandwidth limiter. We have however added a static filter connected to the read() function which logs the amount of received traffic.\n\nWhen using Dtour, network dynamics are expressed as events. A typical event might be that at time t, add a path to the path set, initiated as down. The time can be expressed either in global time or as relative time from when the scenario is started. What kind of time you use for to describe events is configurable at runtime. The IP and port numbers can be set to 0, which matches all values.\n\nDtour can react to events in two modes. First you can provide a scenario file with network events to be loaded when the libdtour.so library is initiated. The event file is parsed and the events are stored sorted per path to minimize lookup time when filtering.\n\nThe second mode of Dtour is to use it interactively. If this mode is enabled Dtour polls a named pipe for events to be applied as they are read from the pipe. The events written to the pipe are in the same format as in the scenario file apart from not having a timestamp. The two modes can be combined by providing a scenario file and later, or in parallel, modify the links interactively.\n\nWhen the rule set is loaded, Dtour opens the actual system libraries using the dlopen() system call to be able to reach the functions to be overridden. Any number of system calls could be overridden by Dtour but we currently override the functions that are used to send data.\n\nWe have considered to override read(), recvfrom(), etc. but have not yet implemented it. We believe that it will be harder to be completely transparent to an application if we would like to alter how reads are done. We would probably have to alter the behavior of select() to handle incoming data without returning to the application.\n\nWhen we simulated the same scenario we could add extra delay on the weak nodes access links, but that possibility is currently unavailable to nodes on PlanetLab. While we do not make strong nodes churn, they experience a low churn rate caused by the dynamics in the Internet in combination with occasional crashes of PlanetLab nodes. Due to the dynamics of PlanetLab and the roll out of PlanetLab v4 we limited the size of the DHT network to about 170 nodes, even though we in simulation could simulate up to 500 nodes on short time scales.\n\nAfter running the experiments for different ratios of weak nodes, we studied how the PlanetLab measurements compared to our previous results from simulations. Like in our simulations, we cut off the start of the measurements to reduce the influence of stabilization disturbances. While we in our simulations found that 80 seconds was enough, it seemed necessary to cut formance will vary greatly over time with a period depending on ratio of weak nodes and churn rates. We have made further inquiries into the cause of that but it is beyond the scope of this paper to go into the details of the performance decrease. It is however caused by the combination of churn and bandwidth limitations.\n\nThe results presented in figure 4.3 indicate that the cluster model used is a sufficient model to study latencies, but that the lack of cross traffic makes it too simplistic when studying success ratios. For simulation, we use very high bandwidth on the inter-cluster links as we did not want packet loss in the core network, only on access links. For future simulations, it would be interesting to introduce bursty losses in the core network according to our PlanetLab measurements to see if that could make observed success ratios more similar. The problem is that we want to study effects of access links drops as that is characteristic to weak nodes -by introducing core network drops the effect of the heterogeneous access links will be less obvious and therefore harder to analyze.\n\nAn alternative to using PlanetLab is to use an emulation testbed like EmuLab [16] to evaluate a DHT. It would however mean that we would have to design physical network scenarios like in simulation. If we used the same cluster model we used in simulation, we could not be sure how much similarities between simulation and emulation results was caused by the network model. This is something we do not have to worry about when doing real experiments on PlanetLab as the network environment is real and we only emulate the access links for the weak nodes.\n\nIt is also interesting in it self that we can get similar results using two different evaluation methods which we believe gives the actual results higher credibility. We think that the phenomenas we found when running longer experiments do not make our simulation results less relevant but rather that it shows the need to use different methods when evaluating a system. We also find it interesting that the use of Dtour works well to study an application level network in heterogeneous environments.\n\nWe find it promising that Bamboo actually can handle such an extreme amount of churn and still serve a clear majority of successful lookups. If a majority of the lookups are successful you can improve system correctness by using parallel lookups as suggested in the evaluation of OpenDHT [11].\n\nWe would like to investigate if introducing packet loss in the core network in simulations could make our simulation show success ratios that better fit the PlanetLab measurements. We are also working on understanding the changes of lookup latencies over time.\n\nTo be able to design scenarios on PlanetLab that were comparable with our simulations we used our connectivity emulation library Dtour. It has proved essential to model heterogeneity with PlanetLab. Not only by allowing us to limit node's available bandwidth, but also by enabling per packet logging which allowed us to study the traffic distribution within the DHT.\n\nWe have found that our simulations of a Bamboo-like DHT produces relevant results in the case of a heterogeneous network. However, The simulations indicate significantly better success ratio compared to PlanetLab experiments, but this is something we believe can be improved with better understanding of packet loss in the core network.\n\nThe fact that the DHT performs well for many hours indicates that it might be possible to have mobile phones as full members of a DHT. Even though the network suffers from increased delay after a few hours.\n\nWhile doing PlanetLab experiments we found strange behavior that we did not see in simulation. We found that Bamboo experienced a big increase in lookup latencies after a quite extended period of operation. We expected it to be caused by the very high churn rate that we used, but with closer examination it did not seem to be that simple. Although the churn was obviously adding stress to the DHT, it was not the sole explanation. Neither was it the bandwidth limited nodes but combination of churn and weak nodes. Further studies are required to understand the cause of this phenomena in more detail.\n\nWe have a C implementation of the client but it is not yet tested in real experiments"
}