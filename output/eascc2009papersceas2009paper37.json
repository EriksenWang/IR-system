{
    "title": "Anti-Phishing Landing Page: Turning a 404 into a Teachable Moment for End Users",
    "publication_date": "1993",
    "authors": [
        {
            "full_name": "Ponnurangam Kumaraguru",
            "firstname": "Ponnurangam",
            "lastname": "Kumaraguru",
            "affiliations": [
                {
                    "organization": "Carnegie Mellon University",
                    "address": {}
                }
            ]
        },
        {
            "full_name": "Lorrie Faith Cranor",
            "firstname": "Lorrie Faith",
            "lastname": "Cranor",
            "affiliations": [
                {
                    "organization": "Carnegie Mellon University",
                    "address": {}
                }
            ]
        },
        {
            "full_name": "Laura Mather",
            "firstname": "Laura",
            "lastname": "Mather",
            "affiliations": [
                {
                    "organization": "Anti-Phishing Working Group",
                    "address": {}
                }
            ]
        }
    ],
    "abstract": "This paper describes the design and implementation of the Anti-Phishing Working Group (APWG) anti-phishing landing page, a web page with a succinct anti-phishing training message designed to be displayed in place of a phishing website that has been taken down. The landing page is currently being used by financial institutions, phish site take-down vendors, government organizations and online merchants. When would-be phishing victims try to visit a phishing web site that has been taken down, they are redirected to the landing page, hosted on the APWG website. In this paper, we discuss the iterative user-centered design process we used to develop the landing page content. We present the data we collected from the landing page log files from October 1, 2008 through March 31, 2009, during the first six months of the landing page program. Our analysis suggests that approximately 70,000 Internet users have been educated by the landing page during this period. We identified 3,917 unique phishing URLs that had been redirected to the landing page. We found 81 URLs that appeared in our log files in email messages archived in the APWG phishing email repository. We present our analysis of the features of these emails.",
    "full_text": "Since 2003, there has been a dramatic increase in a form of semantic attack known as phishing, in which victims get conned by spoofed emails or fraudulent websites. Phishers send out spoofed emails that look as if they were sent by trusted companies. These emails lead to spoofed websites that are similar or virtually identical to legitimate websites, luring people into disclosing sensitive information. Phishers use this information for criminal purposes such as identity theft, financial fraud, and corporate espionage [9,14].\n\nSoftware vendors and service providers attempt to neutralize phishing attacks by blocking or warning users about phishing messages and phishing web sites. However, until a phishing website is taken down, unprotected users and those who ignore phishing warnings may continue to fall for the attack. Most phishing websites are taken down within three days. Researchers have estimated that the mean lifetime of a typical phishing website is 61.7 hours, while rock phish domains are live for 94.7 hours [19,18]. Users who click on a phishing link after the phishing website has been taken down are typically directed to a 404 error page, as shown in Figure 1 (left). However, an error page provides no feedback to the user about what has just happened. In previous research we observed that the point at which a user has just fallen for a phishing attack is a time when that user is likely to be highly receptive to learning about how to avoid such attacks in the future [11]. Thus, we developed a \"landing page\" to display to would-be phishing victims rather than a 404 error message, as shown in Figure 1 (right).\n\nThe Anti-Phishing Working Group (APWG) is the global pan-industrial and law enforcement association focused on eliminating the fraud and identity theft that result from phishing, pharming and email spoofing of all types. The APWG's Internet Policy Committee (IPC) worked with the CyLab Usable Privacy and Security Laboratory (CUPS) at Carnegie Mellon University (CMU) to apply the results of their anti-phishing education research to the creation of an APWG-sponsored anti-phishing landing page that the industry would adopt. Bank of America had already implemented its own program to present a warning page to users who clicked on a link to a site that had been taken down [22], but was eager to have APWG develop a standard warning page. Once the APWG landing page was launched, Bank of America was one of the first companies to start using it.\n\nThe landing page approach is compelling for several reasons. It enables users to be trained without taking time out of their busy schedules for the purpose of getting trained and it motivates users to pay attention to the training. In addition, the landing page directs training to the users who need it most -those who have \"fallen\" for a phishing attack. The landing page substitutes educational information for a 404 error page that otherwise has no useful information for end users. Finally, use of the landing page creates a repository of data that can be analyzed to better understand phishing.\n\nThe data that we collected from this implementation helps us to estimate the effectiveness of the phish site take-down process and the adoption rate of an industry-wide solution for solving the phishing problem. This data allows us to find out which parts of the world are most vulnerable to phishing emails and how long (in days) a particular URL is being viewed by victims who are getting redirected to the landing page. Finally, through the corpus of phishing emails collected from the APWG, we were able to examine the actual phishing emails and determine the aspects of these emails that make them effective in convincing potential victims to give up valuable information. At the same time, these emails help us determine ways to distinguish phishing emails from legitimate emails sent by the true brand owners.\n\nThe remainder of the paper is organized as follows: In the Brand owner or takedown provider identifies phish site ISP or registrar takes the phish site down\n\nBrand owner or takedown provider identifies phish site ISP or registrar takes the phish site down\n\nFigure 1: Left: The past situation. Users were presented with \"The page cannot be found\" message when they click on a link to a phish site that has been taken down. Right: APWG landing page. Users are presented with a version of the PhishGuru intervention (http://education.apwg.org/r/en/) when they click on a link to a phish site that has been taken down.\n\nnext section we discuss briefly the studies that showed the effectiveness of PhishGuru, learning science principles that we applied in designing PhishGuru, and how phishing sites are taken down. In Section 3, we explain how we developed an effective educational page that teaches people to avoid phishing scams. In Section 4, we discuss the infrastructure used to collect data from the landing page. In Section 5, we discuss the results of our landing page deployment. Finally, in Section 6, we present the implications of the results.\n\nThe APWG landing page training is based on the Phish-Guru embedded training approach developed at CMU. In this section we introduce PhishGuru and discuss previous work studying its effectiveness. We also introduce some of the relevant learning theory concepts. Finally, we discuss the phishing website take-down process.\n\nPhishGuru is an embedded training system that teaches users to avoid falling for phishing attacks by sending them simulated phishing emails. People access these training emails in their inbox when they check their regular email. The training emails look just like phishing emails, urging people to go to some website and login. If people fall for the training email -that is, if they click on a link in that email -we provide an intervention message that explains that they are at risk for phishing attacks and offers tips they can follow to protect themselves. The training materials present the user with a comic strip that defines phishing, offers steps the user can follow to avoid falling for phishing attacks, and illustrates how easy it is for criminals to perpetrate such attacks.\n\nOur previous user studies in the laboratory and in the real world have validated the effectiveness of the PhishGuru approach. In a laboratory study we found that embedded training made participants less likely to fall for subsequent phishing attacks. However, the current practice of employers and service providers sending out security notices did little to protect users from phishing attacks as most users do not read these messages carefully, if at all [11]. In another laboratory study we found that our PhishGuru cartoon was an effective training mechanism when sent as part of an embedded training message, but not when sent directly via email [12]. In a field study conducted with employees of a telecommunications company, we found that PhishGuru can effectively train people in a real-world setting [13]. In another real-world PhishGuru study, we showed (1) users trained with PhishGuru retain knowledge even after 28 days; (2) adding a second training message to reinforce the original training decreases the likelihood of people giving information to phishing websites; and (3) training does not decrease users' willingness to click on links in legitimate messages [10].\n\nProviding immediate feedback at the \"teachable moment\" enhances learning [1], [16]. There is a plethora of literature on teachable moments in fields such as sexual behavior and HIV prevention, injury prevention, and smoking cessation [17]. When users click on a link in a PhishGuru training email, they encounter a training message that alerts them to the risk of clicking on suspicious links, thereby creating a teachable moment that can influence user behavior. This approach enables a system administrator or training company to continually train people as new phishing methods arise.\n\nLearning science literature shows that training is most effective when training materials are presented in the con-text of the real world [4]. Additionally, researchers have shown that providing immediate feedback during the learning phase results in more efficient learning [21]. Researchers have shown that when tutors provide immediate feedback during the knowledge acquisition phase, students learn effectively, move towards more correct behavior, and engage in less unproductive floundering [21,15]. One of the principles developed by Anderson et al. in the context of tutoring is to \"provide immediate feedback on errors\" [3]. Using LISP tutors, Corbett et al. showed that students who received immediate feedback performed significantly better than students who received delayed feedback [6]. Anderson et al. have emphasized that feedback should be immediate; otherwise, students begin to think about something else [2]. Research has also shown that simple forms of feedback, like \"yes\" or \"no,\" and more detailed forms of feedback, like \"the shot was off target to the right by 22mm,\" both encourage effective learning [8]. The PhishGuru system takes advantage of all of these learning tendencies by giving contextual, immediate feedback, thereby encouraging training recipients to maintain safer Internet habits in the future.\n\nBrand owners and take down services try to identify phishing websites as quickly as possible and attempt to get them taken down. Many take-down vendors receive spam feeds from ISPs to use in identifying emerging phishing attacks. These feeds usually contain any URLs that were in the emails marked as spam, but no other information; this ensures that no personal identifiable information is transmitted. Additional email feeds are generated through email honeypot accounts, user reports, feeds sent from financial organizations, and feeds sent from government agencies. Brand owners that perform their own take-downs use customer reports and feeds from other organizations to identify candidate phishing URLs.\n\nThe take-down vendors use an automated process, applying heuristic algorithms to the URLs in the spam feeds in an attempt to identify which ones may belong to phishing sites. These potential phishing URLs are then manually reviewed to verify which are definitively phishing sites. Once the site has been confirmed as a phishing site, it is taken down by one of three groups. The first group that takes down phishing sites is comprised of take-down vendors hired by brand owners. Examples of take-down vendors include Cyveillance, Internet Identity, MarkMonitor, and RSA. Next, some brands have their own internal teams that perform phish site verification and take-down. Finally, there are some vigilante organizations that monitor for phishing URLs on the Internet and perform take-downs.\n\nThe process of taking down a phish site usually consists of several steps. The first step is to determine where the phish site is being hosted. This means determining the owner of the domain, the ISP hosting the domain, whether there is a web service hosting the site, etc. The second step is to contact as many of the individuals or entities that may be hosting the phish site as possible and request that the site be shut down. The person doing the take-down will contact the domain owner, ISP, web hosting service, and anyone else associated with the domain or URL in an attempt to get the phishing site disabled. Sometimes it is necessary to explain what is happening (what a phishing is, how the site got on the web server) to the domain owner or ISP. It may also be necessary to work with them to get the site disabled, as it could be the first instance of phishing the domain owner or ISP has seen.\n\nMany ISPs don't have the knowledge needed to take down rock-phishfoot_0 and fast-fluxfoot_1 type attacks; therefore, these kinds of phishing sites tend to be live for a longer period of time. Rock-phish sites can only be removed if the registrars remove the domain name from the DNS, while fast-flux sites can only be removed by suspending a domain name [20]. Once the phishing URL has been disabled, any user who clicks on a link in a phishing email will typically see a 404 error page stating that the linked URL is not available.\n\nWe designed the landing page using a user-centered iterative design process. Our goal was to design a succinct and engaging training message that could be translated into multiple languages and formatted for a variety of devices, including handheld devices. We began by compiling suggestions for training content from members of the APWG IPC. This first committee design is shown in Figure 2. While this design included all of the content committee members wanted to include, there were concerns that it was too long and not clear enough for non-experts to understand. We developed a second condensed version that omitted some of the content not directly related to phishing, the section on how phishers steal personal information, and the list of links to external resources. We then conducted two focus group studies to evaluate both the short and long version of the proposed landing page and compare them with one of our PhishGuru cartoons. In this section, we discuss each study's setup and results. We explain how the results helped us develop an effective landing page.\n\nThe first focus group we conducted was a 2-hour session at Carnegie Mellon University with 9 participants. There were five females and four males. The average age of participants was 26 years (min: 18, max: 53). Participants received an average of 20 emails per day (min: 5, max: 35). None of the participants knew what phishing was. Participants came from a variety of backgrounds -business, arts and science, social work, fine arts, nursing, music, and psychology. Two of the participants had only a high school degree. Using a wall projector, we began the focus group by demonstrating how someone might click on a link in a phishing email and arrive at the landing page. We then showed them what they might see on a landing page. We discussed details of three versions of the intervention: (1) the committee draft (Figure 2); (2) a condensed draft; and (3) PhishGuru (Figure 4). We provided participants with a color printout of the designs Figure 2: Left: IPC committee version page 1. This design was created by the IPC at APWG. This has information about phishing but also information about software updates and viruses. Participants in the focus group felt that this design was too long. They also had difficulty navigating through the intervention. Right: IPC committee version page 2. Participants in the focus group studies liked the phisher character, but did not understand the meaning of \"Enterprise Users.\" Participants also did not like the idea of links to other sources. and gave them pencils so they could provide feedback on the printouts. We also voice recorded the entire session.\n\nCommittee draft: Participants felt that the committee draft was too much to read. Most participants said they would not read past the \"Help Protect Yourself\" headline because there were too many things for them to parse and understand. Due to the length of the text, six of the nine participants said they would only read until the first instruction. Two said they would read the entire intervention, while only one was willing to read the additional resources.\n\nParticipants had difficulty navigating through the intervention (i.e. participants read down the left column and then down the right rather than reading across). Participants were confused by the browser images; some participants thought they were text entry forms, while others had trouble understanding the DANGER! text above the images.\n\nAll of the participants wanted information about what to do after reading the landing page. Since this page warns users about links in emails or instant messages, participants were concerned about clicking links on this page to get more information.\n\nParticipants liked some of the visual features of the page, particularly the owl. Participants thought the phisher char-acter was very appropriate and wanted to see him at the top of the page.\n\nCondensed version: Participants felt that even though this intervention was short, it was still too long to read from beginning to end. Only three of the nine participants said they would read the entire intervention, while the rest said they would only read until the first instruction.\n\nParticipants recognized the same problems in this version as the committee version. Since navigation in this page was the same as in the committee version, participants felt it was not easy to navigate through the instructions. Participants were also confused by the browser images.\n\nParticipants noted that the phisher's absence in this intervention. Participants suggested bringing the phisher back and putting him at the top. One participant mentioned \"Put the scary guy back.\"\n\nPhishGuru: Most participants said they would be much more likely to read the entire PhishGuru version (see Figure 4 left) of the intervention. Participants found it both entertaining and informative, and they liked the PhishGuru goldfish character. One participant said, \"I really enjoy it and I would probably read it because it is entertaining, but people wouldn't take it seriously.\" One participant ex-claimed: \"Exactly what we were looking for\" after looking at the PhishGuru version. Some had concerns about the perceived credibility of a comic strip. Participants also raised concerns that the comic book font was hard to read and didn't look very official. All participants liked the fish character; some said, \"I like this goldfish\" and \"It is just cute.\" All of the participants agreed that they would definitely read the PhishGuru intervention, adding that the comic script would appeal to their parents and grandparents. One participant mentioned \"I think my grandma would get the comic spot on.\" Participants mentioned that \"Guru\" sounds very official, as it refers to some knowledgeable person.\n\nParticipants stated that having \"Carnegie Mellon\" in the intervention added credibility to the presented information. This sentiment could have been more prevalent because all of the participants were from Pittsburgh.\n\nAlthough participants liked the PhishGuru goldfish character, they complained that the phisher character looked like Batman. They said that the phisher character was not evil or scary enough. They also said they would like the phisher to wear a hat like the phisher character in the committee draft.\n\nAfter listening to participants make many comments about how their grandparents would react to the landing page, we decided to determine how the landing pages would appeal to older people. To that end, we conducted a second focus group study.\n\nThe second focus group we conducted was a 2.5 hour session with six participants at The Jewish Community Center of Greater Pittsburgh. We worked with AgeWell's Independent Adult Services Department to recruit participants who were more than 65 years old. This study involved three females and three males. The average age of the participants was 76 years (min: 66, max: 83), with one participant declining to give her age. Participants received an average of 7.3 emails per day (min: 2, max: 15). None of the participants knew what phishing was. Participants had a variety of educational backgrounds -business, english, architecture, medicine, and engineering. One participant's highest education was a high school degree. As with the first focus group study, we began this focus group by demonstrating how someone might click on a link in a phishing email and arrive at the landing page; we then showed the group what they might see on the landing page. None of the participants knew how easy it is to spoof an email address and send fake emails pretending to come from legitimate organizations.\n\nUsing feedback from the first focus group study, we revised the condensed and PhishGuru versions of the landing page. In the revised condensed version, the instructions were made exactly the same as the PhishGuru version (the second pane with six instructions), but the rest was kept the way it had been in the earlier condensed version. In the PhishGuru version, we changed the comic font to Helvetica, modified some instructions, and cleaned up some text. In focus group study II, we discussed details of three versions of the intervention: (1) the committee draft (Figure 2); (2) the revised condensed draft (Figure 3); and (3) the revised PhishGuru (right Figure 4). We provided color printouts of these designs to participants and gave them pencils so they could provide feedback on the printouts. We also voice recorded the entire session. We made the instructions look exactly the same as in the PhishGuru design. We made the phisher more prominent in this design and added the email address where any complaints or reports could be sent.\n\nCommittee draft: Participants in this study, like those in the first study, responded negatively to the committee draft. Most of the participants said they would not read the complete page. Since the page was long, most of the participants mentioned that they would only scan the whole intervention, while two said they would read it completely.\n\nParticipants in this study were also confused by the browser images. Some participants thought the URLfoot_2 in second instruction (see Figure 2 left) was a link they could click.\n\nParticipants had mixed reactions about the characters in this intervention. Almost all of the participants liked the phisher character, saying \"He looks like a thief or a crimi- Participants enjoyed reading this version and suggested that all age groups would read this intervention in its entirety. Some participants did not think the phisher character looked evil enough.\n\nnal.\" Most did not like the owl character. Some were confused by the owl's magnifying glass and mortar board hat; others thought the owl's hand was a duck's head.\n\nRevised condensed version: Participants liked the fact that the revised condensed version was short and had less text. Some participants mentioned that, even though it was shorter than the committee version, it was still too long for them to read the complete intervention.\n\nParticipants enjoyed the images in this version. Participants liked the emphasis put on things by the \"DANGER\" symbol; they said that this would get their attention and get them to read it. All participants liked the fact that the instructions had pictures they could understand. One participant said \"This is more pictorial than the first one . . . so much better.\"\n\nAs in the previous focus group study, almost all of the participants liked having the Carnegie Mellon name in the intervention. This could again be due to the fact that all of the participants were from Pittsburgh.\n\nOne instruction in particular generated a lot of discussion: most of the participants did not know that calling a phone number in a phishing message could be dangerous. One participant mentioned \"I didn't know that -even if you call a company phone number you will get into trouble.\"\n\nPhishGuru: Participants were attracted to the Phish-Guru intervention, stating that it was fun to read and that people of all ages would read it. Participants were interested in the cartoon format and characters. All participants liked the fish character. Some reactions from the participants about the interventions were: \"I like this one . . . I really do,\" \"eye catchy,\" and \"1 [the committee version] & 2 [the condensed version] are business like and 3 is fun.\" All participants said they would read the complete intervention. All participants agreed that having characters is good and likely to attract readers' attention. Some participants did not think the phisher character looked evil enough, preferring the phisher in the revised committee version. No par-ticipants had concerns about people not taking the revised PhishGuru intervention seriously.\n\nOverall, these focus group studies showed that people in both younger and older age groups like the PhishGuru intervention and would be likely to read it. The main reasons people liked PhishGuru was its character style, pictorial representation, use of narrative, and comic format. Using the results of the focus group studies, we convinced the IPC at APWG to make an intervention for the landing page that was more like PhishGuru.\n\nWe used the focus group results to develop the intervention for the landing page. One constraint was that, in the real world, people might access the landing page from a variety of devices, such as desktop PCs, hand held devices like PDAs, or mobile phones. This meant that the entire intervention needed to be in plain HTML, with as few images as possible. This would make it easier for all types of devices to load the page. Based on feedback from the focus group studies and these constraints, we developed the intervention. 4 This page went live in September 2008.\n\nIn order to make this an industry-wide initiative that any organization could use, a publicly available sub-domain was set up on the APWG website. 5 Information about the project was posted on this website. The English version of the landing page was hosted on the same website. Since this page was going to be translated into many other languages, it was decided that users would be redirected to a specific language depending on the default language of their web browser. As of March 2009, people had volunteered to translate the landing page into Arabic, Bulgarian, Catalan, Danish, Dutch, French, German, Hebrew, Japanese, Korean, Romanian, Spanish, and Swedish. The French landing page is already live. 6The success of this end user phishing education program depends on brands adopting the landing page as their redirect page. To that end, we created a \"how to\" file that provided information about redirecting sites to the landing page. 7 We suggested that, while doing the redirect, the ISP or registrar add the phishing URL as a parameter in the URL requesting the landing page. This is achieved by putting the phishing URL after a \"?\" in the HTTP request for the landing page.\n\nThe APWG's server access log records all requests in Apache's combined log format. 8 By mining the landing page log files, we can create a list of phishing URLs that are redirected to the landing page. We correlated the log data with the APWG's feed of reported phishing emails 9 to find out which emails led most users to visit the landing page. This provided insights into which phishing emails users are most likely to fall for.\n\nIn this section, we present: (1) the complete analysis of the logs we collected and (2) results of the feature analysis performed on the emails retrieved from the email feed, which were done using the URLs in the logs.\n\nThe data that we are collecting in the log files does not represent the entire population of people who are clicking on links in the phishing emails where the redirection was once setup. If a user clicks on a link in the email and the link is already in the blacklist of the browser, then he/she will be blocked and will not get redirected to the landing page. Also, the takedown vendors eventually stop redirecting users to the landing page some time after a site has been taken down. If users click on such a link after the redirection has been removed, the user experience will be same as before redirection (i.e. they will be presented with a 404 error page). Thus, our data is a good lower bound for people who click on links in phishing emails.\n\nTo analyze the results from the logs, we used only log entries that contained '/r/en/?', as these entries were created because users clicked on links in emails to websites that had been taken down. We removed entries that contained the terms 'ORIGINAL PHISH URL' or 'www.phishsite.com' or 'the-phishing-page.html.' These are used in the documentation on how to implement the landing page and are thus likely to be hits from people testing the landing page. The data we used for this analysis was collected from Oct 1, 2008 to March 31, 2009. After filtering the entries, we viewed three segments of the data: (1) the whole-to see the total number of hits the landing page was getting; (2) only those URLs with more than 5 hits; and (3) only those URLs with less than or equal to 5 hits.\n\nWe needed to find a way to differentiate between end users visiting a phishing URL and those involved in the takedown process testing phishing URLs. We tried to identify hits that did not come from end users by looking at the frequency distribution of hits corresponding to each URL, and by getting the IP ranges/subnets from takedown vendors. After experimenting with various approaches we decided to consider URLs that had five or fewer hits as URLs that probably were not hit by end users. By plotting the distribution of hits versus URLs, we found a significant drop in the URLs with more than five hits compared to less than or equal to five hits. We also confirmed that most hits that had more than five hits came from different IP addresses. We believe that phish site URLs that appear in the logs less than five times are mostly takedown vendors or organizations testing their implementation of the landing page or checking whether the landing page is active. The organizations and takedown vendors we worked with said that they check each phished URLs for the redirect at least a couple times. One can argue that the URLs that we are analyzing with hits more than five may include some hits from organizations or takedown vendors, but this is difficult to detect with our current methods. We believe the landing page has created many teachable moments in which users have been trained to avoid falling for future phishing attacks. Table 2 presents statistics for the hits on the landing page. From the entire data, there were 78,541 total hits on the page; among these hits, there were 3,917 unique phishing URLs redirected to the landing page. These statistics suggest that the landing page has been responsible for at least 71,504 \"teachable moments,\" in which a consumer has had the opportunity to learn from the intervention.\n\nTable 3 shows statistics for how long people are clicking Using the IP addresses from the log entries, we identified the country of origin for users viewing the landing page. We saw that most hits (85.9%) came from the United States. This may be due to the fact that, at least for time being, the brands who have adopted the landing page are mainly from the US. This also may be because the organizations being phished are mostly from the US [5]. This result may change as more brands around the world start using the landing page. We also found that around 97.3% of the total hits on the landing page were from the top 10 countries.\n\nTo study the emails that correspond to the URLs being redirected to the landing page, we compared the unique URLs from the landing page logs to the URLs in the APWG email feed. We searched the APWG email feed for emails containing the URLs in the landing page logs and retrieved those emails. We found 81 matches between URLs in our log data and in the APWG feed for the period from Oct 1, 2008 to March 31, 2009. We manually examined the 81 emails and analyzed the features in these emails. Around 95% of the 81 emails pretended to be from one particular financial institution. The rest pretended to be from other popular financial institutions and government agencies. Most of the emails had features similar to legitimate emails. Ninety-one percent of the emails had some form of logo or banner at the top. As Dhamija et al. showed [7], the fact that these logos and banners look legitimate is one of the main reasons people fall for phishing emails. Seventy-three percent of the emails had some sort of footer with logos; in particular, Bank of America emails had an Olympics logo in the bottom right corner (See Figure 5). In some cases, phishers used an exact replica of a legitimate email. Figure 5 presents both a legitimate email and a similar phishing email found in the AWPG feed.\n\nMost of the emails use compelling information to get people to click on the link to the phishing site. Seventy-eight percent had some form of urgent actionable message in their subject line (e.g. \"Online Banking Alert -Your Online Banking Account is Locked\" or \"Your Account Has been Temporarily Suspended\"). Most of the emails (94%) asked users to click on the link and update or verify their account information. Only a few of the emails mentioned that users had a new message in their \"secure message\" inbox and that they should click on the enclosed link to view the message. Most of the emails requested account information, but some explicitly asked recipients to provide \"your username or SSN and your password.\" One of the most common scenarios presented in the phishing emails was an alert to users about suspicious activity on their online bank account. Most of the emails mentioned some form of consequence (e.g. account suspension), and 13% of the emails suggested that there would be consequences if the recipient failed to act within a given time frame. Eighty-five percent of the emails that included a consequence had a deadline of 2 days or 48 hours from the time the email was sent. One common message regarding the timeline was \"Please update your records on or before 48 hours, a failure to update your records will result in a temporary hold on your funds.\"\n\nWe found that the emails contained many formatting and grammatical errors. Some errors in the emails were: \"If this is not completed by octobre 03, 2008,\" \"If we do no receive,\" Figure 5: Left: Phishing email from the APWG email dump that pretends to come from Bank of America. Right: A real email from Bank of America to their customers. All information with \"%\" are used to customize the emails with personal information.\n\nand \"check you account profile.\" Another error was \"please supply all of the following information,\" but it offered no list of what information the recipient should provide. One email was entirely center aligned to the page; this email also presented many telephone numbers in a table format. Some were 1-800 numbers and some were not. Some emails contained text in an entirely bold font, some had text that was all blue, and some contained a combination of black, blue, and red text.\n\nWe found that phishers are still using traditional techniques to con people. Some of the domain names used in the \"from\" email address of these phishing emails look similar to legitimate domains (e.g. onlinebanking@alert. bank0famerica.com, where the 'o' in 'of' is replaced with '0' -zero). Around 74% of the emails have text like \"Click here to continue\" or \"Signin\" or \"click here\" as a link in the email. These sentences are linked to the phishing websites. The rest of the emails had some sort of disguised link leading to the phishing website.\n\nWe also found that, in some emails, there was a mismatch between the subject line and the content of the email. For example, in one email, the subject line was \"Online Banking Alert,\" but the email content scenario was \"Online Banking Sign-in Error.\" In another email, the subject line was \"online Banking Sign-in Error,\" but the content of the email was about verifying account information. We also found mismatches between brands in the sender address and in the content of the email. For example, the content of one email mentioned financial institution A, while the sender information was for a different financial institution B.\n\nNinety-six percent of the emails were not customized for the recipient with any form of personal information. Three of the emails included some form of personal information:\n\n(1) customer ID, (2) account type and ending number, and (3) account type. It is not clear whether this was really customized for the recipient or not.\n\nTo increase the likelihood of people falling for these attacks, phishers are using a wide variety of techniques. Twenty-one percent of the emails invited readers to call for clarification or assistance. A typical example was \"If you are not aware of this situation, please contact us immediately at 1.800.123.456.\" As expected, most of those numbers don't match legitimate phone numbers for the spoofed institution. We also found that the phone numbers for the same institution were different across emails. In its legitimate emails, one financial institution uses different phone numbers depending on the location of the customer or the nature of the email. It looks like, as in other respects, phishers are emulating what legitimate organizations are doing.\n\nIn this paper, we discussed a real-world implementation of a landing page, based on PhishGuru, that educates consumers to avoid phishing attacks at the most teachable moment. In general, we found that a majority of the phishing emails that people \"fall\" for use the same phishing kits to generate the emails used to scam consumers. Since most phishing emails replicate legitimate emails, researchers and industry could reap substantial benefits by creating a corpus of legitimate emails, studying their features, and incorporating these features into email filters. Phishing emails haven't changed much over time, remaining relatively unsophisticated and containing a great number of errors in grammar and formatting. Most of the emails in the log analysis asked users to click on a link in the email to update their account details.\n\nThe results of this analysis confirm that the instructions in the PhishGuru and the landing page cover features contained in most phishing emails. Users who know these cues will be better able to identify phishing emails and avoid being victims of phishing. In particular: (1) We found that all emails had disguised links; this relates to the PhishGuru instruction \"Don't trust links in an email.\" (2) We found that most of the emails ask for account details; this relates to the PhishGuru instruction \"Never give out personal information upon email request.\" (3) Most URLs in the emails look similar to legitimate ones; this relates to the PhishGuru instruction \"Look carefully at the web address.\" (4) Some emails lure people into calling a fake number; this relates to the PhishGuru instruction \"Don't call company phone numbers in emails or instant messages.\"\n\nWe did not find any rock-phish or fast-flux URLs in the data from the logs. We believe the reasons for this are: (1) there is anecdotal evidence that phishers have stopped conducting rock-phish attacks; (2) ISPs don't have the knowledge needed to take down and redirect rock-phish and fastflux attacks.\n\nFor this paper, we only had access to logs and emails, so we don't know anything about the users arriving at the landing page. It would be interesting to conduct a qualitative study among the users who click on links that have already been taken down.\n\nAs the landing page is translated into multiple languages we plan to analyze data longitudinally for different languages and publish a report comparing the quarterly data from these landing page logs. This will help us identify trends relating to the implementation of the landing page.\n\nIn this attack, phishers compromise a machine and use it to run fraudulent versions of many banks' websites. Phishers register meaningless domain names such as recy248.com and create URLs that look legitimate but have a randomly generated alpha-numeric character in the URL. These unique URLs are placed in the emails sent to potential victims.\n\nThis technique involves multiple nodes within the network (mostly compromised machines or botnets) registering and de-registering their address as part of the DNS NS record list. This process constantly changes the destination address for the addresses in the DNS zone.\n\nhttp://www.abcbankexample.com\n\nhttp://education.apwg.org/r/en/\n\nhttp://education.apwg.org/\n\nhttp://education.apwg.org/r/fr/\n\nhttp://education.apwg.org/r/how to.html\n\nhttp://httpd.apache.org/docs/1.3/logs.html#combined\n\nEmails sent to reportphishing@antiphishing.org"
}