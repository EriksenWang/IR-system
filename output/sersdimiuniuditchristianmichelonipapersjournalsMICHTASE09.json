{
    "title": "Active tuning of intrinsic camera parameters",
    "publication_date": "2001-10",
    "authors": [
        {
            "full_name": "Christian Micheloni",
            "firstname": "Christian",
            "lastname": "Micheloni",
            "affiliations": []
        },
        {
            "full_name": "Gian Luca Foresti",
            "firstname": "Gian",
            "lastname": "Luca Foresti",
            "affiliations": []
        }
    ],
    "abstract": "In the last years, the research effort of the scientific community to study systems for ambient intelligence has been really strong. Usually, the systems developed so far base their analysis on images acquired by automatic cameras. In this paper, we propose a way to develop new smart systems that are able to actively decide both what to see and how to see it. In particular, the main idea is to tune the acquisition parameters on the basis of what the system desires to acquire. The regulation strategy is based on two camera parameters, focus and iris. It aims to identify an optimal sequence of steps to enhance the acquisition quality of an object of interest. To this end, a hierarchy of neural networks has been employed first to select which parameter must be regulated then to adjust it. The proposed solution can be applied to both static and moving cameras. The results show how the proposed technique can be applied to images acquired by a moving camera with zoom capabilities for surveillance purposes.",
    "full_text": "Modern visual-based surveillance systems [1]- [4] base their analysis on the detection of moving objects. Different application domains require different object detection techniques and system settings. In the transportation field, vehicles are the main object of interest for traffic monitoring purposes [5], [6]. Persons are fundamental for human activity analysis [7], [8], then trajectories computed on such objects can be exploited for behaviour understanding [9]. Usually, video surveillance applications imply to pay attention to wide areas, hence different kinds of cameras are generally used, e.g, fixed cameras [10], omnidirectional cameras [11] or moving cameras [12]- [16]. Recently, systems using moving cameras have been widely considered thanks to their capacity of patrolling large sections of the monitored area. Moreover, systems using these cameras can look at what they need simply by controlling the motion parameters to redirect the camera's gaze. In the literature, such systems are considered as belonging to the field of Active Vision [17].\n\nFor all image processing systems, the quality of the acquired image is an important feature for the image formation process. In this research, we focus our attention on the determination of the intrinsic camera parameters to enhance the image acquisition quality. The idea behind this work is that controlling the image acquisition quality is preferable than simply assessing it [18]. Managing the image formation process with respect to the current image processing tasks is better than designing complex and robust image processing modules operating on low quality images. This exactly fits with the definition of Active Vision. More, such a concept is extended in such a way that Active Vision is not just the interaction among the observer and the sensors to actively decide what to see [17] but also how to see it.\n\nFollowing this new concept, Active Vision should mean more than just using moving sensors. We want to expand this concept by including techniques that allow to actively control the quality of the image acquisition with respect to an object of interest for image processing purposes. Actual digital cameras consider only a restricted area around the centre of the image to tune the optimal intrinsic parameters. In addition, such techniques have the objective to improve the quality for the humans perception. With this research we want to change this perspective by keeping the objects of interest at the centre of the tuning process. Hence, the proposed solution could be useful in all the ambient intelligent tasks in which decisions have to be taken automatically by intelligent processes. These will take their decisions not on the basis of the perception quality but on the objects' appearance quality.\n\nTo achieve such a result, the proposed method adaptively tunes two acquisition parameters (i.e., focus and iris) by applying quality operators on the object of interest. The control strategy is based on a hierarchy of neural networks trained on quality operators values and camera parameters.\n\nTo better clarify the purposes of this research it is important to understand which are the characteristics of good quality images. Typically, good quality images have a high sharpness degree with respect to focused objects [19]. If we analyse these objects in the frequency domain, we see how their edges are described by high frequency components while smooth (defocused) zones are described by low frequencies [20], [21]. Therefore, since images with high sharpness degree mean focused images, the modern cameras technology aims to maximise the high frequency components [22], [23].\n\nKrotkov in [24] presented and tested various criteria for computing the defocusing degree. Such criteria are all based on quality functions that effectively play an important role in determining the complexity and the efficiency of the solution. These functions should present a set of desired characteristics to be efficiently employed in a regulation strategy:\n\n• The function should achieve the maximum (minimum) value when the best quality is reached. • The function should not present local maxima (minima).\n\n• The function should be independent of the structure of the objects inside the image.\n\nIn addition, the algorithms for increasing the image quality should be evaluated considering the following rules:\n\n• Unimodality: the function should have just a maximum (minimum). • Amplitude difference: the quality function should present significant variation between optimum and poor quality. • Range amplitude: the quality function should continuously vary inside a considerable range of values.\n\n• Complexity: the implementation should be as fast as possible.\n\n• Generalisation: the solution should not consider or be limited to particular cases.\n\nThe regulation process here proposed, to guarantee the desired properties, is based on well known quality operators like tenengrad, flatness, entropy, saturation, max/min grey levels, luminance, maximum local difference, variance, etc. [25], [26]. In particular, the work introduced by Murino et. al [27], which uses different quality operators [28], has been taken into account for the proposed solution. The operators are linearly combined to produce a classification function for the assessment of the image quality. The quality degree of an image is given by a global quality monotonic descendant function Q able to carry information about the global quality of the image. To enhance the image quality, Q has to be minimized.\n\nThe proposed strategy iteratively tunes an intrinsic parameter among Focus, Iris aperture, Gain and Black level. Once a parameter is set to a new position, a new image is acquired and a new Q value is computed. If Q has decreased, the change is accepted. Otherwise, the change is discharged and a new parameter is chosen. This problem can be seen as the search of the minimum value of a function whose domain is represented by the hyperplane defined on the camera parameters ranges.\n\nThe impossibility to estimate the surface of the function Q avoids the use of gradient descendant algorithms. Murino et al., by analysing the situation that may occur during the acquisition process, identified that the image quality can be generally classified into two main categories: a) out of focus and b) with a bad brightness. Moreover, a relation between the focus or brightness quality and the four intrinsic parameters has been derived.\n\nThis yielded to define a strategy based on extracting simple features in order to choose the parameters to set. The general strategy is based on the definition of two semi quality functions defined as linear combinations of simple operators. A function called Brightness Quality (BQ) is introduced to estimate the brightness degree and a function called Sharpness Quality (SQ) is introduced to estimate the focus degree (see [27] for the definitions of BQ and SQ).\n\nThus, to complete the regulation strategy Murino et al. adopted different thresholds T H S , T H B1 and T H B2 to establish if an image is well focused and/or has a correct brightness. In general, an image has a good focus degree if SQ > T H S , while if T H B1 < BQ < T H B2 then the image has a correct brightness. Depending on the values of the two functions three situations may occur:\n\n1) an image is out of focus and its brightness is correct ⇒ activation of the focusing strategy. 2) an image has a good focus degree and its brightness is wrong ⇒ activation of the brightness strategy. 3) an image is out of focus and its brightness is wrong ⇒ activation of the brightness quality While the control of the brightness could be deterministic (lighter or darker), the authors state that there is no information for deciding how to adjust the focus parameter (e.g. near or far). Summarising, the method proposed by Murino et al.\n\npresents two main limits:\n\n• The determination of the optimal values for the thresholds T H B1 , T H B2 , T H S • The development of an efficient strategy for the regulation of the parameters. The use of fixed thresholds strongly depends on experimental tuning, on the conditions of the experiments, on the context of the acquisition, on the adopted sensor, etc. In addition, the use of a random strategy to decide the focus direction (far or close), even though supported by a trial-and-error technique, does not represent an optimal solution. A deterministic solution for the automatic focus parameter regulation is still missing. As matter of fact the number of steps to determine the optimum focus is a key issue. In [29], Kehtarnavaz and Oh present a new rule based approach that speeds up the search for the focus peek. With respect to the trivial global search and to a binary search the proposed solution achieves considerable improvements. In particular, the number of steps to compute the optimal focus is reduced to be around 61 on the average.\n\nIn the remainder of the paper, a description of the proposed system is presented in Section III. In Section IV, the strategy to decide how to tune the intrinsic parameters is presented. Finally, in Section V a deep validation of the proposed solution showing how the introduced novelties improve the actual solutions is given. A CCD progressive colour camera mounted on a Pan-Tilt platform is used to acquire images of the monitored area. Within the controlled environment, there are no restrictions either in terms of number of moving objects or in terms of their movements. The object detection is assigned to a low level change detection module adopting the registration technique proposed by Micheloni and Foresti in [30]. A history of the moving objects is stored in the states of a finite automaton which allows to maintain track of all the objects of interest inside the environment. Within the set of objects, a high level module identifies an object of particular interest. Its blob is then considered as the area of the image on which the proposed regulation strategy is applied.\n\nThe objective is to overcome the limits in [27] by proposing two major improvements on the strategy regulation:\n\n• Exploitation of a neural tree to determine if an image is out of focus and/or it presents a non optimal brightness. • Development of a hierarchy of neural networks for a fast computation of the optimal focus and iris positions for focusing and tuning the brightness. In particular, as shown in Fig. 1, first a Generalised Neural Tree (GNT) [31] is used to decide whether to tune the focus or to improve the brightness. Thus, such a tree operating as a classifier, identifies whether the image area needs to be focused or needs a different brightness. Comparing this novelty with the method proposed in [27], there is no need to adopt any ad hoc fixed threshold for biasing the tuning strategy. In addition, the generalisation of the tree is guaranteed by the heterogeneity of the samples introduced in the training set. This allows to use the developed solution in different contexts without requiring any further tuning of the thresholds.\n\nThe considered GNT has been trained by patterns composed by couples of BQ and SQ values. The possible classifications are represented by the set C={out-of-focus, wrong brightness, out-of-focus with wrong brightness}.\n\nOnce the classification has been performed, two different branches can be taken for the regulation of the considered intrinsic parameters. If the GNT demands a focus regulation, four neural networks, based on the tenengrad measure, are used. The definition of the tenengrad operator is based on:\n\nwhere I x and I y are the gradients of the image respectively on the x and y directions. The tenengrad operator is then given by\n\nwhere N is the number of pixels belonging to the area of interest and T h is a threshold. In the current work, such a threshold is automatically computed by using the thresholding technique introduced by Kapur [32].\n\nThe objective is to identify four points within the focus range in order to estimate an optimal focus position. Thus, the required four focus regulations guarantee a considerable improvement if compared to the 61 steps needed by the technique proposed in [29].\n\nOn the other hand, if the GNT demands a brightness regulation, a first neural network is used to identify whether the iris must be opened or closed. Then, specific networks for each of the two cases have been trained to identify the optimal iris position for the next time instant.\n\nWith respect to commercial solutions, the proposed solution allows to restrict the area of interest as desired (e.g. the bounding box of a selected object). This feature applied on a target tracked by an active method [30] allows to maintain the gaze on an object of interest by keeping an optimal quality of its acquisition. This is a real breakthrough. We are proposing a new Active Vision paradigm in which the observer (i.e. the system) totally tunes the intrinsic (focus and iris) and extrinsic (pan, tilt and zoom) camera parameters to optimally acquire what the system requires.\n\nIn the following section, the attention will be focused on the techniques to tune the selected intrinsic parameters.\n\nThe tuning of the parameters takes into account two intrinsic camera parameters: the focus and the iris. To tune each of these two parameters, two different neural network hierarchies have been studied. Since the tuning process of one value does not locally (for each tuning step) depend on the other parameter, the training processes of the two hierarchies have proceeded separately. In the following subsections, the two processes are presented.\n\nAs shown in Fig. 1, the adopted strategy employs four different neural networks (NNs) that we called respectively: Entry Point, Step, First Step, Optimum Focus.\n\nStudying the tenengrad operator with respect to different environmental contexts and camera configurations, it is noticeable how such an operator is very noisy. In addition, since it is basically a sharpness measure, it does not allow to infer information about the optimal focal position. In particular, from a single measure, it is not possible to determine the direction (far/near) for the regulation. Therefore, it is mandatory a sequence of measurements in order to determine the right direction.\n\nIn Fig. 2, different tenengrad functions with respect to different focus and zoom positions are shown. It is clear that the optimal focus position of the same object varies when the zoom changes. This is due to the fact that the depth of field is linked to the focal length (zoom) of the optics. What is interesting to notice is how the bell shape of the tenengrad function narrows when the zoom level increases (lowest zoom 421 vs. highest zoom 1395). This means that, as we acquire an object with a higher zoom level, we must pay more attention to the focus positions. In these cases, a small displacement from the optimal position results in a big variation of the sharpness quality. Another parameter that influences the depth of filed is the iris aperture. While a zoom operation changes the optimal focus position the iris aperture does not. This analysis suggested to introduce the zoom position as a valuable parameter for the tuning of the focus. Instead, the iris position has not been considered. Anyhow, both zoom and iris parameters cannot be tuned while a focusing regulation is active.\n\nIn Fig. 3, the meaning of the proposed strategy is shown. The tenengrad measure computed on an object of interest is plotted. The horizontal line, called Mean TN, represents the mean value for the tenengrad measures computed on different objects acquired with the same zoom level. Thus, for our purposes, such a line represents a threshold for deciding if the current tenengrad measure describes an object really out of focus or an object that needs a fine focus tuning.\n\nFor the first case, we have developed the following four steps strategy:\n\n• Compute, inside the entire range, a first focus position F 1 that falls below the threshold T N . • Given the entry point F 1 , move the focus into a new position F 2 . • Based on the slope of the line passing through F 1 and F 2 , compute two new focus positions F 3 and F 4 that reside above the threshold T N . • Based on the slope of the line passing through F 3 and F 4 , compute the optimal focus position F OF . In the second case, when the object is not really out-of-focus (i.e. the current tenengrad measure is greater than the threshold T N ), we just need to get two points above the threshold T N before running the last step of the aforesaid strategy. Since the current measure is already above T N , a small motion of the focus is performed for getting the second point. In particular, from the current position the focus is moved nearer into F n position and farther into F f position. The second point F 4 is therefore chosen as follows:\n\nwhere T N x is the tenengrad value computed at focus position F x .\n\nTo develop such a tuning strategy, a neural network hierarchy has been developed (see Fig. 4). If the object is really outof-focus (i.e. the tenengrad value is lower than a threshold), the entry point NN is applied on the current focus position F c , its related tenengrad measure T N c and the current zoom value. The developed NN, defined by means of a trial and error procedure, is full connected and composed by three input nodes, two hidden layers each composed by three nodes and an output node. The output value determines the first focus position F 1 within the focus range.\n\nAt this point, the system moves the lens to reach the focus position F 1 . Once the repositioning is achieved, a new tenengrad measure T N 1 is computed over the area of interest. This new tenengrad value, the corresponding focus position F 1 and the zoom value are given as input to a second NN called\n\nStep. This is also full connected and composed by three input nodes, three hidden layers with three nodes each and an output node. Such a NN finds out a second focus position F 2 whose tenengrad value falls below the threshold T N . The camera is reconfigured to reach such a focus position for which a new tenengrad measure T N 2 is computed.\n\nAfter these two first steps, it is possible to determine the slope of the line connecting the two focus positions in order to estimate two new values greater than T N . For such a purpose, a further NN takes as input the pattern (F 1 , T N 1 , F 2 , T N 2 , Zoom) given by the two computed focus positions with the corresponding tenengrad measures and the zoom position. This NN, called first step, is full connected and composed by five input nodes, two hidden layers with five nodes each and two output nodes. These, determine the two focus positions F 3 and F 4 .\n\nComputed the tenengrad measure for these two last focus positions, the strategy reaches its last step. The last optimum focus NN is executed on the pattern (F 3 , T N 3 , F 4 , T N 4 , Zoom) given by the two focus positions, the corresponding tenengrad measures and the zoom position. Such a NN is full connected and composed by five input nodes, two hidden layers with five nodes each and an output node. The value returned by the output node is the optimal focus position F OF that improves the sharpness value computed on the object of interest.\n\nIf at the next step a new focus regulation is requested, the last two focus positions F 4 and F OF are chosen as F 3 and When M eanT N is plotted together with a tenengrad chart (see Fig. 3), the pour focus positions generate two tails. One on the left side of the chart and one on the right. For the clarity of the presentation, hereafter only the left tail of the chart is considered. Though, the same applies to the right tail.\n\nTo train the Entry Point neural network, for each sequence i = 1, . . . , 120, the tail of the tenengrad chart, given by the range that goes from the nearest focus position to the point F i T N , has been considered (see Fig. 6(a)). The tenengrad value computed on F i T N is the greatest value smaller than the M eanT N value. Within such a range, a focus position F i rnd has been randonmly selected and its tenengrad value T N i rnd has been computed. These two values and the zoom position Zoom i have been included in the training set as a input pattern. Therefore, the mid point F i 1 of the range [nearest, F i\n\nT N ] has been selected as the desired output focus position. The selection of the mid point follows a binary search strategy also adopted in [29]. Summarising, the training set for the first Entry Point NN is defined as follows:\n\n.120]} (4) It is worth noticing that the training set is composed by data related to focus positions whose tenengrad values are smaller then M eanT N . The same holds for the desired output values. The stop criteria adopted for the training process has consisted in a maximum expected error of 0.01 equivalent to 1/100 of the minimum focus step. Such a stop criteria required 9 × 10 4 training epochs to converge to a solution.\n\nFor the training of the Step NN, the operative situation has been considered (see Fig. 6(b)). In particular, the Step NN is executed on a pattern that is determined by the Entry Point NN (i.e. F 1 ). Thus, to train the Step, for each sequence i, a focus position F i rnd ∈ [nearest, F i T N ] has been randomly selected. Such a value defines the input pattern for the Entry Point NN whose output is the focus position F i 1 EP . For each sequence i, the position F i 1 EP represents a good approximation of the optimal output F i 1 associated to the previous NN. So, the input pattern of the Step network is determined on the basis of the F i 1 EP focus position. To define the desired output value, following the strategy adopted in the previous training process, the mid point F i 2 between F i 1 EP and F i T N has been selected. Hence, the training set for the Step NN is defined as follows:\n\n.120]} (5) It is worth noticing that, even in this case, the training set is composed by patterns containing focus positions whose tenengrad values are smaller than M eanT N . In addition, the input patterns are composed by values computed by the first NN as it will be in an operative situation. For the training process of this NN, the maximum expected error has been set to 4 × 10 -3 . Such a stop criteria required 9.6 × 10 4 training epochs to converge to a solution.\n\nTo train the First Step NN, the same concept used for the Step NN has been adopted (see Fig. 6(c)). To determine the input pattern, for each sequence i, a focus position F i rnd has been randomly selected and given in input to the Entry Point NN. Its output F i Zoom i ) have been given in input to the trained Step NN. The output F i 2 s of this second NN represents a good approximation of the optimal value F i 2 determined for the sequence i during the definition of the previous training set. At this point F i 1 EP and F i 2 s represent two focus positions whose tenengrad values are minor than M eanT N . Hence, looking at the focusing strategy, they represent two good positions to train the third NN First Step. To determine two optimal output positions, the mid point F i 3 of the range [F i T N , F OF ] and the mid point F i 4 of the range [F i 3 , F OF ] have been considered. Thus, the range related to the left side of the good focus values has been iteratively bisected. F i 3 and F i 4 are focus positions whose tenengrad values are both greater than M eanT N . Therefore, the training set for the First Step NN can be defined as follows\n\nLet be {((\n\n)} a pattern determined by randomly selecting a focus position F i rnd on the sequence i and by running the Entry Step and Step NNs on it (see Fig. 6(d)). Such a pattern given in input to the First Step NN generates two focus position F i 3 F S and F i 4 F S . These two positions define the input pattern of the last NN. Obviously, to train the Optimum Focus NN, the desired output value is the position F OF whose tenengrad value is the maximum. Thus, to train the Optimum Focus NN the training set has been defined as:\n\nFor the training process of the last two NNs, the maximum expected error has been set to 2 × 10 -3 . Such a stop criteria required about 10 × 10 4 and 11 × 10 4 training epochs to converge to a solution respectively for the First Step and Optimum Focus NNs.\n\nAs it can be understood, the adopted strategy for the definition of the training set is not trivial. As matter of fact it is a synthesis of several tests as much as selection methods. In particular, selecting the training sets autonomously for each network (i.e. without considering the output of the preceding NNs) yielded to the missed convergence of the NNs or to unpredictable outputs. For the convergence, the high variability of the randomly selected input patterns did not allowed to reach the required training errors. To achieve the convergence, greater expected errors have been necessary. This solution yielded to NNs whose outputs were not in accordance with the defined strategy. In particular, the Step network computed focus positions whose tenengrad values were greater than M eanT N . Similarly, the First Step computed values were either not greater than M eanT N or on either side of the optimal point. These unpredictable values do not allow to reach a position that well approximates the optimal focus. Instead, by adopting a cascade strategy, the patterns variability Fig. 7. Network hierarchy and strategy for the iris tuning is reduced and the convergence is achieved. In addition, the\n\nStep NN gives always focus positions whose tenengrad values are smaller than M eanT N . Similarly the First Step NN guarantees two focus positions on the same side with respect to the optimal focus. Thus, the strategy is always guaranteed to operate on expected values.\n\nCompared to the focus tuning, the regulation of the brightness is much easier. Whilst the tenengrad value does not suggest the tuning direction, the brightness does. From the classification of the current object brightness as too dark or too bright, it is possible to decide if the iris should be opened or closed. Hence, the regulation strategy (see Fig. 7) for the brightness value takes into account three different feed forward NNs trained with a back propagation algorithm. The first NN is composed by two input nodes, a hidden layer with two nodes and two output nodes. This is the network that classifies the image region as too dark or too bright. The input pattern is composed by the value of the semi-quality function BQ and by the Luminance value defined as follows:\n\nwhere B is the blob of the considered object and V (x) is the Variance value computed on the position of the pixel x and on the HSV colour model. The Variance is computed by considering the grey level as a stochastic variable. It is given by the quadratic difference between the grey values and their mean µ:\n\nThe desired output of the network is the classification of the blob brightness as low or high.\n\nThe classification of the blob as too dark or too bright allows to run the proper NN to open or close the iris. As shown in Fig. 7, both NNs, to estimate the optimal iris position, use a input pattern defined by the current iris position and by the luminance values at the current time instant t and at previous time instant t -1. The difference between these two NNs is that one is trained to open the iris while the other to close it. The outputs of the NNs is the number of closing or opening iris steps. To train the NNs the strategy followed was the same adopted to train the focusing NNs. Thus, 20 sequences have been acquired by scanning all the iris positions within the range. This means that, by acquiring 30 images for each sequence, a total of 600 training images have been considered. Some examples of the training sequences are presented in Fig. 8.\n\nSuch images have been therefore classified by human operators into three classes: a) Too dark, b) Too bright and c) Good quality (just one for each sequence). The images classified as too dark or too bright have been used to compute the quality function BQ and the luminance value L. Then, the first NN has been trained with the following patterns:\n\nwhere i is the index of the the current image and Dark or Bright its expected classification.\n\nTo train the following NNs, the images classified as too dark have been employed to train the opening NN. While the images too bright have been used to train the closing NN. Since the two procedures are similar, only the training of the opening NN is presented. For each image, the corresponding iris position Iris and luminance value L t have been inserted in the training pattern. In addition, the luminance value corresponding to the image related to the previous iris position L t-1 in the opening direction is also included in the training pattern. The expected value of the network is given by the number of iris positions (steps) between the current iris position and the iris position of the image classified as optimal.\n\nTo test the effectiveness of the proposed method and its impact on active vision applications, the experiments have been first conducted on a IEEE-1394 SONY DFW-VL 500 camera mounted on a PTU platform. This set of experiments has been considered to verify the reliability and accuracy of the proposed method to focus on targets, to tune the target brightness and finally to adjust both parameters. A second round of experiments has been considered to compare the proposed solution to standard systems available on the market. This type of experiments aimed to demonstrate how the capability to tune parameters on selected image areas is of  great advantage for applications based on image processing.\n\nTo achieve such an objective, a standard Canon MV-600i handycam and an Axis PTZ-213 have been selected as metre to assess the proposed solution benefits. All the sequences used for the experimental evaluation have been independently acquired from those used during the training phase. As matter of fact, the chosen environment, the lighting and the weather conditions were different.\n\nThe focusing capabilities of the proposed solution have been evaluated by tracking and focusing objects in an outdoor environment. In this context, the proposed method succeeded in focusing the objects in a period that spans from 0.5s to 2s. This time range is in contrast with real-time applications. However, the transmission delays to control the camera and to acquire new images have to be taken into account in such an evaluation. A noticeable of the focusing time could be achieved by implementing the proposed solution on-board to smart cameras. The important thing is that, no matter which are the initial focus position and the defocusing degree of the objects, the hierarchy achieves its goal about focusing the object.\n\nIt is worth noticing how the proposed technique, by operating on information extracted only from the object of interest, introduces a side effect on the remaining areas of the image. In particular, by improving the acquisition quality of the object, the remaining regions of the image could exhibit a lower quality. This effect can be seen in Fig. 9 where the tenengrad values computed just within the bounding box of the object Fig. 9(a  first 10 seconds of a test sequence. In these two charts the trend of the two curves is the opposite.\n\nTo compare the proposed solution with a commercial camera, a first experiment has been executed by shooting the same scene, represented by a moving object acquired in an outdoor environment, with both automatic and proposed regulation strategy. For both cases, during the acquisition process the tenengrad value has been computed on the bounding-box of the object of interest. Some frames of a test sequence can be seen in Fig. 10. The experiment has been conducted on 20 different sequences of 30s each. In this context, after few frames, needed to estimate the first Optimum Focus position, the performance of the proposed strategy shows a mean increment of the tenengrad value of about 15%. If we consider the case in which the object is not in the centre of the image, the increment is of about 25%. As can be seen in Fig. 11, when the position of the object of interest is not close to the centre of the image, the commercial approach shows a worsening of the tenengrad value (Fig. 11(b)). Instead, the proposed techinque, focusing on the object, maintains the tenengrad value almost constant regardless the position of the object inside the image (Fig. 11(a)). The difference of the two approaches is considerable (Fig. 11(c)). The overall gain of the proposed solution is estimated in a fair 6% for objects near the image center to a remarkable 23% when the objects are near to the corners of the image.\n\nRegarding the tuning of the image brightness the experiments have been performed by comparing the results obtained by the proposed and common techniques. In particular, the segmentation performance for both methods has been selected as evaluation metric. Due to the difficulties of creating the ground truth data of the blobs, the tests have been limited to 4 sequences of 30s each. A sample of the result obtained by the two approaches on a test sequence is shown in Fig. 12. It is worth noticing how the proposed solution generates images whose global quality is not good. They seem overexposed. But, if we consider just the region of the target, the contrast between the object and the background is much greater than in the common sequence.\n\nTo evaluate the impact of the proposed strategy in the context of video surveillance applications, a last set of experiments has been taken into account. A first test shows the advantages of the proposed solution in tough lighting conditions. When the object moves in an area whose brightness is much different from the remaining part of the scene ( shadow vs. sunlight), the proposed method shows an increment in the segmentation (i.e., number of corrected foreground pixels detected with respect to ground truth data) of about 35%. To show this, an Axis PTZ 213 with auotmatic or controlled iris regulation has been used. Fig. 13 shows some frames of the sequence acquired with the camera in autoiris mode. Fig. 14 shows some frames of the sequence acquired exploiting the proposed tuning strategy.\n\nIt is worth noticing that in the first case the automatic configuration is not able to handle the brightness compensation when the person walks in the bright area. As matter of fact, in the Fig. 13(d),(i)-(l) frames, the upper part of the body is totally unnoticeable. In particular, in the frame Fig. 13(k) the person has the arms wide open at the shoulder level. In these cases, there is no surveillance system able to segment the person to identify the silhouette for further analysis like behaviour understanding. To demonstrate this, in Fig. 13(h),(m)-(p) frames, the results obtained by the system described in [30] show how the torso of the person has not been detected.\n\nOn the opposite, in the same lighting conditions, the pro- (a) (b) (c) (d) (e) (f) (g) (h) (i) (j) (k) (l) (m) (n) (o) (p) Fig. 14. Frames extracted from a sequence acquired with the camera Axis 213 using the proposed regulation strategy. First and third rows present the acquired frames (full sequence available at http : //users.dimi.uniud.it/ ∼ christian.micheloni/T ASE/StrategyIndeo.rar.). Second and fourth row show change detection images obtained by the system proposed in [30].\n\nposed strategy continuously tunes the iris parameter making it possible to acquire the person of interest with a good quality.\n\nIndeed, comparing Fig. 14(j) with Fig. 13(k) it is clear how in similar conditions the acquisition quality of the proposed solution outperforms the acquisition quality of automatic techniques on-board of modern CCTV cameras. This difference is even more evident comparing the change detection results. All the frames 14(e)-14(h),14(m)-14(p) show how the same system [30] applied on images acquired by tuning the parameters returns the entire silhouette of the person. This result is of great importance for further processing steps.\n\nA second test has been performed to show how the proposed technique could be coupled with usual active vision techniques. In particular, we have measured the accuracy of the blobs identification, computed by applying a change detection technique [30], on the images acquired by a moving camera. The proposed strategies (i.e. tuning both focus and iris) and a common camera approaches have been considered. Also in this context, the proposed method has shown good results by incrementing the blob segmentation of about 10% with peaks of 543%.\n\nIn the current paper, a new method that aims to expand the paradigm of Active Vision has been proposed. In particular, the concept \"how to see the object\" has been introduced. This is a new way of thinking how a sensor should actively acquire an object of interest. To achieve such an objective, we have studied and developed a new method for enhancing the acquisition quality just on the image area related to the object of interest. A hierarchy of neural networks has been developed to control two main intrinsic camera parameters: Focus and Iris.\n\nThese two parameters are unequivocally related to the control of the image focus and brightness. It has been shown how the possibility to control these two parameters is relevant for improving surveillance applications. To achieve such results, quality operators have been exploited to define quality functions and tuning strategies. In particular, two quality functions have been used to classify the image and two operators have been used as principal values for the input patterns for the neural networks classifications.\n\nThe experimental results have shown how the proposed technique is really effective to control the quality of the image acquisition. Improving the quality of the acquired object improves the detection performance of low level techniques. This implies that new surveillance systems can be developed by considering the control of the acquisition quality as part of the loop for improving the system performance."
}