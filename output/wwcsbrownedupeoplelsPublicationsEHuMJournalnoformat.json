{
    "title": "HumanEva: Synchronized Video and Motion Capture Dataset and Baseline Algorithm for Evaluation of Articulated Human Motion",
    "publication_date": "2009-07-27",
    "authors": [
        {
            "full_name": "Leonid Sigal",
            "firstname": "Leonid",
            "lastname": "Sigal",
            "affiliations": [
                {
                    "organization": "Department of Computer Science, Department of Computer Science, Department of Computer Science, University of Toronto, Brown University, Brown University, Brown University",
                    "address": {}
                }
            ]
        },
        {
            "full_name": "Alexandru O Balan",
            "firstname": "Alexandru O",
            "lastname": "Balan",
            "affiliations": [
                {
                    "organization": "Department of Computer Science, Department of Computer Science, Department of Computer Science, University of Toronto, Brown University, Brown University, Brown University",
                    "address": {}
                }
            ]
        },
        {
            "full_name": "Michael J Black",
            "firstname": "Michael J",
            "lastname": "Black",
            "affiliations": [
                {
                    "organization": "Department of Computer Science, Department of Computer Science, Department of Computer Science, University of Toronto, Brown University, Brown University, Brown University",
                    "address": {}
                }
            ]
        }
    ],
    "abstract": "While research on articulated human motion and pose estimation has progressed rapidly in the last few years, there has been no systematic quantitative evaluation of competing methods to establish the current state of the art. We present data obtained using a hardware system that is able to capture synchronized video and ground-truth 3D motion. The resulting HUMANEVA datasets contain multiple subjects performing a set of predefined actions with a number of repetitions. On the order of 40, 000 frames of synchronized motion capture and multi-view video (resulting in over one quarter million image frames in total) were collected at 60 Hz with an additional 37, 000 time instants of pure motion capture data. A standard set of error measures is defined for evaluating both 2D and 3D pose estimation and tracking algorithms. We also describe a baseline algorithm for 3D articulated tracking that uses a relatively standard Bayesian framework with optimization in the form of Sequential Importance Resampling and Annealed Particle Filtering. In the context of this baseline algorithm we explore a variety of likelihood functions, prior models of human motion and the effects of algorithm parameters. Our experiments suggest that image observation models and motion priors play important roles in performance, and that in a multi-view laboratory environment, where initialization is available, Bayesian filtering tends to perform well. The datasets and the software are made available to the research community. This infrastructure will support the development of new articulated motion and pose estimation algorithms, will provide a baseline for the evaluation and comparison of new methods, and will help establish the current state of the art in human pose estimation and tracking.",
    "full_text": "The recovery of articulated human motion and pose from video has been studied extensively in the past 20 years with the earliest work dating to the early 1980's [28,53]. A variety of statistical [1,2,7,17,30,74,75,76] as well as deterministic methods [46,83,69] have been developed for tracking people from single [1,2,21,30,36,45,46,58,59,60,63,75] or multiple [7,17,26,74] views. All these methods make different choices regarding the state space representation of the human body and the image observations required to infer this state from the image data. Despite clear advances in the field, evaluation of these methods remains mostly heuristic and qualitative. As a result, it is difficult to evaluate the current state of the art with any certainty or even to compare different methods with any rigor.\n\nQuantitative evaluation of human pose estimation and tracking is currently limited due to the lack of common datasets containing \"ground truth\" with which to test and compare algorithms. Instead qualitative tests are still widely used and evaluation often relies on visual inspection of results. This is usually achieved by projecting the estimated 3D body pose into the image (or set of images) and visually assessing how the estimates explain the image [17,21,60]. Another form of inspection involves applying the estimated motion to a virtual character to see if the movements appear natural [76]. The lack of the quantitative experimentation at least in part can be attributed to the difficulty of obtaining 3D ground-truth data that specify the true pose of the body observed in video sequences.\n\nTo obtain some form of ground truth, previous approaches have resorted to custom action-specific schemes; e.g. motion of the arm along a circular path of known diameter [34]. Alternatively, synthetic data have been extensively used [1,2,26,69,76] for quantitative evaluation. With packages such as POSER (e frontier, Scotts Valley, CA) or MAYA (Autodesk, San Rafael, CA), semi-realistic images of humans can be rendered and used for evaluation. Such images, however, typically lack realistic camera noise, often contain very simple backgrounds and provide simplified types of clothing. While synthetic data allow quantitative evaluation, current datasets are still too simplistic to capture the complexities of natural images of people and scenes.\n\nIn the last few years, there have been a few successful attempts [23,35,47,65] to simultaneously capture video and ground truth 3D motion data (in the form of marker-based tracking); some groups were also able to capture 2D motion ground truth data in a similar fashion [89]. Typically hardware systems similar to the one proposed here have been employed [35] where the video and motion capture data were captured either independently (and synchronized in software off-line) or with hardware synchronization. While this allowed some quantitative analysis of results [23,35,47,65,89], to our knowledge none of the synchronized data captured by these groups (with the exception of [89], discussed in Section 2) has been made available to the community at large, making it hard for competing approaches to compare performance directly. For 2D human pose/motion estimation, quantitative evaluation is more common and typically uses hand-labeled data [30,58,59]. Furthermore, for both 2D and 3D methods, no standard error measures exist and results are reported in a variety of ways which prevent direct comparison; e.g. average root-mean-squared (RMS) angular error [1,2,76], normalized error in joint angles [69], silhouette overlap [58,59], joint center distance [7,26,36,39,41,74,75], etc.\n\nHere we describe two datasets containing human activity with associated ground truth that can be used for quantitative evaluation and comparison of both 2D and 3D methods. We hope that the creation of these datasets, which we call HUMANEVA, will advance the state of the art in human motion and pose estimation by providing a structured, comprehensive, development dataset with support code and quantitative evaluation measures. The motivation behind the design of the HUMANEVA datasets is that, as a research community, we need to answer the following questions:\n\n• What is the state-of-the art in human pose estimation?\n\n• What is the state-of-the art in human motion tracking?\n\n• What algorithm design decisions affect human pose estimation and tracking performance and to what extent?\n\n• What are the strengths and weaknesses of different pose estimation and tracking algorithms?\n\n• What are the main unsolved problems in human pose estimation and tracking?\n\nIn answering these questions, comparisons must be made across a variety of different methods and models to find which choices are most important for a practical and robust solution. To support this analysis, the HUMANEVA datasets contain a number of subjects performing repetitions (trials) of a varied set of predefined actions. The datasets are broken into training, validation, and test sub-sets. For the testing subset, the ground truth data are withheld and a web-based evaluation system is provided. A set of error measures is defined and made available as part of the dataset. These error measures are general enough to be applicable to most current pose estimation and tracking algorithms and body models. Support software for manipulating the data and evaluating results is also made available as part of the HUMANEVA datasets. This support code shows how the data and error measures can be used and provides an easy-to-use Matlab (The Mathworks, Natick, MA) interface to the data. This allows different methods to be fairly compared using the same data and the same error measures.\n\nIn addition we provide a baseline algorithm for 3D articulated tracking in the form of simple Bayesian filtering. We analyze the performance of the baseline algorithm under a variety of parameter choices and show how these parameters affect the performance. The reported results on the HUMANEVA-II dataset are intended to be the baseline against which future algorithms that use the dataset can be compared. In addition, this Bayesian filtering software is freely available, and can serve as a foundation for new algorithm development and experimentation with image likelihood models and new prior models of human motion.\n\nIn systematically addressing the problems of articulated human pose estimation and tracking using the HUMAN-EVA datasets, other related research areas may benefit as well, such as foreground/background segmentation, appearance modeling and voxel carving. It is worth noting that similar efforts have been made in related areas including the development of datasets for face detection [55,56], human gait identification [27,67], dense stereo vision [68] and optical flow [4]. These efforts have helped advance the state-of-the-art in their respective fields. Our hope is that the HUMANEVA datasets will lead to similar advances in articulated human pose and motion estimation. In the short time that the dataset has been made available to the research community, it has already helped with the development and evaluation of new approaches for articulated motion estimation [8,9,38,40,41,50,62,84,88,91]. The dataset has also served as a basis for a series of workshops on Evaluation of Human Motion and Pose Estimation (EHuM) 1 set forth by the authors.\n\nArticulated Pose and Motion Estimation. Classically the solutions to articulated human motion estimation fall into two categories: pose estimation and tracking. Pose estimation is usually formulated as the inference of the articulated human pose from a single image (or in a multi-view setting, from multiple images captured at the same time). Tracking, on the other hand, is formulated as inference of the human pose over a set of consecutive image frames throughout an image sequence. Tracking approaches often assume knowledge of the initial pose of the body in the first frame and focus on the evolution of this pose over time. These approaches can be combined [74,76], such that tracking benefits from automatic initialization and failure recovery in the form of static pose estimation and pose estimation benefits from temporal coherence constraints.\n\nIt is important to note that both tracking and pose estimation can be performed in 2D, 2.5D, or 3D, corresponding to different ways of modeling the human body. In each case, the body is typically represented by an articulated set of parts corresponding naturally to body parts (limbs, head, hands, feet, etc.). Here 2D refers to models of the body that are defined directly in the image plane while 2.5D approaches also allow the model to have relative depth information. Finally 3D approaches typically model the human body using simplified 3-dimensional parts such as cylinders or superquadrics. A short summary of different approaches with evaluation and error measures employed (when appropriate) can be seen in Table 1; for a more complete taxonomy, particularly of older work, we refer readers to [24] and [44].\n\nCommon Datasets. While HUMANEVA is the most extensive dataset for evaluation of human pose and motion estimation, there have been several related efforts. A similar approach was employed by Wang et al. [89] where synchronized motion capture and monocular video was collected. The dataset, used by the authors to analyze performance of 2D articulated tracking algorithms, is available to the public 5 . The dataset, however, only contains 4 sequences (2 of which come from old movie footage and required manual labeling); only 2D ground truth marker positions are provided. The INRIA Perception Group also employed a similar approach for collection of ground truth data [35], however, only the multi-view video data is currently made available to the public.\n\nThe CMU Graphics Lab Motion Capture Database [15] is by far the most extensive dataset of publicly available motion capture data. It has been used by many researchers within the community to build prior models of human motion. The dataset, however, is not well suited for evaluating video-based tracking performance. While, for many of the motion capture sequences, low-resolution monocular videos are available, the calibration information required Table 1: Short survey of the human motion and tracking algorithms. Methods are listed in the chronological order by the first author. Type refers to the type of the approach, where (P) corresponds to the pose-estimation and (T) to tracking. Approaches that employ (⋆) and (⋆⋆) evaluation measures are consistent with the evaluation measures proposed in this paper.\n\nYear First Author Model Type Parts Dim Type Evaluation Measure Hogg [28] Cylinders 14 2.5 T Qualitative Ju [33] Patches 2 2 T Qualitative Kakadiaris [34] D Silhouettes 2 3 T Quantitative Bregler [11] Ellipsoids 10 3 T Qualitative* Rosales [64] Stick-Figure 10 3 P Synthetic ⋆ 2 Sidenbladh [73] Cylinders 2/10 3 T Qualitative Ronfard [63] Patches 15 2 P Hand Labeled Sidenbladh [71] Cylinders 2/10 3 T Qualitative Grauman [26] Mesh N/A 3 P Synthetic/POSER ⋆ Ramanan [59] Rectangles 10 2 T,P Hand Labeled ⋄⋄ Shakhnarovich [69] Mesh N/A 3 P Synthetic/POSER ‡ Sminchisescu [78, 79] Superquadric Ellip. 15 3 T Qualitative 3 Agarwal [1, 2] Mesh N/A 3 P Synthetic/POSER † Deutscher [17] R-Elliptical Cones 15 3 T Qualitative Lan [37] Rectangles 10 2 T,P Qualitative Mori [46] Stick-Figure 9 3 P Qualitative Roberts [61] Prob. Template 10 2 P Qualitative Sigal [74] R-Elliptical Cones 10 3 T,P Motion Capture ⋆⋆ Balan [7] R-Elliptical Cones 10 3 T Motion Capture ⋆⋆ Felzenszwalb [21] Rectangles 10 2 P Qualitative Hua [30] Quadrangular 10 2 P Hand Labeled ♮ Lan [36] Rectangles 10 2 P Motion Capture ⋆ Ramanan [58] Rectangles 10 2 T,P Hand Labeled ⋄⋄ Ren [60] Stick-Figure 9 2 P Qualitative Sminchisescu [76] Mesh N/A 3 T,P Synthetic/POSER † Gall [23] Mesh N/A 3 T Motion Capture † Lee [39] R-Elliptical Cones 5/10 3 T,P Hand Labeled ⋆⋆ 4 Li [41] R-Elliptical Cones 10 3 T HUMANEVA ⋆⋆ Rosenhahn [65] Free-form surface patches N/A 3 T Motion Capture † Sigal [75] Quadrangular 10 2 P Motion Capture ⋆ Urtasun [85] Stick-figure 15 3 T Qualitative Wang [89] SPM + templates 10 2 T Motion Capture ⋆ and ⋄ Lee [38] Joint centers N/A 3 T HUMANEVA ⋆⋆ Mundermann [47] SCAPE 15 3 T Motion Capture ⋆⋆ and ⋄ Navaratnam [48] Mesh N/A 3 P Motion Capture † Srinivasan [82] Exemplars 6 2 P Hand Labeled ⋆ and ⋄ Xu [91] Cylinders 10 3 T HUMANEVA ⋆⋆ Bo [9] Joint centers N/A 3 P HUMANEVA ⋆⋆ Ning [50] Stick-figure 10 3 P HUMANEVA † Rogez [62] Joint centers 10 2/3 P HUMANEVA ⋆ Urtasun [84] Joint centers N/A 3 P HUMANEVA ⋆⋆ Vondrak [88] Ellipsoids + prisms 13 3 T HUMANEVA ⋆⋆ ⋆ -Mean squared distance in 2D between the set of M = 15 (or fewer) virtual markers corresponding to the joint centers and limb ends. Measured in pixels (pix).\n\n, where mi(x) ∈ R 2 is the location of 2D marker i with respect to pose x. ⋆⋆ -Mean squared distance in 3D between the set of M = 15 virtual markers corresponding to the joint centers and limb ends. Measured in millimeters (mm).\n\n, where mi(x) ∈ R 3 is the location of 3D marker i with respect to pose x. † -Root mean square (RMS) error in joint angle. Measured in degrees (deg). Error units were in fractions of the subject's height. While only qualitative analysis of the overall tracking performance was presented, a quantitative analysis of the number of local minima in the posterior was performed.\n\nAdditional per-limb weighting was applied to downweight the error proportionally with the size of the limb. to project the 3D models into the images is not. Nevertheless, the video data has proved useful for the analysis of discriminative methods that do not estimate 3D body location e.g. [48]. In addition, the subjects are dressed in tight fitting motion capture suits and hence lack the realistic clothing variations exhibited in less controlled environments.\n\nThe CMU Motion of Body (MoBo) Database [27], initially developed for gait analysis, has also proved useful in analyzing the performance of articulated tracking algorithms [20,92]. While the initial dataset, which contains an extensive collection of walking motions, did not contain joint-level ground truth information, manually labeled data has been made available 6\n\nA more direct comparison of HUMANEVA to other datasets that are available to the community is given in Table 2.\n\nTo simultaneously capture video and motion information, our subjects wore natural clothing (as opposed to tightfitting motion capture suits typically used for pure motion capture sessions [15]) on which reflective markers were attached using invisible adhesive tape. 7 Our motivation was to obtain \"natural\" looking image data that contained all the complexity posed by moving clothing. One negative outcome of this is that the markers tend to move more than they would with a tight-fitting motion capture suit. As a result, our ground truth motion capture data may not always be as accurate as that obtained by more traditional methods; we felt that the trade-off of accuracy for realism here was acceptable. We have applied minimal post-processing to the motion capture data, steering away from the use of complex software packages (e.g. Motion Builder) that may introduce biases or alter the motion data in the process. As a result, motion capture data for some frames in some sequences are missing markers or are inaccurate. We made an effort to detect such cases and exclude them from the quantitative comparison. Note that the presence of markers on the body may also alter the natural appearance of the body. Given that the marker locations are known, it would be possible to provide a pixel mask in each image covering the marker locations; these pixels could then be excluded from further analysis. We felt this was unnecessary since the markers are often barely noticeable at video resolution and hence will likely have an insignificant impact on the performance of image-based tracking algorithms.\n\nWe have developed two datasets that we call HUMANEVA-I and HUMANEVA-II. HUMANEVA-I was captured earlier and is the larger of the two sets. HUMANEVA-II was captured using a more sophisticated hardware system that allowed better quality motion capture data and hardware synchronization. The differences between these two datasets are outlined in the Figure 1.\n\nSince all the data was captured in a laboratory setting, the sequences do not contain any external occlusions or significant clutter, but do exhibit the challenges imposed by strong illumination (e.g. strong shadows that tend to confuse background subtraction); grayscale cameras used in the HUMANEVA-I dataset present additional challenges when it comes to background subtraction and image features. Even at 60 Hz the images still exhibit a fair amount of motion blur.\n\nThe split of the training and test data was specifically designed to emphasize the ability of the pose and motion estimation approaches to generalize to novel subjects and unobserved motions. To this end, one subject and one motion for all subjects were withheld from the training and validation dataset for which ground truth is given out. We believe the proposed datasets exhibit a moderately complex and varied set of motions under realistic indoor imaging conditions that are applicable to most pose and motion estimation techniques proposed to date.\n\nHUMANEVA-I contains data from 4 subjects performing a set of 6 predefined actions in three repetitions (twice with video and motion capture, and once with motion capture alone). A short description of the actions is provided in Figure 1. Example images of a subject walking are shown in Figure 2 where data from 7 synchronized video cameras is illustrated with an overlay of ground truth body pose.\n\nGround truth motion of the body was captured using a commercial motion capture (MoCap) system from Vicon-Peakfoot_3 . The system uses reflective markers and six 1M-pixel cameras to recover the 3D position of the markers and thereby estimate the 3D articulated pose of the body.\n\nVideo data was captured using two commercial video capture systems. One from Spica Technology Corporationfoot_4 and one from IO Industriesfoot_5 . The Spica system captured video using four Pulnixfoot_6 TM6710 grayscale cameras (grayscale, progressive scan, 644x488 resolution, frame rate of up to 120 Hz). The IO Industries system used three UniQfoot_7 UC685CL 10-bit color cameras with 659x494 resolution and a frame rate of up to 110 Hz. The raw frames were re-scaled from 659x494 to 640x480 by IO Industries software. To achieve better image quality under natural indoor lighting conditions both video systems were set up to capture at 60 Hz. The rough relative placement of cameras is illustrated in Figure 1 (left).\n\nThe motion capture system and video capture systems were not synchronized in hardware, and hence a software synchronization was employed. The synchronization and calibration procedures are described in Sections 3.3 and 3.4 respectively.\n\nHUMANEVA-II contains only 2 subjects (both also appear in the HUMANEVA-I dataset) performing an extended sequence of actions that we call Combo. In this sequence a subject starts by walking along an elliptical path, then continues on to jog in the same direction and concludes with the subject alternatively balancing on each of the two feet roughly in the center of the viewing volume. Unlike HUMANEVA-I, this later dataset contains a relatively small test\n\nHUMANEVA-I HUMANEVA-II MoCap Hardware System Manufacturer ViconPeak ViconPeak Number of cameras 6 12 Camera resolution 1M-pixel MX13 1.3M-pixel Frame rate 120 Hz 120 Hz Video Capture System Color Cameras Number of cameras 3 4 Frame grabber IO Industries ViconPeak Camera model UniQ UC685CL Basler A602fc Sensor Progressive Scan Progressive Scan Camera resolution 659 x 494 pixels 656 x 490 pixels Frame rate 60 Hz 60 Hz Grayscale Cameras Number of cameras 4 Frame grabber Spica Tech Camera model Pulnix TM6710 Sensor Progressive Scan Camera resolution 644 x 448 pixels Frame rate 60 Hz Synchronization Software Hardware Data Actions (1) Walking, (2) Jogging, (3) Gesturing Combo (4) Throwing and Catching a ball, (5) Boxing, (6) Combo Number of subjects 4 2 Number of frames Training (synchronized) 6,800 frames Training (MoCap only) 37,000 frames Validation 6,800 frames Testing 24,000 frames 2,460 frames Capture Space Layout BW3 Control Station Capture Space C2 C3 C1 BW4 BW1 BW2 3 m 2 m C1 Control Station Capture Space C4 C3 C2 3 m 2 m Figure 1: HUMANEVA Datasets. The table illustrates the hardware system and configuration used to capture the two datasets, HUMANEVA-I and HUMANEVA-II. The main difference between the hardware systems lies in hardware synchronization employed in HUMANEVA-II. The contents of the two datasets in terms subjects, motion and amount of data are also noted. The bird's eye view sketch of the capture configuration is also shown with rough dimensions of the capture space and placement of video and motion capture cameras. The color video cameras (C) are designated by RGB stripped pattern, grayscale video cameras (BW) by the empty camera icon and motion capture cameras are denoted by gray circles. set of synchronized frames (≈ 2, 500). The HUMANEVA-I training and validation data is intended to be shared across the two datasets with test results primarily being reported on HUMANEVA-II.\n\nAs with HUMANEVA-I, the ground truth motion capture data was acquired using a system from ViconPeak. However, here we used a more recent Vicon MX system with twelve 1.3M-pixel cameras. This newer system produced more accurate motion capture data.\n\nVideo data was captured using a 4-camera reference system provided by ViconPeak which allowed for frameaccurate synchronization (using the Vicon MX Control module) of the video and motion capture data. Video was captured using four Baslerfoot_8 A602fc progressive scan cameras with 656x490 resolution operated at 60 Hz. The rough relative placement of cameras is illustrated in Figure 1 (right). A calibration procedure to align the Vicon and Basler coordinate systems is discussed in the next section.\n\nThe motion capture system was calibrated using Vicon's proprietary software and protocol. Calibration of the intrinsic parameters for the video capture systems was done using a standard checker-board calibration grid and the Camera Calibration Toolbox for Matlab [10]. Focal length (F c ∈ R 2 ), principle point (C c ∈ R 2 ) and radial distortion coefficients (K c ∈ R 5 ) were estimated for each camera c ∈ C. We assume square pixels and let the skew α c = 0 for all cameras c ∈ C.\n\nThe extrinsic parameters corresponding to the rotation, R c ∈ SO(3), and translation, T c ∈ R 3 , of the camera with respect to the global (shared) coordinate frame were solved for using a semi-automated procedure to align the global coordinate axis of each video camera with the global coordinate axis of the Vicon motion capture system. A single moving marker was captured by the video cameras and the motion capture system for a number of synchronized frames (> 1000). The resulting 3D tracked position of the marker Γ (3D) t , t ∈ {1 . . . T (3D) } was recovered using the Vicon software. The 2D position of the marker in the video, Γ (2D) t , t ∈ {1 . . . T (2D) }, was recovered using a Hough circle transform [29] that was manually initialized in the first frame and subsequently tracked. The projection of the 3D marker position f (Γ (3D) t ; R c , T c ) onto the image was then optimized directly for each camera by minimizing min Rc,Tc ,Ac,Bc\n\nfor the rotation, R c , and translation, T c . Note that the video cameras were calibrated with respect to the calibration parameters of the Vicon system, as opposed to from the images directly.\n\nIn the HUMANEVA-I dataset, the video and motion capture systems were not temporally synchronized in hardware, hence we also solved for the relative temporal scaling, A c ∈ R, between the video and Vicon cameras, and the temporal offset B c ∈ R. In doing so we assumed that the temporal scaling was constant over the length of a capture sequence 14(i.e. no temporal drift). The 3D position f (Γ (3D) tAc+Bc ; R c , T c ) was linearly interpolated to cope with non-integer indices tA c + B c . Finally, in Eq. ( 1), δ(t; A c , B c ) is defined as:\n\nThe calibration accuracy of the video cameras appears most accurate in the center of the viewing volume (close to the world origin).\n\nFor the HUMANEVA-II data, frame-accurate synchronization was achieved in hardware and we used fixed values A c = 2 and B c = 0 for the temporal scaling and offset.\n\nWhile the extrinsic calibration parameters and temporal scaling, A c , can be estimated once per camera (the Vicon system was only re-calibrated when cameras movedfoot_10 ), without hardware synchronization, the temporal offset B c was different for every sequence captured. To temporally synchronize the motion capture and the video in software, for HUMANEVA-I we manually labeled visible markers on the body for a small sub-set of images (6 images were used with several marker positions labeled per frame). These labeled frames were subsequently used in the optimization procedure above but with fixed values for R c , T c , and A c to recover a least squares estimate of the temporal offset B c for every sequence captured.\n\nVarious evaluation measures have been proposed for human motion tracking and pose estimation. For example, a number of papers have suggested using joint-angle difference as the error measure (see Table 1). This measure, however, assumes a particular parameterization of the human body and cannot be used to compare methods where the body models have different degrees of freedom or have different parameterizations of the joint angles. For this dataset we introduce a more widely applicable error measure based on a sparse set of virtual markers that correspond to the locations of joints and limb endpoints. This error measure was first introduced for 3D pose estimation and tracking in [74] and later extended in [7]. It has since been also used for 3D tracking in [41] and for 2D pose estimation evaluation in [36,75].\n\nLet x represent the pose of the body. We define M = 15 virtual markers as\n\n) is a function of the body pose that returns the position of the i'th marker in the world (or image respectively). Notice that defining functions m i (x) for any standard representation of the body pose x is trivial. The error between the estimated pose x and the ground truth pose x is expressed as the average Euclidean distance between individual virtual markers:\n\nTo ensure that we can compare algorithms that use different numbers of parts, we add a binary selection variable per-marker ∆ = { δ1 , δ2 , ..., δM } and obtain the final error function\n\nwhere δi = 1 if the algorithm is able to recover marker i, and 0 otherwise. For the sequence of T frames we compute the average performance using the following:\n\nSince many tracking algorithms are stochastic in nature, an average error and the standard deviation computed over a number of runs is most useful. As a convention from previous methods [7,36,74,75] that have already used this error measure, we compute the 3D error in millimeters (mm) and the 2D error directly in the image in pixels (pix).\n\nThe error measures formulated above are appropriate for measuring the performance of approaches that are able to recover the full 3D articulated pose of the person in space or the 2D articulated pose of the person in an image. Some approaches, however, are inherently developed to recover the pose but not the global position of the body (most discriminative approaches fall into this category, e.g. [2,48,76]). To make the above error measures appropriate for this class of approaches we employ a relative variant\n\nwith mi (x) = m i (x)m 0 (x), where m i (x) is defined as before and m 0 (x) is the position of the marker corresponding to the origin of the root segment. The rest of the equations can also be modified accordingly. It is worth noting that this measure assumes that the orientation of the body relative to the camera is recovered; this is typical of most discriminative methods. Note that the error measures assume that an algorithm returns a unique body pose estimate rather than a distribution over poses. For algorithms that model the posterior distribution over poses as uni-modal, the mean pose is likely to give a good estimate of x. Most recent methods, however, model multi-modal posterior distributions implicitly or explicitly.\n\nHere the maximum-a-posteriori estimate may be a more appropriate choice for x. This is discussed in greater detail in [7]. Alternative error measures that compute lower-bounds for sample-or kernel-based representations of the posterior are discussed in [7].\n\nIn addition to the datasets and quantitative evaluation measures, we provide a baseline algorithm 16 against which future advances can be measured. While no \"standard\" algorithm exists in the community, we implemented a fairly common Bayesian filtering method based on the methods of Deutscher and Reid [17] and Sidenbladh et al. [71]. Several variations on the base algorithm are explored with the goal of giving some insight into the important design choices for human trackers. Quantitative results are presented in the following section.\n\nWe pose the tracking problem in a standard way as one of estimating the posterior probability distribution p(x t |y 1:t ) for the state x t of the human body at time t given a sequence of image observations y 1:t ≡ (y 1 , . . . , y t ). Assuming a first-order Markov process p(x t |x\n\n1:t-1 ) = p(x t |x t-1 ) with a sensor Markov assumption p(y t |x 1:t , y 1:t-1 ) = p(y t |x t ), a recursive formula for the posterior can be derived [3, 19]: p(x t |y 1:t ) ∝ p(y t |x t ) p(x t |x t-1 )p(x t-1 |y 1:t-1 )dx t-1 .\n\nwhere the integral in Eq. 7 computes the prediction using the previous posterior and the temporal diffusion model p(x t |x t-1 ). The prediction is weighted by the likelihood p(y t |x t ) of the new image observation conditioned on the pose estimate.\n\nNon-parametric approximate methods represent posterior distributions by a set of N random samples or particles with associated normalized weights that are propagated over time using the temporal model and assigned new weights according to the likelihood function. This is the basis of the Sequential Importance Resampling (SIR) algorithm, or Condensation [3,31]. A variation of SIR is the Annealed Particle Filter (APF) introduced for human tracking by Deutscher and Reid [17]. An APF iterates these steps multiple times at each time instant in order to better localize the modes of the posterior distribution, and relies on simulated annealing to avoid local optima.\n\nWe briefly summarize our implementation of the Annealed Particle Filter algorithm used here since this forms the core of our baseline algorithm in the experiments that follow. The Sequential Importance Resampling algorithm is also tested in the following section but is not described in detail as it is similar to APF.\n\nAt each time instant the APF algorithm proceeds in a set of \"layers\", from layer M down to layer 1, that update the probability density over the state parameters. The state density at layer m + 1 is represented using a set of N particles with associated normalized weights S t,m+1 ≡ {x\n\nFor the prediction step at layer m, a Gaussian diffusion model is implemented (section 5.1.4). Specifically, hypotheses are drawn with replacement using Monte Carlo sampling from the state probability density at the previous layer m + 1 using\n\nThe sampling covariance matrix Σ controls the breadth of the search at each layer with a large Σ spreading sampled particles more widely. From layer to layer we scale Σ by a parameter α. This parameter is used to gradually reduce the diffusion covariance matrix Σ at lower layers in order to drive the particles towards the modes of the posterior distribution. Typically α is set to 0.5. Sampled poses that exceed the joint angle limits of the trained action model or result in inter-penetration of limbs are rejected and not re-sampled within a layer. The remaining particles are assigned new normalized weights based on an \"annealed\" version of the likelihood function (section 5.1.3)\n\nwhere β m is a temperature parameter optimized so that approximately half the particles get selected for propagation/diffusion to the next layer by the Monte-Carlo sampler (Eq. 8). The resulting particle set S t,m ≡ {x\n\nis then used to compute layer m -1 by re-applying Eqns. (8,9). In tracking, the top layer is initialized with the particle set of the bottom layer at the previous time instant: S t,M+1 = S t-1,1 .\n\nThe expected as well as the maximum a posteriori poses at frame t can be computed from the particle set S t,1 at the bottom layer using: SIR is a special case of APF which has only one annealing layer (M = 1) and for which the effect of the annealing temperature parameter is removed (β m = 1).\n\nAs is common in the literature, the skeleton of the body is modeled as a 3D kinematic tree with the limbs represented by truncated cones (Figure 4(b)). We consider 15 body parts: pelvis area, torso, head, upper and lower arms and legs, hands and feet. There are two types of parameters that describe the pose and shape of the body. The shape is given by the length and width of the limbs, which in our case are assumed known and fixed. Our objective is to recover the pose of the body, which is parametrized by a reduced set of 34 parameters comprising the global position and orientation of the pelvis and the relative joint angles between neighboring limbs. The hips, shoulders and thorax are modeled as ball and socket joints (3 DoF), the clavicles are allowed 2 DoFs, while the knees, ankles, elbows, wrists and head are assumed to be hinge joints with 1 DoF.\n\nThe subjects in the dataset were all manually measured using a standard Vicon protocol to obtain their height, weight, limb width and shoulder joint offsets. Motion capture training data was then used to estimate limb lengths for each subject as well as to learn static and dynamic priors for different motion styles. The raw data provided by the Vicon motion capture system consists of the location and orientation of local coordinate systems at each joint, with consecutive joints along the skeleton not constrained to be a fixed distance from each other. Limb lengths are computed as the median distance between pairs of corresponding joint locations over a large set of training motions and are kept fixed during testing. We also derive joint angle limits and inter-frame joint angle variations from the statistics of the relative joint angles between neighboring body parts.\n\nFor each particle in the posterior representation, its likelihood represents how well the projection of a given body pose fits the observed image(s). Many image features could be used, including appearance models and optical flow constraints, however, most common approaches rely on silhouettes and edges [17].\n\nEdge-based Likelihood Functions. We detect edges using image gradients that have been thresholded to obtain binary maps [17]. An edge distance map M e is then constructed for each image to determine the proximity of a pixel to an edge. This can be achieved by convolving the binary edge map with a Gaussian kernel, and then re-mapping it between 0 and 1. This can be thought of as representing the edge probability [17] at a given pixel.\n\nThe negative log-likelihood is then estimated by projecting into the edge map sparse points (for computational efficiency) along the apparent boundaries of all model parts and computing the mean square error (MSE) of the edge map responses:\n\nwhere {ξ e xt } is the set of pixel locations corresponding to all projected points (indexed by j) along all body part edges induced by pose x t , and M e t is the edge distance map at time t (Figure 4(c)). The reader is referred to [17] for a more detailed discussion.\n\nSilhouette-based Likelihood Function. Binary foreground silhouette maps M f t are generated using a learned Gaussian model for each pixel; the model is learned from 10 static background images and silhouettes subsequently obtained by comparing the background pixel probability to that of a uniform foreground model. We model the constraint that the silhouette of the body model should project inside the image silhouette. As before, for computational efficiency, we only check for a sparse number of points within the limbs (Figure 4(d)). The negative log-likelihood of the observations given pose x t is then estimated by taking a number of visible points inside all limbs and projecting them into the image {ξ f xt }. The MSE between the predicted and observed silhouette values for these points is computed [17]:\n\nBi-directional Silhouette-based Likelihood Function. The advantage of the previous silhouette likelihood formulation is computational efficiency and similarity to the edge-based likelihood formulation. However, this comes at the expense of being asymmetric: the body is constrained to lie inside the image silhouette, but not vice versa. This becomes a problem when the model predicts occluded parts and consequently does not fully cover the image silhouette.\n\nIn Figure 5(b) both legs track the same physical leg, but the penalty is minimal using p f (y t |x t ). We can correct this by defining a symmetric silhouette likelihood [80,81] that penalizes non-overlapping regions for both silhouettes. For this it is convenient to use a pixel-dense silhouette representation. Let M b t represent the binary silhouette map for the cylindrical body model and M f t the image foreground. Figure 5 (d) shows the overlap between the two silhouettes. We seek to minimize the non-overlapping regions, Red and Blue, therefore maximizing the Yellow region. The size of each region can be computed by summing over all image pixels p using\n\nThe negative log-likelihood of a pose is then defined as a linear combination of the fractions of each silhouette not explained by the other:\n\nWe make the likelihood symmetric by setting a = 0.5. When a is 0, we effectively get the behavior of the previous 1-sided silhouette likelihood p f (y t |x t ).\n\nCombining Likelihood Functions. We combine image measurements from multiple cameras or multiple likelihood formulations as follows:\n\nlog p(y\n\nwhere K is the number of cameras, y\n\nis the image observation in the k-th camera and L ⊂ {e, f, d} is a set of likelihood functions such as the ones in Eqns. (12,13,17).\n\nPredictions from the posterior are made using temporal models. The simplest model applicable to generic motions assumes no change in state from one time to the next: xt = x t-1 [17]. The predictions are diffused using normally distributed random noise to account for errors in the assumption. The noise is drawn from a multi-variate Gaussian with diagonal covariance Σ where the sampling standard deviation of each body angle is set to equal the maximum absolute inter-frame angular difference for a particular motion style [17].\n\nWe also implement a hard prior on individual poses to reduce the search space. Specifically, we reject (without resampling) any particle corresponding to an implausible body pose. We check for angles exceeding joint angle bounds and producing inter-penetrating limbs [79]. In our implementation we explicitly test for intersections between the torso and the lower arms and between the left and the right calves.\n\nWe use the term action model (AM) to denote the sampling covariance Σ used for particle filtering and the valid range of the joint angles. Action models can be learned specifically for a certain actor or for a particular motion style, or they can be generic. We only learned subject-generic action models by combining the data from all three available subjects in the training dataset.\n\nDifferent motion styles influence the sampling covariance and joint angle limits used. The training data in the HUMANEVA-I dataset contains walking, jogging, hand gestures, throwing and catching a ball, and boxing action styles from three different subjects. Subsets of these were used to learn style-specific action models. For example, the sampling covariance and the valid range of joint angles are typically smaller for walking than for jogging models, making the problem simpler for walking test sequences. For sequences containing both walking and jogging, it is typical for the flexion-extension movement of the elbow to cover disjoint angle intervals for the two styles. A combined action model for walking and jogging can be learned instead.\n\nTo represent a generic (style-independent) action model, we use the entire HUMANEVA-I training data to learn the sampling covariance Σ G . For the joint limits, our training data is not diverse enough to be suitable for discovering the full anatomical range of every joint angle, particularly for the leg joints. Instead we rely on standard anatomical joint limits (AJL).\n\nWe performed a series of experiments with the two different algorithms (APF and SIR), several likelihoods and various action models (priors); details of each variant are described along with the corresponding experiment. Most of these are variations of a base configuration (BC) that uses annealed filtering with 200 particles per layer, 5 layers of annealing, a likelihood based on bi-directional silhouette matching (BiS), and an action model appropriate for generic motions which enforces anatomical joint limits (G-AJL). We also reject samples where the limbs penetrate each other as described above. The experiments were conducted on the two sequences in the HUMANEVA-II dataset. In each case, ground truth was available in the first frame to initialize the tracker.\n\nThe error of an individual pose was computed using Eq. ( 4) which averages the Euclidean distance between virtual markers placed as shown in Figure 4(b). Given the samples (particles) at each frame, we computed the error of the expected pose using Eq. ( 10). This is appropriate for the APF since we expect the posterior representation to be unimodal at the bottom layer of annealing. Alternatively we could have estimated the error of the most likely pose in the posterior distribution. In our experiments we found this measure to be consistently worse than the error of the expected pose by an average of 2 mm with noisier reconstructed joint angle trajectories. We attribute this to the fact that particle filtering methods represent the posterior probability as a function of both the likelihood weights and the density of particles. The MAP estimate may miss a region that has a high posterior probability due to high particle density but small individual weights.\n\nOur optimization strategy is stochastic in nature and produces different results when running experiments with the same configuration parameters. To get a measure of performance consistency and repeatability, we ran each experiment five times for each of the sequences, unless explicitly noted otherwise. We plot the mean of our error measure (3D or 2D depending on the experiment) for each time instant over all the runs, while for the BC we also highlight the standard deviation as a gray overlay in Figures 6,9,10,11,12,13, and in the corresponding rows in the error tables.\n\nThe errors at each frame are combined to compute the average error µ seq (Eq. 5) for each of the three activity phases (walking, jogging and leg balancing), as well as the overall error over the two sequences. We report the mean and standard deviation of the average error µ seq over multiple runs.foot_12 Computation time. The computation time is directly proportional to the number of particles, number of camera views and number of layers used, and vastly depends on the choice of likelihood function. Performing full inference using the one-sided silhouette likelihood p f (y t |x t ) jointly with the edge likelihood p e (y t |x t ) with 1,000 particles per frame for 4 camera views takes about 40 seconds on a standard PC with software written in Matlab, as opposed to 250 seconds when using the bi-directional silhouette likelihood p d (y t |x t ). Likelihood evaluations dominate the overall computation time; particle diffusion and checking for limb inter-penetration are relatively insignificant by comparison.\n\nSample tracking results overlaid on the images using the BC are shown in Figures 14 and 15, and illustrate visually what different amounts of error correspond to. The 5 runs of BC suggest that the tracking performance is fairly stable across runs. This is illustrated in Figure 6 for 3D errors. Performance results using other error measures are included in Figure 7 to allow easy comparison with other methods.\n\nThe occasional spikes in the error correspond to the tracker losing track of the arms or the legs swapping places (e.g. frame 656 in Fig. 14). Since walking and jogging are periodic motions, the arms and legs are usually found again during the next cycle. This is also illustrated when investigating the error for individual joints. Figure 8 shows that limb extremities are the hardest to track. Large errors for the wrists are obtained when the elbow becomes fully flexed and gets stuck in this position (e.g. frame 1030 in Fig. 15). From trial to trial, these events may or may not happen due to the stochastic nature of the optimization, making the error variance in these cases higher (identified in the plots in Fig. 6 as spikes in the gray overlay).\n\nThe results also highlight the relative degree of difficulty of the two sequences. They are relatively similar, except for the jogging phase where the second sequence is significantly more difficult to track than the first and presents a\n\n200 400 600 800 1000 1200 0 50 100 150 200 250 300 350 Error (mm) Walking Phase Jogging Phase Balancing Phase Sequence 1 200 400 600 800 1000 1200 0 50 100 150 200 250 300 350 Error (mm) Walking Phase Jogging Phase Balancing Phase Sequence 2 W-AM WJ-AM G-AM AJL G-AM 0JL Sequence 1 Sequence 2 Overall Walk Jog Balance Walk Jog Balance W 65±7 mm 88±4 mm 140±16 mm 55±4 mm 138±43 mm 97±7 mm 97±9 mm WJ 66±11 mm 71±4 mm 122±14 mm 58±1 mm 79±3 mm 102±3 mm 83±3 mm G-AJL 76±5 mm 85±4 mm 86±11 mm 60±3 mm 93±19 mm 80±20 mm 80±5 mm G-0JL 115 mm 182 mm 245 mm 76 mm 143 mm 135 mm 149 mm For the generic action model, the joint angle limits were not derived from training data as in the case of walking or jogging, but rather were set to standard anatomical joint limits (AJL). We use 0JL to denote when joint limits were not enforced. In this case performance degraded considerably and we only show results for one run. For W, WJ and G-AJL results were averaged over 5 different runs to more effectively compare different action models.\n\nproduct of the number of layers ( 5) and the number of particles per layer (200) in the annealing method. A comparison of the methods is shown in Figure 11.\n\nIn contrast to the APF, the SIR could maintain multi-modal posterior distributions in which case computing the error of the expected pose might not be appropriate. Therefore we also report the error of the most likely particle (MAP). We found, however, that the difference in error between the expected pose and the most likely pose was insignificant, and the error curves overlapped. Either way, relative to APF, SIR was significantly worse and more prone to losing track of the subject during fast motions.\n\nVarying the Number of Particles. We also varied the number of particles used in the baseline configuration. Using more particles helps prevent the tracker from losing track and improves performance. The tracker is much more stable when run using 200 particles. Using 100 particles or fewer makes the tracker unstable as illustrated by the significant increase in error variance in Figure 12. Based on these results we conclude that the number of particles needed depends on the type of motion being tracked, with more particles being needed for fast motions than for slow motions.\n\nVarying the Number of Camera Views. The HUMANEVA-II dataset used 4 cameras placed on the corners of a rectangular area as shown in Figure 1. Assessing performance for different number of cameras depends on the choice of cameras. We ran experiments with BC for all subsets of cameras, once for each camera configuration, combining the errors for configurations with the same number of cameras. Mean errors and standard deviations are reported in Figure 13 over 4 configurations for the one camera case, 6 pairs and 4 triples, respectively.\n\nThe results clearly show that monocular tracking is beyond the abilities of the present algorithm. Adding a second camera view significantly improved the results but still the tracker could not cope with simple walking motions. At least 3 camera views were needed to track walking motions and 4 were needed for more complex motions such as jogging. For walking motions there was no statistical difference between using 3 or 4 camera views.\n\n200 400 600 800 1000 1200 0 50 100 150 200 250 300 350 Error (mm) Walking Phase Jogging Phase Balancing Phase Sequence 1 200 400 600 800 1000 1200 0 50 100 150 200 250 300 350 Error (mm) Walking Phase Jogging Phase Balancing Phase Sequence 2 EG SH EG+SH BiSH Sequence 1 Sequence 2 Overall Walk Jog Balance Walk Jog Balance E a 315 mm 1367 mm 1298 mm 116 mm 530 mm 344 mm 662 mm S 184±20 mm 229±32 mm 253±90 mm 105±22 mm 229±10 mm 233±77 mm 205±8 mm E+S 118±32 mm 216±43 mm 265±133 mm 75±7 mm 199±24 mm 356±116 mm 205±22 mm BiS 76±5 mm 85±4 mm 86±11 mm 60±3 mm 93±19 mm 80±20 mm 80±5 mm E+S b 90 mm 95 mm 161 mm 68 mm 100 mm 132 mm 111 mm a Performance with the edge likelihood alone was so poor that we only show results for a single run. b This experiment was run once and differs from BC in that it uses the WJ action model with only 100 particles instead of the G-AJL action model with 200 particles. Its plot is not shown in the graph above. The bi-directional model is more computationally expensive, but it is the only one able to completely track the subject using a generic prior. The E+S model is shown to be competitive when combined with a stronger prior that matches the test motion.\n\nModel. Our model of the body is an approximation to the true human body shape (though it is fairly typical of the state of the art). We make two key assumptions that (1) the body is made of rigid cylindrical or conical segments and (2) joints only model the most significant degrees of freedom. We make no attempts to fit the shape of the limbs to the image measurements. More accurate body models may lead to more accurate tracking results but this hypothesis needs to be verified experimentally. Also, a more anatomically correct modeling of the DoF of the joints may be required for applications in bio-mechanics [47].\n\nImage likelihoods. One of the main observations of our experiments with the baseline algorithm is that results of the approach heavily rely on the quality of the likelihood model. It is our belief that one of the key problems in human motion tracking is the formulation of reliable image likelihood models that are general, do not require background subtraction, and can be applied over a variety of imaging and lighting conditions. We have implemented relatively standard likelihood measures, however, other likelihoods have been proposed and should be evaluated.\n\nFor example, more principled edge likelihoods have been formulated using measurable model edge segments [90], phase information [57] and the learned statistics of filter responses [66,70]. Non-edge-based likelihood measures include optical flow [11,73], flow occlusion/disocclusion boundaries [79], segmented silhouettes based on level sets [65], image templates [89], spatio-temporal templates [18], principal component-based models of appearance [72], and robust on-line local [6,85] and global appearance models [6].\n\nMotion priors. While the action models used for diffusion within our framework work relatively well in a multiview setting, it is likely that monocular tracking can benefit from stronger prior models of human motion. The use of strong 18 prior motion models are common with early work concentrating on switching dynamical models [54] and 200 400 600 800 1000 1200 0 50 100 150 200 250 300 350 Error (mm) Walking Phase Jogging Phase Balancing Phase Sequence 1 200 400 600 800 1000 1200 0 50 100 150 200 250 300 350 Error (mm) Walking Phase Jogging Phase Balancing Phase Sequence 2 SIR SIR (MAP) APF Sequence 1 Sequence 2 Overall Walk Jog Balance Walk Jog Balance SIR 101±6 mm 178±36 mm 251±204 mm 75±4 mm 201±35 mm 188±148 mm 166±50 mm SIR (MAP) 101±6 mm 176±36 mm 254±203 mm 76±5 mm 198±35 mm 190±147 mm 166±49 mm APF 76±5 mm 85±4 mm 86±11 mm 60±3 mm 93±19 mm 80±20 mm 80±5 mm Figure 11: Algorithm comparison. Performance of the Annealed Particle Filter (APF) and Sequential Importance Resampling (SIR) methods is shown. SIR performed significantly worse than APF and started diverging during jogging, which affected the performance during the balancing phase.\n\neigen-models of cyclic motions [51,52,73]. More recently, motion priors that utilize latent spaces as a means of modeling classes of motions that are inherently low-dimensional in nature have become popular. Low-dimensional non-linear latent variable priors were first (to our knowledge) introduced in [77] and later extended in [42]; Gaussian Processes Latent Variable Models [86], Gaussian Processes Dynamical Models [85] and Factor Analyzers [41] are popular and effective choices particularly for instances where little training data is available. Weaker implicit priors that utilize motion capture data directly [71] have also been effective. Lastly, priors based on abstracted [12] or full-body [88] physical simulations recently have been proposed for specific classes of motions (e.g. walking).\n\nInference. While we explored two inference algorithms, SIR and APF, other promising methods do exist and may lead to more robust or faster performance. For example, hybrid Monte Carlo sampling [57], partitioned sampling [43], or covariance-scaled sampling [79] are all promising alternatives. Kalman filtering [90] is another alternative that may be appropriate for the applications where one can ensure that the likelihood and the dynamics are uni-modal.\n\nWe have observed that it is generally harder to track the upper body, due to frequent occlusions between the arms and the torso. We attribute these difficulties to the poor likelihood functions that are not able to effectively model internal structure within the silhouette region. The upper body also tends to exhibit more stylistic variation across people; the lower body must provide support and hence is more constrained by the dynamics of the motion itself.\n\nThe infrequent failures of the baseline algorithm can be classified into two categories: (1) minor tracking failures for individual body parts and (2) 180-degree rotation in the overall body pose; the latter is much harder to recover from in practice. We suspect these failures at least to some extent can be attributed to the nature of annealing which may not represent multi-modal distributions in the posterior effectively.\n\nWe have introduced a dataset for evaluation of human pose estimation and tracking algorithms. This is a comprehensive dataset that contains synchronized video from multiple camera views, associated 3D ground truth, quantitative evaluation measures, and a baseline human tracking algorithm. All the data and associated software is made freely\n\n200 400 600 800 1000 1200 0 50 100 150 200 250 300 350 Error (mm) Walking Phase Jogging Phase Balancing Phase Sequence 1 200 400 600 800 1000 1200 0 50 100 150 200 250 300 350 Error (mm) Walking Phase Jogging Phase Balancing Phase Sequence 2 50 particles 100 particles 200 particles Sequence 1 Sequence 2 Overall Walk Jog Balance Walk Jog Balance 50 particles 88±18 mm 133±45 mm 144±73 mm 70±5 mm 180±41 mm 313±173 mm 155±42 mm 100 particles 83±10 mm 109±14 mm 121±49 mm 63±3 mm 123±65 mm 154±156 mm 109±45 mm 200 particles 76±5 mm 85±4 mm 86±11 mm 60±3 mm 93±19 mm 80±20 mm 80±5 mm available to the research community. 19 We hope that this dataset will lead to further advances in articulated human motion estimation as well as provide the means of establishing the state of the art performance of current algorithms.\n\nWhile not new, the baseline algorithm, in addition to providing performance against which future advances on this data can be measured, is designed to serve as a test-bed for future experiments with likelihood functions, prior models and inference methods within the context of Bayesian filtering. We found that the annealed particle filter with 5 layers and 200 particles per layer worked reliably in practice (better than SIR) and that four camera views were necessary for stable tracking. Furthermore we found that the bi-directional silhouette likelihood performed significantly better than the edges and/or standard silhouettes. A fairly weak (generic) \"prior\" (embodied here as the sampling covariance) that enforced anatomic joint limits and non-interpenetration of parts worked well across activities; stronger models should be explored.\n\nWhile we treat the marker-based motion capture data as the \"ground truth\", it is worth noting that the true human motion is somewhat elusive. Even with perfect marker-based motion capture data, deriving the location of joints in the human body is not a trivial task. For example, hip joints are not well defined and can only be measured to about 2 -10 (mm) accuracy given the marker protocol employed by the Vicon system [13]. The true gold standard in localizing the position of hip joints is still debated in the bio-mechanics literature [16]. The placement of markers over regular clothes and limits on the calibration accuracy of the video cameras with respect to the Vicon calibration may lead to additional errors that are hard to quantify. While currently unavailable, it is clear that other methods of simultaneously capturing video and motion capture data are necessary if not to allow better ground truth, then to at least lift the need of performing the motion in a laboratory environment. Current research in non-marker-based methods for capturing human motion [87] may prove to be viable alternatives in a few years.\n\nAcknowledgements. This project was supported in part by gifts from Honda Research Institute and Intel Corporation. Funding for portions of this work was also provided by NSF grants IIS-0534858 and IIS-0535075. We would like to thank Ming-Hsuan Yang, Rui Li, Payman Yadollahpour and Stefan Roth for help in data collection and postprocessing. We also would like to thank Stan Sclaroff for making the color video capture equipment available for this effort.\n\nWhile the workshops did not have any printed proceedings, submissions can be viewed on-line: http://www.cs.brown.edu/people/ls/ehum/ http://www.cs.brown.edu/people/ls/ehum2/ 5 http://www.cc.gt.atl.ga.us/grads/w/Ping.Wang/Project/FigureTracking.html\n\nhttp://www.cs.cmu.edu/ ˜zhangjy/\n\nParticipation in the collection process was voluntary and each subject was required to read, understand, and sign an Institutional Review Board (IRB) approved consent form for collection and distribution of data. A copy of the consent form for the \"Video and Motion Capture Project\" is available by writing to the authors. Subjects were informed that the data, including video images, would be made available to the research community and could appear in scientific publications.\n\nhttp://www.vicon.com/\n\nhttp://www.spicatek.com/\n\nhttp://www.ioindustries.com/\n\nhttp://www.pulnix.com/\n\nhttp://www.uniqvision.com/\n\nhttp://www.baslerweb.com/\n\nIn practice Ac ≈ 2 since the frame rate of motion capture system was roughly 120 Hz and video system is 60 Hz.\n\nCalibration of the Vicon motion capture system changes the global coordinate frame and hence requires re-calibration of extrinsic parameters of the video cameras as well.\n\nThe implementation is available for download from http://vision.cs.brown.edu/humaneva/baseline.html\n\nWhen computing the results, we ignore 38 frames (298-335) for sequence 2 where accurate ground truth was not available. The apparent gap in the error plots during the walking phase is a result of this.\n\nBy strong prior motion models here we mean models that bias inference towards a particular pre-defined class of motions.\n\nData and code available at http://vision.cs.brown.edu/humaneva/"
}