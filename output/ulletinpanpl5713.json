{
    "title": "The art and science of large-scale disasters",
    "publication_date": "2008",
    "authors": [
        {
            "full_name": "M Gad-El-Hak",
            "firstname": "M",
            "lastname": "Gad-El-Hak",
            "affiliations": [
                {
                    "organization": "Department of Mechanical Engineering, Virginia Commonwealth University",
                    "address": {
                        "city": "Richmond",
                        "country": "U.S.A",
                        "postcode": "23284-3015"
                    }
                }
            ]
        }
    ],
    "abstract": "The subject of large-scale disasters is broadly introduced in this article. Both the art and science of predicting, preventing and mitigating natural and manmade disasters are discussed. A universal, quantitative metric that puts all natural and manmade disasters on a common scale is proposed. Issues of prediction, control and mitigation of catastrophes are presented. The laws of nature govern the evolution of any disaster. In some cases, as for example weather-related disasters, the first-principles laws of classical mechanics could be written in the form of field equations, but exact solutions of these often nonlinear differential equations are impossible to obtain particularly for turbulent flows, and heuristic models together with intensive use of supercomputers are necessary to proceed to a reasonably accurate forecast. In other cases, as for example earthquakes, the precise laws are not even known and prediction becomes more or less a black art. Management of any type of disaster is more art than science. Nevertheless, much can be done to alleviate the resulting pain and suffering. The expansive presentation of the broad field of large-scale disasters precludes a detailed coverage of any one of the many topics touched upon. Three take-home messages are conveyed, however: a universal metric for all natural and manmade disasters is presented; all facets of the genre are described; and a proposal is made to view all disasters as dynamical systems governed for the most part by the laws of classical mechanics.",
    "full_text": "In this article, the subject of large-scale disasters is broadly introduced. Both the art and science of predicting, preventing and mitigating natural and manmade disasters are discussed. A universal, quantitative metric that puts all natural and manmade disasters on a common scale is proposed. Issues of prediction, control and mitigation of catastrophes are presented. The expansive presentation of the many facets of disaster research precludes a detailed coverage of any one of the many topics covered. We merely scratch the surface of a broad subject that may be of interest to all those who view the world mechanistically. The hope is that few readers of Bulletin of the Polish Academy of Sciences who are not already involved in disaster research would want to be engaged in this exciting endeavor whose practical importance cannot be overstated. The article is excerpted from Chapter 2 of the book edited by Gad-el-Hak [1].\n\nAlthough it appears that way when the past few years are considered, large-scale disasters have been with us since Homo sapiens set foot on this third planet from the Sun. Frequent disasters struck the Earth even before then, as far back as the time of its formation around 4.5 billion years ago. In fact, the geological Earth that we know today is believed to be the result of agglomeration of the so-called planetesimals and subsequent impacts of bodies of similar mass [2]. The planet was left molten after each giant impact, and its outer crust was formed on radiative cooling to space. Those were the \"good\" disasters perhaps. On the bad side, there have been several mass extinctions throughout the Earth's history. The dinosaurs, along with about 70% of all species existing at the time, became extinct because a large meteorite struck the Earth 65 million years ago and the resulting airborne dust partially blocked the Sun, thus making it impossible for cold-blooded animals to survive. However, if we concern ourselves with our own warm-blooded species, then starting 200,000 years ago, ice ages, famines, infections, and attacks from rival groups and animals were constant reminders of human vulnerability. On average, there are about three large-scale disasters that strike the Earth every day, but only a few of these natural or manmade calamities make it to the news. Humans have survived because we were programmed to do so. We return to this point in Section [7].\n\nBecause of the nature of the subject, few of the topics discussed are not mainstream for this journal, for example the sociological and political aspects of disasters. The mechanics of disasters are more extensively covered, but even here we begin the conversation rather than actually solving specific problems. Appropriate references are made, however, to close the gap.\n\nThe article is organized as follows. We begin by proposing a metric by which disasters are sized in terms of the number of people affected and/or the extent of the geographic area involved. In Section 3, the different facets of largescale disasters are described. The science, particularly the mechanics, of disasters is outlined in Section 4. Global Earth Observation System of Systems is briefly described in Section 5. Sections 6-8 respectively cover the art of disaster management, a bit of sociology, and few recent disasters as ex-amples. Finally, brief concluding remarks are given in Section 9.\n\nThere is no easy answer to the question of whether a particular disaster is large or small. The mild injury of one person may be perceived as catastrophic by that person or by his or her loved ones. What we consider herein, however, is the adverse effects of an event on a community or an ecosystem. What makes a disaster a large-scale one is the number of people affected by it and/or the extent of the geographic area involved. Such disaster taxes the resources of local communities and central governments. Under the weight of a large-scale disaster, a community diverges substantially from its normal social structure. Return to normalcy is typically a slow process that depends on the severity, but not the duration, of the antecedent calamity as well as the resources and efficiency of the recovery process.\n\nThe extreme event could be natural, manmade, or a combination of the two in the sense of a natural disaster made worse by human's past actions. Examples of naturally occurring disasters include earthquakes, wildfires, pandemics, volcanic eruptions, mudslides, floods, droughts, and extreme weather phenomena such as ice ages, hurricanes, tornadoes, and sandstorms. Human foolishness, folly, meanness, mismanagement, gluttony, unchecked consumption of resources, or simply sheer misfortune may cause war, energy crisis, economic collapse of a nation or corporation, market crash, fire, global warming, famine, air/water pollution, urban sprawl, desertification, deforestation, bus/train/airplane/ship accident, oil slick, or terrorist act. Citizens suffering under the tyranny of a despot or a dictator can also be considered a disaster too, and, of course, genocide, ethnic cleansing and other types of mass murder are gargantuan disasters that often test the belief in our own humanity. Although technological advances exponentially increased human prosperity, they also provided humans with more destructive power. Manmade disasters have caused the death of at least 200 million people during the twentieth century, a cruel age without equal in the history of man [3].\n\nIn addition to the degree or scope of a disaster, there is also the issue of the rapidity of the calamity. Earthquakes, for example, occur over extremely short time periods measured in seconds, whereas anthropogenic catastrophes such as global warming and air and water pollution are often slowly-evolving disasters, their duration measured in years and even decades or centuries, although their devastation, over the long term, can be worse than that of a rapid, intense calamity [4]. The painful, slow death of a cancer patient who contracted the dreadful disease as a result of pollution is just as tragic as the split-second demise of a human at the hands of a crazed suicide bomber. The latter type of disaster makes the news, but the former does not. This is quite unsettling because the death of many spread over years goes unnoticed for the most part. The fact that 100 persons die in a week in a particular country as a result of starvation is not a typical news story. However, 100 humans perishing in an airplane crash will make CNN all day.\n\nFor the disaster's magnitude, how large is large? Much the same as is done to individually size hurricanes, tornadoes, earthquakes, and, very recently, winter storms, we propose herein a universal metric by which all types of disaster are sized in terms of the number of people affected and/or the extent of the geographic area involved. This quantitative scale applies to both natural and manmade disasters. The suggested scale is nonlinear, logarithmic in fact, much the same as the Richter scale used to measure the severity of an earthquake. Thus, moving up the scale requires an order of magnitude increase in the severity of the disaster as it adversely affects people or an ecosystem. Note that a disaster may affect only a geographic area without any direct and immediate impact on humans. For example, a wildfire in an uninhabited forest may have long-term adverse effects on the local and global ecosystem, although no human is immediately killed, injured, or dislocated as a result of the event.\n\nThe scope of a disaster is determined if at least one of two criteria is met, relating to either the number of displaced/tormented/injured/killed people or the adversely affected area of the event. We classify disaster types as being of Scopes I to V, according to the scale pictorially illustrated in Figure 1. For example, if 70 persons were injured as a result of a wildfire that covered 20 km 2 , this would be considered Scope III, large disaster (the larger of the two categories II and III). However, if 70 persons were killed as a result of a wildfire that covered 2 km 2 , this would be considered Scope II, medium disaster. An unusual example, at least in the sense of even attempting to classify it, is the close to 80 million citizens of Egypt (area slightly larger than 1 million sq. km) who have been tormented for more than a half-centuryfoot_0 by a virtual police state. This manmade cataclysm is readily stigmatized by the highest classification, Scope V, gargantuan disaster. The quantitative metric introduced herein is contrasted to the conceptual scale devised by Fischer [5,6], which is based on the degree of social disruption resulting from an actual or potential disaster. His ten disaster categories are based on the scale, duration, and scope of disruption and adjustment of a normal social structure, but those categories are purely qualitative. For example, Disaster Category 3 (DC-3) is indicated if the event partially strikes a small town (major scale,\n\nThe art and science of large-scale disasters major duration, partial scope), whereas DC-8 is reserved for a calamity massively striking a large city (major scale, major duration, major scope).\n\nThe primary advantage of having a universal classification scheme such as the one proposed herein is that it gives officials a quantitative measure of the magnitude of the disaster so that proper response can be mobilized and adjusted as warranted. The metric suggested applies to all types of disaster. It puts them on a common scale, which is more informative than the variety of scales currently used for different disaster types; the Saffir-Simpson scale for hurricanes, the Fujita scale for tornadoes, the Richter scale for earthquakes, and the recently introduced Northeast Snowfall Impact Scale (notable, significant, major, crippling, extreme) for the winter storms that occasionally strike the northeastern region of the United States. Of course, the individual scales also have their utility; for example, knowing the range of wind speeds in a hurricane as provided by the Saffir-Simpson scale is a crucial piece of information to complement the number of casualties the proposed scale supplies. In fact, a prediction of wind speed allows estimation of potential damage to people and property. The proposed metric also applies to disasters, such as terrorist acts or droughts, where no quantitative scale is otherwise available to measure their severity.\n\nIn formulating all scales, including the proposed one, a certain degree of arbitrariness is unavoidable. In other words, none of the scales is totally objective. The range of 10 to 100 persons associated with a Scope II disaster, for example, could very well be 20 to 80, or some other range. What is important is the relative comparison among various disaster degrees; a Scope IV disaster causes an order of magnitude more damage than a Scope III disaster, and so on. One could arbitrarily continue beyond five categories, always increasing the influenced number of people and geographic area by an order of magnitude, but it seems that any calamity adversely affecting more than 10,000 persons or 1,000 km 2 is so catastrophic that a single Scope V is adequate to classify it as a gargantuan disaster. The book Catastrophe is devoted to analyzing the risk of and response to unimaginable but not impossible calamities that have the potential of wiping out the human race [7]. Curiously, its author, Richard A. Posner, is a judge in the U.S. Seventh Circuit Court of Appeals.\n\nIn the case of certain disasters, the scope can be predicted in advance to a certain degree of accuracy; otherwise, the scope can be estimated shortly after the calamity strikes with frequent updates as warranted. The magnitude of the disaster should determine the size of the first-responder contingency to be deployed; which hospitals to mobilize and to what extent; whether the military forces should be involved; what resources, such as, food, water, medicine, and shelter should, be stockpiled and delivered to the stricken area, and so on. Predicting the scope should facilitate the subsequent recovery and accelerate the return to normalcy. The proposed metric is systematically applied in Section 8. 13 to the twelve examples of disasters presented in Sections 8.1-8.12.\n\nA large-scale disaster is an event that adversely affects a large number of people, devastates a large geographic area, and taxes the resources of local communities and central governments. Although disasters can naturally occur, humans can cause their share of devastation. There is also the possibility of human actions causing a natural disaster to become more damaging than it would otherwise. An example of such an anthropogenic calamity is the intense coral reef mining off the Sri Lankan coast, which removed the sort of natural barrier that could mitigate the force of waves. As a result of such mining, the 2004 Pacific tsunami devastated Sri Lanka much more than it would have otherwise. A second example is the soil erosion caused by overgrazing, farming, and deforestation. In April 2006, wind from the Gobi Desert dumped 300,000 tons of sand and dust on Beijing, China. Such gigantic dust tempests-exasperated by soil erosion-blow around the globe, making people sick, killing coral reefs, and melting mountain snow packs continents away. Examples such as this incited the 1995 Nobel laureate and Dutch chemist Paul J. Crutzen to coin the present geological period as anthropocene to characterize humanity's adverse effects on global climate and ecology <http://www.mpch-mainz.mpg.de/ air/anthropocene/>.\n\nWhat could make the best of a bad situation is to be able to predict the disaster's occurrence, location, and severity. This can help prepare for the calamity and evacuating large segments of the population out of harm's way. For certain disaster types, their evolution equations can be formulated mostly from a mechanistic viewpoint. Predictions can then be made to different degrees of success using heuristic models, empirical observations, and giant computers. Once formed, the path and intensity of a hurricane, for example, can be predicted to a reasonable degree of accuracy up to 1 week in the future. This provides sufficient warning to evacuate several medium or large cities in the path of the extreme event. However, smaller-scale severe weather such as tornadoes can only be predicted up to 15 minutes in the future, giving very little window for action. Earthquakes cannot be predicted beyond stating that there is a certain probability of occurrence of a certain magnitude earthquake at a certain geographic location during the next 50 years. Such predictions are almost as useless as stating that the Sun will burn-out in a few billion years.\n\nOnce disaster strikes, mitigating its adverse effects becomes the primary concern: how to save lives, take care of the survivors' needs, and protect properties from any further damage. Dislocated people need shelter, water, food, and medicine. Both the physical and the mental health of the survivors, as well as relatives of the deceased, can be severely jeopardized. Looting, price gouging, and other law-breaking activities need to be contained, minimized, or eliminated. Hospitals need to prioritize and even ration treatments, especially in the face of the practical fact that the less seriously injured tend to arrive at emergency rooms first, perhaps because they transported themselves there. Roads need to be operable and free of landslides, debris, and traffic jams for the unhindered flow of first responders and supplies to the stricken area, and evacuees and ambulances from the same. This is not always the case, especially if the antecedent disaster damages most if not all roads as occurred after the 2005 Kashmir Earthquake. Buildings, bridges, and roads need to be rebuilt or repaired, and power, potable water, and sewage need to be restored.\n\nFigure 2 depicts the different facets of large-scale disasters. The important thing is to judiciously employ the finite resources available to improve the science of disaster prediction, and to artfully manage the resulting mess to minimize loss of life and property.\n\nScience, particularly classical mechanics, can help predict the course of certain types of disaster. When, where, and how intense would a severe weather phenomena strike? Are the weather conditions favorable for extinguishing a particular wildfire? What is the probability of a particular volcano erupting? How about an earthquake striking a population center? How much air and water pollution is going to be caused by the addition of a factory cluster to a community? How would a toxic chemical or biological substance disperse in the atmosphere or in a body of water? Below a certain concentration, certain danger substances are harmless, and \"safe\" and \"dangerous\" zones could be established based on the dispersion forecast. The degree of success in answering these and similar questions varies dramatically. Once formed, the course and intensity of a hurricane (tropical cyclone), which typically lasts from inception to dissipation for a few weeks, can be predicted about one week in advance. The path of the much smaller and short-lived, albeit more deadly, tornado can be predicted only about 15 minutes in advance, although weather conditions favoring its formation can be predicted a few hours ahead.\n\nEarthquake prediction is far from satisfactory but is seriously attempted nevertheless. The accuracy of predicting volcanic eruptions is somewhere in between those of earthquakes and severe weather. Patanè et al. [8] report on the ability of scientists' to 'see' inside Italy's Mount Etna and forecast its eruption using seismic tomography, a technique similar to that used in computed tomography scans in the medical field. The method yields time photographs of the three-dimensional movement of rocks to detect their internal changes. The success of the technique is in no small part due to the fact that Europe's biggest volcano Mount Etna is equipped with a highquality monitoring system and seismic network, tools that are not readily available for most volcanoes.\n\nScience and technology can also help control the severity of a disaster, but here the achievements to date are much less spectacular than those in the prediction arena. Cloud seeding to avert drought is still far from being a routine, practical tool. Nevertheless it has been tried since 1946. In 2008, Los Angeles county officials used the technique as part of a droughtrelief project that used silver iodide to seed clouds over the San Gabriel Mountains to ward off fires. China employed the same technology to bring some rain and clear the air before the 2008 Beijing Summer Olympics. Despite the difficulties, cloud seeding is still a notch more rational than the then Governor of Texas George W. Bush's 1999 call in the midst of a dry period to \"pray for rain\".\n\nSlinging a nuclear device toward an asteroid or a meteor to avert its imminent collision with Earth remains solidly in the realm of science fiction (in the 1998 film Armageddon, a Texas-size asteroid was courageously nuked from its interior!). In contrast, employing scientific principles to combat a wildfire is doable, as is the development of scientifically based strategies to reduce air and water pollution; moderate urban sprawl; evacuate a large city; and minimize the probability of accident for air, land, and water vehicles. Structures could be designed to withstand an earthquake of a given magnitude, wind of a given speed, and so on. Dams could be constructed to moderate the flood-drought cycles of rivers, and levees/dikes could be erected to protect land below sea level from the vagaries of the weather. Storm drains; fire hydrants; fire-retardant materials; sprinkler systems; pollution control; simple hygiene; strict building codes; traffic rules and regulations in air, land and sea; and many other examples are the measures a society should take to mitigate or even eliminate the adverse effects of certain natural and manmade disasters. Of course, there are limits to what we can do. Although much better fire safety will be achieved if a firehouse is erected, equipped, and manned around every city block, and less earthquake casualties will occur if every structure is built to\n\nThe art and science of large-scale disasters withstand the strongest possible tremor, the prohibitive cost of such efforts clearly cannot be justified or even afforded.\n\nAt the extreme scale, geoengineering is defined as options that would involve large-scale engineering of our environment in order to combat or counteract the effects of changes in atmospheric chemistry. Along those lines, Nobel laureate Paul Crutzen has proposed a method of artificially cooling the global climate by releasing particles of sulphur in the upper atmosphere, which would reflect sunlight and heat back into space. The controversial proposal is being taken seriously by scientists because Crutzen has a proven track record in atmospheric research. Sponsored by the U.S. National Science Foundation, a scientific meeting was held in 2008 to explore far-fetched strategies to combat hurricanes and tornadoes.\n\nIn contrast to natural disasters, manmade ones are generally somewhat easier to control but more difficult to predict. The war on terrorism is a case in point. Who could predict the behavior of a crazed suicide bomber? A civilized society spends its valuable resources on intelligence gathering, internal security, border control, and selective/mandatory screening to prevent (control) such devious behavior, whose dynamics (i.e., time evolution) obviously cannot be distilled into a differential equation to be solved. However, even in certain disastrous situations that depend on human behavior, predictions can sometimes be made; crowd dynamics being a prime example where the behavior of a crowd in an emergency can to some degree be modeled and anticipated so that adequate escape or evacuation routes can be properly designed [9]. Helbing et al. [10] write on simulation of panic situations and other crowd disasters modeled as nonlinear dynamical systems. All such models are heuristic and do not stem from the first-principles laws of classical mechanics.\n\nThe tragedy of the numerous manmade disasters is that they are all preventable, at least in principle. We cannot prevent a hurricane, at least not yet, but using less fossil fuel and seeking alternative energy sources could at least slow global warming trends down. Conflict resolution strategies can be employed between nations to avert wars. Speaking of wars, the Iraqi-American poet Dunya Mikhail, lamenting on the many manmade disasters, calls the present period \"The Tsunamical Age\". A bit more humanity, commonsense, selflessness, and moderation, as well as a bit less greed, meanness, selfishness, and zealotry, and the world will be a better place for having fewer manmade disasters.\n\nFor disasters that involve (fluid) transport phenomena, such as severe weather, fire, and release of toxic substance, the governing equations can be formulated subject to some assumptions, the less the better. Modeling is usually in the form of nonlinear partial differential equations with an appropriate number of initial and boundary conditions. Integrating those field equations leads to the time evolution, or the dynamics, of the disaster. In principle, marching from the present (initial conditions) to the future gives the potent predictability of classical mechanics and ultimately leads to the disaster's forecast. However, the first principles equations are typically impossible to solve analytically, particularly if the fluid flow is turbulent, which unfortunately is the norm for the high Reynolds number flows encountered in the atmosphere and oceans. Furthermore, initial and boundary conditions are required for both analytical and numerical solutions, and massive amounts of data need to be collected to determine those conditions with sufficient resolution and accuracy. Computers are not big enough either, so numerical integration of the instantaneous equations (direct numerical simulations) for high Reynolds number natural flows is computationally prohibitively expensive if not outright impossible at least for now and the foreseeable future. Heuristic modeling then comes to the rescue but at a price. Large eddy simulations, spectral methods, probability density function models, and the more classical Reynolds stress models are examples of such closure schemes that are not as computationally intensive as direct numerical simulations, but are not as reliable either. This type of second-tier modeling is phenomenological in nature and does not stem from first principles. The more heuristic the modeling is, the less accurate the expected results are. Together with massive ground, sea, and sky data to provide at least in part the initial and boundary conditions, the models are entered into supercomputers that come out with a forecast, whether it is a prediction of a severe thunderstorm that is yet to form, the future path and strength of an existing hurricane, or the impending concentration of a toxic gas that was released in a faraway location some time in the past. The issue of nonintegrability of certain dynamical systems is an additional challenge and opportunity that is revisited in Section 4.9.\n\nFor other types of disasters such as earthquakes, the precise laws are not even known mostly because proper constitutive relations are lacking. Additionally, deep underground data are difficult to gather to say the least. Predictions in those cases become more or less a black art.\n\nIn the next seven subsections, we focus on the prediction of disasters involving fluid transport. This important subject has spectacular successes within the past few decades, for example, in being able to predict the weather a few days in advance. The accuracy of today's 5-day forecast is the same as the 3-day and 1.5-day ones in 1976 and 1955, respectively. The 3-day forecast of a hurricane's strike position is accurate to within 100 km, about a 1-hour drive on the highway [11]. The painstaking advances made in fluid mechanics in general and turbulence research in particular together with the exponential growth of computer memory and speed undoubtedly contributed immeasurably to those successes.\n\nThe British physicist Lewis Fry Richardson was perhaps the first to make a scientifically based weather forecast. Based on data taken at 7:00 am, 20 May 1910, he made a 6 hour \"forecast\" that took him 6 weeks to compute using a slide rule. The belated results 2 were totally wrong as well! In his M. Gad-el-Hak remarkable book, Richardson [12] wrote \"Perhaps some day in the dim future it will be possible to advance the computations faster than the weather advances and at a cost less than the saving to mankind due to the information gained. But that is a dream\". (p. vii.) We are happy to report that Richardson's dream is one of the few that came true. A generation ago, the next day's weather was hard to predict. Today, the 10-day forecast is available 24/7 on www.weather.com for almost any city in the world. Not very accurate perhaps, but far better than the pioneering Richardson's 6-hour forecast.\n\nThe important issue is to precisely state the assumptions needed to write the evolution equations, which are basically statements of the conservation of mass, momentum and energy, in a certain form. The resulting equations and their eventual analytical or numerical solutions are only valid under those assumptions. This seemingly straightforward fact is often overlooked and wrong answers readily result when the situation we are trying to model is different from that assumed. Much more details of the science of disaster's prediction are provided in a book edited by the same author [1].\n\nEach fundamental law of fluid mechanics and heat transfer -conservation of mass, momentum, and energy -are listed first in their raw form, (i.e. assuming only that the speeds involved are nonrelativistic and that the fluid is a continuum). In nonrelativistic situations, mass and energy are conserved separately and are not interchangeable. This is the case for all normal fluid velocities that we deal with in everyday situations -far below the speed of light. The continuum assumption ignores the grainy (microscopic) structure of matter. It implies that the derivatives of all the dependent variables exist in some reasonable sense. In other words, local properties such as density and velocity are defined as averages over large elements compared with the microscopic structure of the fluid but small enough in comparison with the scale of the macroscopic phenomena to permit the use of differential calculus to describe them. The resulting equations therefore cover a broad range of situations, the exception being flows with spatial scales that are not much larger than the mean distance between the fluid molecules, as for example in the case of rarefied gas dynamics, shock waves that are thin relative to the mean free path, or flows in micro-and nanodevices. Thus, at every point in space-time in an inertial (i.e., nonaccelerating/nonrotating), Eulerian frame of reference, the three conservation laws for nonchemically reacting fluids, respectively, read in Cartesian tensor notations\n\nwhere ρ is the fluid density, u k is an instantaneous velocity component (u, v, w), Σ ki is the second-order stress tensor (surface force per unit area), g i is the body force per unit mass, e is the internal energy per unit mass, and q k is the sum of heat flux vectors due to conduction and radiation. The independent variables are time t, and the three spatial coordinates x 1 , x 2 , and x 3 or (x, y, z). Finally, the Einstein's summation convention applies to all repeated indices. Gad-el-Hak [13] provides a succinct derivation of the previous conservation laws for a continuum, nonrelativistic fluid.\n\nEquations ( 1), ( 2) and (3) constitute five differential equations for the seventeen unknowns ρ, u i , Σ ki , e, and q k . Absent any body couples, the stress tensor is symmetric having only six independent components, which reduces the number of unknowns to fourteen. To close the conservation equations, relation between the stress tensor and deformation rate, relation between the heat flux vector and the temperature field, and appropriate equations of state relating the different thermodynamic properties are needed. Thermodynamic equilibrium implies that the macroscopic quantities have sufficient time to adjust to their changing surroundings.\n\nIn motion, exact thermodynamic equilibrium is impossible because each fluid particle is continuously having volume, momentum, or energy added or removed, and so in fluid dynamics and heat transfer we speak of quasi-equilibrium. The second law of thermodynamics imposes a tendency to revert to equilibrium state, and the defining issue here is whether the flow quantities are adjusting fast enough. The reversion rate will be very high if the molecular time and length scales are very small as compared to the corresponding macroscopic flow scales. This will guarantee that numerous molecular collisions will occur in sufficiently short time to equilibrate fluid particles whose properties vary little over distances comparable to the molecular length scales. Gas flows are considered in a state of quasi-equilibrium if the Knudsen number -the ratio of the mean free path to a characteristic length of the flow -is less than 0.1. In such flows, the stress is linearly related to the strain rate, and the (conductive) heat flux is linearly related to the temperature gradient. Empirically, common liquids such as water follow the same laws under most flow conditions. Reference [14] provides extensive discussion of situations in which the quasi-equilibrium assumption is violated. These may include gas flows at great altitudes, flows of complex liquids such as long-chain molecules, and even ordinary gas and liquid flows when confined in micro-and nanodevices.\n\nFor a Newtonian, isotropic, Fourierfoot_3 , ideal gas, for example, those constitutive relations read\n\nThe art and science of large-scale disasters\n\nwhere p is the thermodynamic pressure, µ and λ are the first and second coefficients of viscosity, respectively, δ ki is the unit second-order tensor (Kronecker delta), κ is the thermal conductivity, T is the temperature field, c v is the specific heat at constant volume, and R is the gas constant. The Stokes' hypothesis relates the first and second coefficients of viscosity, λ+ 2 3 µ = 0, although the validity of this assumption has occasionally been questioned [15]. With the previous constitutive relations and neglecting radiative heat transferfoot_4 , Eqs. ( 1), (2), and (3), respectively, read\n\nThe three components of the vector Eq. ( 8) are the Navier-Stokes equations expressing the conservation of momentum (or, more precisely, stating that the rate of change of momentum is equal to the sum of all forces) for a Newtonian fluid. In the thermal energy Eq. ( 9), φ is the always positive (as required by the Second Law of Thermodynamics) dissipation function expressing the irreversible conversion of mechanical energy to internal energy as a result of the deformation of a fluid element. The second term on the right-hand side of Eq. ( 9) is the reversible work done (per unit time) by the pressure as the volume of a fluid material element changes. For a Newtonian, isotropic fluid, the viscous dissipation rate is given by\n\nThere are now six unknowns, ρ, u i , p, and T , and the five coupled Eqs. ( 7), (8), and (9), plus the equation of state relating pressure, density, and temperature. These six equations, together with sufficient number of initial and boundary conditions constitute a well-posed, albeit formidable, problem. The system of Eqs. (7) to ( 9) is an excellent model for the laminar or turbulent flow of most fluids, such as air and water under most circumstances, including high-speed gas flows for which the shock waves are thick relative to the mean free path of the molecules.\n\nPolymers, rarefied gases, and flows in micro-and nanodevices are not equilibrium flows and have to be modeled differently. In those cases, higher-order relations between the stress tensor and rate of strain tensor, and between the heat flux vector and temperature gradient, are used. In some cases, the continuum approximation is abandoned altogether, and the fluid is modeled as it really is -a collection of molecules. The molecular-based models used for those unconventional situations include molecular dynamics simulations, direct simulation Monte Carlo methods, and the analytical Boltzmann equation [16]. Under certain circumstances, hybrid molecularcontinuum formulation is required.\n\nReturning to the continuum, quasiequilibrium equations, considerable simplification is achieved if the flow is assumed incompressible, usually a reasonable assumption provided that the characteristic flow speed is less than 0.3 of the speed of sound and other conditions are satisfied. The incompressibility assumption, discussed in greater detail in Reference [17], is readily satisfied for almost all liquid flows and for many gas flows. In such cases, the density is assumed either a constant or a given function of temperature (or species concentration). The governing equations for such flows are\n\nThese are five equations for the five dependent variables u i , p, and T . Note that the left-hand side of Eq. ( 13) has the specific heat at constant pressure c p and not c v . This is the correct incompressible flow limit -of a compressible fluid -as discussed in detail in Section 10.9 of Panton [17]; a subtle point perhaps but one that is frequently missed in textbooks. The system of Eqs. (11) to (13) is coupled if either the viscosity or density depends on temperature; otherwise, the energy equation is uncoupled from the continuity and momentum equations, and can therefore be solved after the velocity and pressure fields are determined from solving Eqs. (11) and (12). For most geophysical flows, the density depends on temperature and/or species concentration, and the previous system of five equations is coupled.\n\nIn non-dimensional form, the incompressible flow equations read\n\nwhere F ν (T ) is a dimensionless function that characterizes the viscosity variation with temperature, and Re, Gr, Pe, and Ec are, respectively, the Reynolds, Grashof, Péclet, and Eckert numbers. These dimensionless parameters determine the relative importance of the different terms in the equations.\n\nFor both the compressible and the incompressible equations of motion, the transport terms are neglected away from solid walls in the limit of infinite Reynolds number (i.e. zero Knudsen number). The flow is then approximated as inviscid, nonconducting and nondissipative; in other words, it is considered in perfect thermodynamic equilibrium. The corresponding equations in this case read (for the compressible case):\n\nThe Euler Eq. ( 18) can be integrated along a streamline, and the resulting Bernoulli's equation provides a direct relation between the velocity and the pressure.\n\nEven with the simplification accorded by the incompressibility assumption, the viscous system of equations is formidable and has no general solution. Usual further simplifications -applicable only to laminar flows -include geometries for which the nonlinear terms in the (instantaneous) momentum equation are identically zero, low Reynolds number creeping flows for which the nonlinear terms are approximately zero, and high Reynolds number inviscid flows for which the continuity and momentum equations can be shown to metamorphose into the linear Laplace equation. The latter assumption spawned the great advances in perfect flow theory that occurred during the second half of the nineteenth century. However, neglecting viscosity gives the totally erroneous result of zero drag for moving bodies and zero pressure drop in pipes. Moreover, none of those simplifications apply to the rotational, (instantaneously) timedependent, and three-dimensional turbulent flows.\n\nNot surprisingly, hydraulic engineers of the time showed little interest in the elegant theories of hydrodynamics and relied instead on their own collection of totally empirical equations, charts, and tables to compute drag, pressure losses, and other practically important quantities. Consistent with that pragmatic approach, engineering students then and for many decades to follow were taught the art of hydraulics. The science of hydrodynamics was relegated, if at all, to mathematics and physics curricula.\n\nIn lamenting the status of fluid mechanics at the dawn of the twentieth century, the British chemist and Nobel laureate Sir Cyril Norman Hinshelwood (1897-1967) jested that fluid dynamists were divided into hydraulic engineers who observed things that could not be explained and mathematicians who explained things that could not be observed.\n\nIn an epoch-making presentation to the third International Congress of Mathematicians held in Heidelberg, the German engineer Ludwig Prandtl resolved, to a large extent, the previous dilemma. Prandtl [18] introduced the concept of a fluid boundary layer, adjacent to a moving body, where viscous forces are important and outside of which the flow is more or less inviscid. At sufficiently high Reynolds number, the boundary layer is thin relative to the longitudinal length scale and, as a result, velocity derivatives in the streamwise direction are small compared to normal derivatives. For the first time, single simplification made it possible to obtain viscous flow solutions, even in the presence of nonlinear terms, at least in the case of laminar flow. Both the momentum and the energy equations are parabolic under such circumstances, and are therefore amenable to similarity solutions and marching numerical techniques. From then on, viscous flow theory was in vogue for both scientists and engineers. Practical quantities such as skin friction drag could be computed from first principles, even for noncreeping flows. Experiments in wind tunnels, and their cousins water tunnels and towing tanks, provided valuable data for problems too complex to submit to analysis.\n\nAll the transport equations listed thus far are valid for non-turbulent and turbulent flows. However, in the latter case, the dependent variables are generally random functions of space and time. No straightforward method exists for obtaining stochastic solutions of these nonlinear partial differential equations, and this is the primary reason why turbulence remains as the last great unsolved problem of classical physics. Dimensional analysis can be used to obtain crude results for a few cases, but first principles analytical solutions are not possible even for the simplest conceivable turbulent flow.\n\nThe contemporary attempts to use dynamical systems theory to study turbulent flows have not yet reached fruition, especially at Reynolds numbers far above transition [19], although advances in this theory have helped with reducing and displaying the massive bulk of data resulting from numerical and experimental simulations [20]. The book by Holmes et al. [21] provides a useful, readable introduction to the emerging field. It details a strategy by which knowledge of coherent structures, finite-dimensional dynamical systems theory, and the Karhunen-Loève or proper orthogonal decomposition could be combined to create low-dimensional models of turbulence that resolve only the organized motion, and describes their dynamical interactions. The utility of the dynamical systems approach as an additional arsenal to tackle the turbulence conundrum has been demonstrated only for turbulence near transition or near a wall, so that the flow would be relatively simple, and a relatively small number of degrees of freedom would be excited. Holmes et al. summarize the (partial) successes that have been achieved thus far using relatively small sets of ordinary differential equations and suggest a broad strategy for modeling turbulent flows and other spatiotemporal complex systems.\n\nA turbulent flow is described by a set of nonlinear partial differential equations and is characterized by an infinite\n\nThe art and science of large-scale disasters number of degrees of freedom. This makes it rather difficult to model the turbulence using a dynamical systems approximation. The notion that a complex, infinite-dimensional flow can be decomposed into several low-dimensional subunits is, however, a natural consequence of the realization that quasiperiodic coherent structures dominate the dynamics of seemingly random turbulent shear flows. This implies that low-dimensional, localized dynamics can exist in formally infinite-dimensional extended systems, such as open turbulent flows. Reducing the flow physics to finite dimensional dynamical systems enables a study of its behavior through an examination of the fixed points and the topology of their stable and unstable manifolds. From the dynamical systems theory viewpoint, the meandering of low-speed streaks is interpreted as hovering of the flow state near an unstable fixed point in the low-dimensional state space. An intermittent event that produces high wall stress -a burst -is interpreted as a jump along a heteroclinic cycle to a different unstable fixed point that occurs when the state has wandered too far from the first unstable fixed point. Delaying this jump by holding the system near the first fixed point should lead to lower momentum transport in the wall region and, therefore, to lower skin friction drag. Reactive control means sensing the current local state and, through appropriate manipulation, keeping the state close to a given unstable fixed point, thereby preventing further production of turbulence. Reducing the bursting frequency by 50%, for example, may lead to a comparable reduction in skin friction drag. For a jet, relaminarization may lead to a quiet flow and very significant noise reduction. We return to the two described facets of nonlinear dynamical systemspredictability and control -in Section 4.9.\n\nDirect numerical simulations (DNS) of a turbulent flow, the brute force numerical integration of the instantaneous equations using the supercomputer, is prohibitively expensive -if not impossible -at practical Reynolds numbers [22]. For the present at least, a statistical approach, where a temporal, spatial, or ensemble average is defined and the equations of motion are written for the various moments of the fluctuations about this mean, is the only route available to obtain meaningful engineering results. Unfortunately, the nonlinearity of the Navier-Stokes equations guarantees that the process of averaging to obtain moments results in an open system of equations, where the number of unknowns is always greater than the number of equations, and more or less heuristic modeling is used to close the equations. This is known as the closure problem, and again makes obtaining first principles solutions to the (averaged) equations of motion impossible.\n\nTo illustrate the closure problem, consider the (instantaneous) continuity and momentum equations for a Newtonian, incompressible, constant density, constant viscosity, turbulent flow. In this uncoupled version of Eqs. (11) and (12) for the four random unknowns u i and p, no general stochastic solution is known to exist. However, would it be feasible to obtain solutions for the nonstochastic mean flow quantities? As was first demonstrated by Osborne Reynolds [23] more than a cen-tury ago, all the field variables are decomposed into a mean and a fluctuation. Let u i = U i + u ′ i and p = P + p ′ , where U i and P are ensemble averages for the velocity and pressure, respectively, and u ′ i and p ′ are the velocity and pressure fluctuations about the respective averages. Note that temporal or spatial averages could be used in place of ensemble average if the flow field is stationary or homogeneous, respectively. In the former case, the time derivative of any statistical quantity vanishes. In the latter, averaged functions are independent of position. Substituting the decomposed pressure and velocity into Eqs. ( 11) and ( 12), the equations governing the mean velocity and mean pressure for an incompressible, constant viscosity, turbulent flow becomes\n\nwhere, for clarity, the primes have been dropped from the fluctuating velocity components u i and u k . This is now a system of four equations for the ten unknowns U i , P , and u i u kfoot_6 . The momentum Eq. ( 21) is written in a form that facilitates the physical interpretation of the turbulence stress tensor (Reynolds stresses), -ρ u i u k , because additional stresses on a fluid element are to be considered along with the conventional viscous stresses and pressure. An equation for the components of this tensor may be derived, but it will contain third order moments such as u i u j u k . The equations are (heuristically) closed by expressing the secondor third-order quantities in terms of the first or second moments, respectively. For comprehensive reviews of these firstand second-order closure schemes [24][25][26][27]. A concise summary of the turbulence problem in general is provided by Jiménez [28].\n\nLeaving aside for a moment less conventional, albeit just as important, problems in fluid mechanics such as those involving non-Newtonian fluids, multiphase flows, hypersonic flows, and chemically reacting flows, in principle almost any laminar flow problem can presently be solved, at least numerically. Turbulence, in contrast, remains largely an enigma, analytically unapproachable yet practically very important. The statistical approach to solving the Navier-Stokes equations always leads to more unknowns than equations (the closure problem), and solutions based on first principles are again not possible. The heuristic modeling used to close the Reynolds-averaged equations has to be validated case by case, and does not, therefore, offer much of an advantage over the old-fashioned empirical approach.\n\nThus, turbulence is a conundrum that appears to yield its secrets only to physical and numerical experiments, provided that the wide band of relevant scales is fully resolveda far-from-trivial task at high Reynolds numbers [29]. Until recently, direct numerical simulations of the canonical turbulent boundary layer have been carried out, at great cost despite a bit of improvising, up to a very modest momentum-thickness Reynolds number of 1,410 [30].\n\nIn a turbulent flow, the ratio of the large eddies (at which the energy maintaining the flow is inputed) to the Kolmogorov microscale (the flow smallest length scale) is proportional to Re 3/4 [31]. Each excited eddy requires at least one grid point to describe it. Therefore, to adequately resolve, via DNS, a three-dimensional flow, the required number of modes would be proportional to (Re 3/4 ) 3 . To describe the motion of small eddies as they are swept around by large ones, the time step must not be larger than the ratio of the Kolmogorov length scale to the characteristic root mean square (rms) velocity. The large eddies, however, evolve on a time scale proportional to their size divided by their rms velocity. Thus, the number of time steps required is again proportional to Re 3/4 . Finally, the computational work requirement is the number of modes × the number of time steps, which scales with Re 3 (i.e. an order of magnitude increase in computer power is needed as the Reynolds number is doubled) [32]. Because the computational resource required varies as the cube of the Reynolds number, it may not be possible to directly simulate very high Reynolds number turbulent flows any time soon.\n\nDespite their already complicated nature, the transport equations introduced previously could be further entangled by other effects. We list herein a few examples. Geophysical flows occur at such large length scales as to invalidate the inertial frame assumption made previously. The Earth's rotation affects these flows, and such things as centrifugal and Coriolis forces enter into the equations rewritten in a non-inertial frame of reference fixed with the rotating Earth. Oceanic and atmospheric flows are more often than not turbulent flows that span the enormous range of length scales of nine decades, from few a millimeters to thousands of kilometers [33,34].\n\nDensity stratification is important for many atmospheric and oceanic phenomena. Buoyancy forces are produced by density variations in a gravitational field, and those forces drive significant convection in natural flows [35]. In the ocean, those forces are further complicated by the competing influences of temperature and salt [33]. The competition affects the large-scale global ocean circulation and, in turn, climate variability. For weak density variations, the Bousinessq approximation permits the use of the coupled incompressible flow equations, but more complexities are introduced in situations with strong density stratification, such as when strong heating and cooling is present. Complex topography further complicates convective flows in the ocean and atmosphere.\n\nAir-sea interface governs many of the important transport phenomena in the ocean and atmosphere, and plays a crucial role in determining the climate. The location of that interface is itself not known a priori and thus is the source of further complexity in the problem. Even worse, the free boundary nature of the liquid-gas interface, in addition to the possibility of breaking that interface and forming bubbles and droplets, introduces new nonlinearities that augment or compete with the customary convective nonlinearity [36]. Chemical reactions are obviously important in fires and are even present in some atmospheric transport problems. When liquid water or ice is present in the air, two-phase treatment of the equations of motion may need to be considered, again complicating even the relevant numerical solutions. However, even in those complex situations described previously, simplifying assumptions can be made rationally to facilitate solving the problem. Any spatial symmetries in the problem must be exploited. If the mean quantities are time independent, then that too can be exploited.\n\nAn extreme example of simplification that surprisingly yields reasonable results includes the swirling giants depicted in Fig. 3. Here, an oceanic whirlpool, a hurricane, and a spiral galaxy are simply modeled as a rotating, axisymmetric viscous core and an external inviscid vortex joined by a Burger's vortex. The viscous core leads to a circumferential velocity proportional to the radius, and the inviscid vortex leads to a velocity proportional to 1/r. This model leads to surprisingly good results in some narrow sense for those exceedingly complex flows. Fig. 3. Simple modeling of an oceanic whirlpool, a hurricane and a spiral galaxy A cyclone's pressure is the best indicator of its intensity because it can be precisely measured, whereas winds have to be estimated. The previous simple model yields the maximum wind speed from measurements of the center pressure, the ambient pressure, and the size of the eye of the storm. It is still important to note that it is the difference in the hurricane's pressure and that of its environment that actually give it its strength. This difference in pressure is known as the \"pressure gradient\" and it is this change in pressure over a distance that causes wind. The bigger the gradient, the faster will be the winds generated. If two cyclones have the same minimum pressure, but one is in an area of higher ambient pressure than the other, that one is in fact stronger. The cyclone must\n\nThe art and science of large-scale disasters be more intense to get its pressure commensurately lower, and its larger pressure gradient would make its winds faster.\n\nThus far in this section, we discussed prediction of the type of disaster involving fluid transport phenomena, weather-related disasters being the most rampant. Predictions are possible on those cases, and improvements in forecast's accuracy and extent are continually being made as a result of enhanced understanding of flow physics, increased accuracy and resolution of global measurements, and exponentially expanded computer power. Other types of disaster do not fare as well, earthquakes being calamities that thus far cannot be accurately predicted. Prediction of weather storms is possible in part because the atmosphere is optically transparent, which facilitates measurements that in turn provide not only the initial and boundary conditions necessary for integrating the governing equations but also a deeper understanding of the physics. The oceans are not as accessible, but measurements there are possible as well, and scientists learned a great deal in the past few decades about the dynamics of both the atmosphere and the ocean [33,34]. Our knowledge of terra firma, in contrast, does not fare as well mostly because of its inaccessibility to direct observation [2]. What we know about the Earth's solid inner core, liquid outer core, mantle, and lithosphere comes mainly from inferences drawn from observations at or near the planet's surface, which include the study of propagation, reflection, and scatter of seismic waves. Deep underground measurements are not very practical, and the exact constitutive equations of the different constituents of the \"solid\" Earth are not known. All that inhibits us from writing down and solving the precise equations, and their initial and boundary conditions, for the dynamics of the Earth's solid part. That portion of the planet contains three orders of magnitude more volume than all the oceans combined and six orders of magnitude more mass than the entire atmosphere, and it is a true pity that we know relatively little about the solid Earth.\n\nThe science of earthquake basically began shortly after the infamous rupture of the San Andreas fault that devastated San Francisco a little more than a century ago. Before then, geologists had examined seismic faults and even devised primitive seismometers to measure shaking. However, they had no idea what caused the ground to heave without warning. A few days after the Great Earthquake struck on 18 April 1906, Governor George C. Pardee of California charged the state's leading scientists with investigating how and why the Earth's crust had ruptured for hundreds of miles with such terrifying violence. The foundation for much of what is known today about earthquakes was laid two years later, and the resulting report [37] carried the name of the famed geologist Andrew C. Lawson. Earthquakes are caused by stresses in the Earth's crust that build up deep inside a fault until it ruptures with a jolt. Prior to the Lawson Report, many scientists believed earthquakes created the faults instead of the other way around. The San Andreas Fault system marks the boundary between two huge moving slabs of the Earth's crust: the Pacific Plate and the North American Plate. As the plates grind constantly past each other, strain builds until it is released periodically in a fullscale earthquake. A few small sections of the San Andreas Fault had been mapped by scientists years before 1906, but Lawson and his team discovered that the entire zone stretched for more than 950 km along the length of California. By measuring land movements on either side of the fault, the team learned that the earthquake's motion had moved the ground horizontally, from side to side, rather than just vertically as scientists had previously believed.\n\nA century after the Lawson Report, its conclusions remain valid, but it has stimulated modern earthquake science to move far beyond. Modern scientists have learned that major earthquakes are not random events -they apparently come in cycles. Although pinpoint prediction remains impossible, research on faults throughout the San Francisco Bay Area and other fault locations enables scientists to estimate the probability that strong quakes will jolt a region within the coming decades. Sophisticated broadband seismometers can measure the magnitude of earthquakes within a minute or two of an event and determine where and how deeply on a fault the rupture started. Orbiting satellites now measure within fractions of an inch how the Earth's surface moves as strain builds up along fault lines, and again how the land is distorted after a quake has struck. \"Shakemaps\", available on the Internet and by e-mail immediately after every earthquake, can swiftly tell disaster workers, utility companies and residents where damage may be greatest. Supercomputers, simulating ground motion from past earthquakes, can show where shaking might be heaviest when new earthquakes strike. The information can then be relayed to the public and to emergency workers.\n\nOne of the latest and most important ventures in understanding earthquake behavior is the borehole drilling project at Parkfield in southern Monterey County, California, where the San Andreas Fault has been heavily instrumented for many years. The hole is about 3.2 km deep and crosses the San Andreas underground. For the first time, sensors can actually be inside the earthquake machine to catch and record the earthquakes right where and when they are occurring.\n\nThe seismic safety of any structure depends on the strength of its construction and the geology of the ground on which it stands -a conclusion reflected in all of today's building codes in the United States. Tragically, the codes in some earthquake prone countries are just as strict as those in the United States, but are not enforceable for the most part. In other nations, building codes are not sufficiently strict or nonexistent altogether.\n\nThere are two additional issues to ponder for all disasters that could be modeled as nonlinear dynamical systems. The volume edited by Bunde et al. [38] is devoted to this topic, and is one of very few books to tackle large-scale disasters purely as a problem to be posed and solved using scientific principles. The modeling could be in the form of a number of algebraic equations or, more likely, ordinary or partial differential equations, with nonlinear term(s) appearing somewhere within the finite number of equations. First, we examine the bad news. Nonlinear dy-M. Gad-el-Hak namical systems are capable of producing chaotic solutions, which limit the ability to predict too far into the future, even if infinitely powerful computers are available. Second, we examine the (potentially) good news. Chaotic systems can be controlled, in the sense that a very small perturbation can lead to a significant change in the future state of the system. In this subsection, we elaborate on both issues.\n\nIn the theory of dynamical systems, the so-called \"butterfly effect\" (a lowly diurnal lepidopteran flapping its wings in Brazil may set off a future tornado in Texas) denotes sensitive dependence of nonlinear differential equations on initial conditions, with phase-space solutions initially very close together and separating exponentially. Massachusetts Institute of Technology's atmospheric scientist Edward Lorenz originally used seagull's wings for the metaphor in a paper for the New York Academy of Sciences [39], but in subsequent speeches and papers he used the more poetic butterfly. For a complex system such as the weather, initial conditions of infinite resolution and infinite accuracy are clearly never going to be available, thus further making certain that precise long-term predictions are never achievable.\n\nThe solution of nonlinear dynamical systems of three or more degrees of freedomfoot_7 may be in the form of a strange attractor whose intrinsic structure contains a well-defined mechanism to produce a chaotic behavior without requiring random forcing [40]. Chaotic behavior is complex, aperiodic, and, although deterministic, appears to be random. The dynamical system in that case is nonintegrable 7 , and our ability for longterm forecast is severely hindered because of the extreme sensitivity to initial conditions. One can predict the most probable weather, for example, a week from the present, with a narrow standard deviation to indicate all other possible outcomes. We speak of a 30% chance of rain 7 days from now, and so on. That ability to provide reasonably accurate prediction diminishes as time progresses because the sensitivity to initial conditions intensifies exponentially, and Lorenz [41] proposes a 20-day theoretical limit for predicting weather. This means that regardless how massive future computers will become, weather prediction beyond 20 days will always be meaningless. Nevertheless, we still have a way to go to double the extent of the current 10-day forecast.\n\nWeather and climate should not be confused, however. The latter describes the long term variability of the climate system whose components comprise the atmosphere, hydrosphere, cryosphere, pedosphere, lithosphere and biosphere. Climatologists apply models to compute the evolution of the climate a hundred years or more into the future [42,43]. Seemingly paradoxical, meteorologists use similar models but have dif-ficulties forecasting the weather beyond just a few days. Both weather and climate are nonlinear dynamical systems, but the former concerns the evolution of the system as a function of the initial conditions with fixed boundary conditions, whereas the latter, especially as influenced by human misdeeds, concerns the response of the system to changes in boundary conditions with fixed initial conditions. For long time periods, the dependence of the time-evolving climate state on the initial conditions becomes negligible asymptotically. Now for the good news. A question arises naturally: just as small disturbances can radically grow within a deterministic system to yield rich, unpredictable behavior, can minute adjustments to a system parameter be used to reverse the process and control (i.e., regularize) the behavior of a chaotic system? This question was answered in the affirmative both theoretically and experimentally, at least for system orbits that reside on low-dimensional strange attractors (see the review by Lindner and Ditto [44]).\n\nThere is another question of greater relevance here. Given a dynamical system in the chaotic regime, is it possible to stabilize its behavior through some kind of active control? Although other alternatives have been devised (e.g., [45][46][47][48]), the recent method proposed by workers at the University of Maryland [49][50][51][52][53][54][55][56][57][58] promises to be a significant breakthrough. Comprehensive reviews and bibliographies of the emerging field of chaos control can be found in the articles [44,[59][60][61][62].\n\nOtt et al. [49] demonstrate, through numerical experiments with the Hénon map, that it is possible to stabilize a chaotic motion about any prechosen, unstable orbit through the use of relatively small perturbations. The procedure consists of applying minute time dependent perturbations to one of the system parameters to control the chaotic system around one of its many unstable periodic orbits. In this context, targeting refers to the process whereby an arbitrary initial condition on a chaotic attractor is steered toward a prescribed point (target) on this attractor. The goal is to reach the target as quickly as possible using a sequence of small perturbations [63].\n\nThe success of the Ott-Grebogi-Yorke's (OGY) strategy for controlling chaos hinges on the fact that beneath the apparent unpredictability of a chaotic system lies an intricate but highly ordered structure. Left to its own recourse, such a system continually shifts from one periodic pattern to another, creating the appearance of randomness. An appropriately controlled system, however, is locked into one particular type of repeating motion. With such reactive control the dynamical system becomes one with a stable behavior.\n\nThe OGY method can be simply illustrated as follows. The state of the system is represented as the intersection of\n\nThe art and science of large-scale disasters a stable manifold and an unstable one. The control is applied intermittently whenever the system departs from the stable manifold by a prescribed tolerance; otherwise, the control is shut off. The control attempts to put the system back onto the stable manifold so that the state converges toward the desired trajectory. Unmodeled dynamics cause noise in the system and a tendency for the state to wander off in the unstable direction. The intermittent control prevents this, and the desired trajectory is achieved. This efficient control is not unlike trying to balance a ball in the center of a horse saddle [64]. There is one stable direction (front/back) and one unstable direction (left/right). The restless horse is the unmodeled dynamics, intermittently causing the ball to move in the wrong direction. The OGY control need only be applied, in the most direct manner possible, whenever the ball wanders off in the left/right direction.\n\nThe OGY method has been successfully applied in a relatively simple experiment by Ditto et al. [65] and Ditto and Pecora [66] at the Naval Surface Warfare Center, in which reverse chaos was obtained in a parametrically driven, gravitationally buckled, amorphous magnetoelastic ribbon. Garfinkel et al. [67] apply the same control strategy to stabilize druginduced cardiac arrhythmias in sections of a rabbit ventricle. Other extensions, improvements and applications of the OGY strategy include higher dimensional targeting [68,69]; controlling chaotic scattering in Hamiltonian (i.e., nondissipative, area conservative) systems [70,71]; synchronization of identical chaotic systems that govern communication, neural, or biological processes [72]; use of chaos to transmit information [73,74]; control of transient chaos [75]; and taming spatiotemporal chaos using a sparse array of controllers [76][77][78].\n\nIn a more complex system, such as a turbulent boundary layer, numerous interdependent modes, as well as many stable and unstable manifolds (directions) exist. The flow can then be modeled as coherent structures plus a parameterized turbulent background. The proper orthogonal decomposition (POD) is used to model the coherent part because POD guarantees the minimum number of degrees of freedom for a given model accuracy. Factors that make turbulence control a challenging task are the potentially quite large perturbations caused by the unmodeled dynamics of the flow, the nonstationary nature of the desired dynamics, and the complexity of the saddle shape describing the dynamics of the different modes. Nevertheless, the OGY control strategy has several advantages that are of special interest in the control of turbulence: (1) the mathematical model for the dynamical system need not be known, (2) only small changes in the control parameter are required, and (3) noise can be tolerated (with appropriate penalty).\n\nHow does all this apply to large-scale disasters? Suppose, for example, global warming is the disaster under consideration. Suppose further that we know how to model this complex phenomena as a nonlinear dynamical system. What if we can ever so gently manipulate the present state to greatly, and hopefully beneficially, affect future outcome? A quintessential butterfly effect. For example, what if we cover a modest-size desert with reflective material that reduces the absorption of radiation from the Sun? If it is done right, that manipulation of a microclimate may result in a macroclimate change in the future. However, is it the desired change? What if it is not done right? This, of course, is the trillion-dollar question! Other examples may include prevention of future severe storms, droughts, famines, and earthquakes. As mentioned earlier in this section, large-scale engineering of our environment, geoengineering, is now taking seriously enough to warrant the involvement of certain Federal government agencies and at least one Noble laureate. More far-fetched examples include being able to control, via small perturbations, unfavorable human behaviors such as mass hysteria, panic, and stampedes. In any case, intensive theoretical, numerical and experimental research is required to investigate the proposed idea.\n\nTo predict weather-related disasters, computers use the best available models, together with massive data. Those data are gathered from satellites and manned as well as unmanned aircraft in the sky, water-based sensors, and sensors on the ground and even beneath the ground. Hurricanes, droughts, climate systems, and the planet's natural resources could all be better predicted with improved data and observations. Coastal mapping, nautical charting, ecosystem, hydrological and oceanic monitoring, fisheries surveillance, and ozone concentration can all be measured and assessed. In this subsection, we briefly describe the political steps that led to the recent formation of a global Earth observation system, a gigantic endeavor that is a prime example of the need for international cooperation.\n\nProducing and managing better information about the environment has become a top priority for nations around the globe. In July 2003, the Earth Observation Summit brought together thirty-three nations, as well as the European Commission and many international organizations, to adopt a declaration that signified a political commitment toward the development of a comprehensive, coordinated, and sustained Earth observation system to collect and disseminate improved data, information, and models to stakeholders and decision makers.\n\nEarth observation systems consist of measurements of air, water and land made on the ground, from the air, or in space. Historically observed in isolation, the current effort is to look at these elements together and to study their interactions. An ad hoc group of senior officials from all participating countries and organizations, named the Group on Earth Observations (GEO), was formed to undertake this global effort. GEO was charged to develop a \"framework document\", as well as a more comprehensive report, to describe how the collective effort could be organized to continuously monitor the state of our environment, increase understanding of dynamic Earth processes, and enhance forecasts on our environmental conditions. Furthermore, it was to address potential societal benefits if timely, high-quality, and long-term data and models were available to aid decision makers at every level, from intergovernmental organizations to local governments to individuals. Through four meetings of GEO, from late 2003 to April 2004, the required documents were prepared for ministerial review and adoption.\n\nIn April 2004, U.S. Environmental Protection Agency Administrator Michael Leavitt and other senior cabinet members met in Japan with environmental ministers from more than fifty nations. They adopted the framework document for a 10year implementation plan for the Global Earth Observation System of Systems (GEOSS).\n\nAs of 16 February 2005, 18 months after the first-ever Earth Observation Summit, the number of participating countries has nearly doubled, and interest has accelerated since the recent tsunami tragedy devastated parts of Asia and Africa. Sixty-one countries agreed to a 10-year plan that will revolutionize the understanding of Earth and how it works. Agreement for a 10-year implementation plan for GEOSS was reached by member countries of the GEO at the Third Observation Summit held in Brussels. Nearly forty international organizations also support the emerging global network. The GEOSS project will help all nations involved produce and manage their information in a way that benefits both the environment and humanity by taking the planet's \"pulse\". In the coming months, more countries and global organizations are expected to join the historic initiative.\n\nGEOSS is envisioned as a large national and international cooperative effort to bring together existing and new hardware and software, making it all compatible in order to supply data and information at no cost. The United States and developed nations have a unique role in developing and maintaining the system, collecting data, enhancing data distribution, and providing models to help the world's nations. Outcomes and benefits of a global informational system will include: The quality and quantity of data collected through GEOSS should help improve the prediction, control, and mitigation of many future manmade and natural disasters.\n\nThe laws of nature are the same regardless of what type of disaster is considered. A combination of first-principles laws of classical mechanics, heuristic modeling, data collection, and computers may help, to different degrees of success, the prediction and control of natural and manmade disasters, as discussed in Section 4. Once a disaster strikes, mitigating its adverse effects becomes the primary concern. Disaster management is more art than science, but the management principles are similar for most types of disaster, especially those that strike suddenly and intensely. The organizational skills and resources needed to mitigate the adverse effects of a hurricane are not much different from those required in the aftermath of an earthquake. The scope of the disaster (Section 2) determines the extent of the required response. Slowly evolving disasters such as global warming or air pollution are different and their management requires a different set of skills, response, and political will. Although millions of people may be adversely affected by global warming, the fact that that harm may be spread over decades and thus diluted in time does not provide immediacy to the problem and its potential mitigation. Political will to solve long-range problems -not affecting the next election -is typically nonexistent except in the case of the rare visionary leader.\n\nIn his book, der Heide [79] states that disasters are the ultimate test of emergency response capability. Once a largescale disaster strikes, mitigating its adverse effects becomes the primary concern. There are concerns about how to save lives, take care of the survivors' needs, and protect property from any further damage. Dislocated people need shelter, water, food, and medicine. Both the physical and the mental health of the survivors, as well as relatives of the deceased, can be severely jeopardized. Looting, price gouging, and other lawbreaking activities need to be contained, minimized, or eliminated. Hospitals need to prioritize and even ration treatments, especially in the face of the practical fact that the less seriously injured tend to arrive at emergency rooms first, perhaps because they transported themselves there. Roads need to be operable and free of landslides, debris, and traffic jams for the unhindered flow of first responders and supplies to the stricken area, and evacuees and ambulances from the same. This is not always the case especially if the antecedent disaster damages most if not all roads, as occurred after the 2005 Kashmir Earthquake. Buildings, bridges, and roads need to be rebuilt or repaired, and power, potable water and sewage need to be restored.\n\nLessons learned from one calamity can be applied to improve the response to subsequent ones [80,81]. Disaster mitigation is not a trial-and-error process, however. Operations research (operational research in Britain) is the discipline that uses the scientific approach to decision making, which seeks to determine how best to design and operate a system, usually under conditions requiring the allocation of scarce resources [82]. Churchman et al. [83] similarly define the genre as the application of scientific methods, techniques, and tools to problems involving the operations of systems so as to provide those in control of the operations with optimum solutions to the problems. Operations research and engineering optimization principles are skillfully used to facilitate recovery and return to normalcy following a large-scale disaster [84]. The always-finite resources available must be utilized so as to maximize their beneficial impact. A lot of uncoordinated, incoherent activities are obviously not a good use of scarce resources. For example, sending huge amounts of perishable food to a stricken area that has no electricity makes little sense. Although it seems silly, it is not difficult to find such examples that were made in the heat of the moment.\n\nMost books on large-scale disasters are written from either a sociologist's or a tactician's point of view, in contrast to the scientist's viewpoint of this article. There are few popular science or high school-level books on disasters [85,86], and even fewer more advanced science books, such as [1,38]. The other books deal, for the most part, with the behavioral response to disasters and the art of mitigating their aftermath. Current topics of research include disaster preparedness and behavioral and organizational responses to disasters. A small sample of recent books includes [3,7,79,80,81,.\n\nAlthough it appears that large-scale disasters are more recent when the past ast few years are considered, they have actually been with us since homo sapiens set foot on Earth. Frequent disasters struck the planet as far back as the time of its formation. The dinosaur went extinct because a meteorite struck the Earth 65 million years ago. However, if we concern ourselves with humans, then starting 200,000 years ago, ice ages, famines, attacks from rival groups or animals, and infections were constant reminders of human's vulnerability. We survived because we were programmed to do so.\n\nHumans deal with natural and manmade disasters with an uncanny mix of dread, trepidation, curiosity, and resignation, but they often rise to the challenge with acts of resourcefulness, courage, and unselfishness. Disasters are common occurrences in classical and modern literature. William Shakespeare's comedy The Tempest opens with a storm that becomes the driving force of the plot and tells of reconciliation after strife. Extreme weather forms the backdrop to three of the bard's greatest tragedies: Macbeth; Julies Caesar and King Lear. In Macbeth, the tempest is presented as unnatural and is preceded by \"portentious things\". Men enveloped in fire walked the streets, lions became tame, and night birds howled in the midday sun. Order is inverted, man acts against man, the gods and elements turn against humanity and mark their outrage with \"a tempest dropping fire\". In Julius Caesar, humanity's abominable actions are accompanied through violent weather. Caesar's murder is plotted while the sea swells, rage,s and foams, and \"All the sway of earth shakes like a thing unfirm\". In King Lear extreme weather conditions mirror acts of human depravity. The great storm that appears in Act 2, Scene 4, plays a crucial part in aiding Lear's tragic decline deeper into insanity.\n\nOn the popular culture front, disaster movies flourish in Hollywood, particularly in times of tribulation. Witness the following sample of the movie genre: San Francisco Does disaster bring out the worst in people? Thomas Glass, professor of epidemiology at The Johns Hopkins Uni-versity, argues the opposite [110,111]. From an evolutionary viewpoint, disasters bring out the best in us. It almost has to be that way. Humans survived ice ages, famines, and infections, not because we were strong or fast, but because in the state of extreme calamity, we tend to be resourceful and cooperative, except when there is a profound sense of injustice -that is, when some group has been mistreated or the system has failed. In such events, greed, selfishness, and violence do occur. A sense of breach of fairness can trigger the worst in people. Examples of those negative connotations include distributing the bird flu vaccine to the rich and mighty first, and the captain and crew escaping a sinking ferry before the passengers. The first of these two examples has not yet occurred, but the second is a real tragedy that recently took place (the sinking of Al-Salam Boccaccio ferry on 3 February 2006 in the Red Sea).\n\nIf reading history amazes you, you will find that the bird flu pandemic (or similar flu) wiped out a lot of the European population in the seventeenth century, before they cleaned it up. The bright side of a disaster is the reconstruction phase. Disasters are not always bad, even if we think they are. We need to look at what we learn and how we grow to become stronger after a disaster. For example, it is certain that the local, state and federal officials in the United States are now learning painful lessons from Hurricane Katrina, and will try to avoid the same mistakes again. It is up to us humans to learn from mistakes and not to forget them. However, human nature is forgetful, political leaders are not historians, and facts are buried.\n\nThe sociologist Henry Fischer [95] argues that certain human depravities commonly perceived to emerge during disasters (e.g., mob hysteria, panic, shock looting) are the exception not the rule. The community of individuals does not break down, and the norms that we tend to follow during normal times hold during emergency times. Emergencies bring out the best in us and we become much more altruistic. Proving his views using several case studies, Fischer writes about people who pulled through a disaster: \"Survivors share their tools, their food, their equipment, and especially their time. Groups of survivors tend to emerge to begin automatically responding to the needs of one another. They search for the injured, the dead, and they begin cleanup activities. Police and fire personnel stay on the job, putting the needs of victims and the duty they have sworn to uphold before their own personal needs and concern. The commonly held view of behavior is incorrect\" (pp. 18-19 of [95]). Fisher's observations are commonly accepted among modern sociologists. Indeed, as stated previously, we survived the numerous disasters encountered throughout the ages because we were programmed to do so.\n\nIt is always useful to learn from past disasters and to prepare better for the next one. Losses of lives and property from recent years are staggering. Not counting the manmade disasters that were tallied in Section 2, some frightening numbers from natural calamities alone are In the following twelve subsections we briefly recall a few manmade and natural disasters. The information herein and the accompanying photographs are mostly as reported in the online encyclopedia Wikipedia (http://en.wikipedia.org/wiki/Main -Page). The numerical data were cross-checked using archival media reports from such sources as The New York Times and ABC News. The numbers did not always match, and more than one source was consulted to reach the most reliable results. Absolute accuracy is not guaranteed, however. The dozen or so disasters sampled herein are not by any stretch of the imagination comprehensive, merely a few examples that may present important lessons for future calamities. Remember, they strike Earth at the average rate of three per day! The metric developed in Section 2 is applied in Section 8.13 to the thirteen disasters sampled in the present section.\n\nA major earthquake of magnitude 7.8 on the Richter scale struck the city of San Francisco, California, at around 5:12 am, Wednesday, 18 April 1906. The Great Earthquake, as it became known, was along the San Andreas Fault with its epicenter close to the city. Its violent shocks were felt from Oregon to Los Angeles and inland as far as central Nevada. The earthquake and resulting fires would go down in history as one of the worst natural disasters to hit a major U.S. city.\n\nAt the time only 478 deaths were reported, a figure concocted by government officials who believed that reporting the true death toll would hurt real estate prices and efforts to rebuild the city. This figure has been revised to today's conservative estimate of more than 3,000 victims. Most of the deaths occurred in San Francisco, but 189 were reported elsewhere across the San Francisco Bay Area. Other places in the Bay Area such as Santa Rosa, San Jose, and Stanford University also received severe damage.\n\nBetween 225,000 and 300,000 people were left homeless, out of a population of about 400,000. Half of these refugees fled across the bay to Oakland, in an evacuation similar to the Dunkirk Evacuation that would occur years later. Newspapers at the time described Golden Gate Park, the Panhandle, and the beaches between Ingleside and North Beach as covered with makeshift tents. The overall cost of the damage from the earthquake was estimated at the time to be around 400 million. The earthquake's notoriety rests in part on the fact that it was the first natural disaster of its magnitude to be captured by photography. Further more, it occurred at a time when the science of seismology was blossoming. Figures 4 depicts the devastation. Eight decades after the Great Earthquake, another big one struck the region. This became known as the Loma Prieta Earthquake. At 5:04 pm, on 17 October 1989, a magnitude 7.1 earthquake on the Richter scale severely shook the San Francisco and Monterey Bay regions. The epicenter was located at 37.04 • N latitude, 121.88 • W longitude near Loma Prieta peak in the Santa Cruz Mountains, approximately 14 km northeast of Santa Cruz and 96 km south-southeast of San Francisco. The tremor lasted for 15 seconds and occurred when the crustal rocks comprising the Pacific and North American Plates abruptly slipped as much as 2 m along their common boundary -the San Andreas Fault system (Section 4.8). The rupture initiated at a depth of 18 km and extended 35 km along the fault, but it did not break the surface of the Earth.\n\nThis major earthquake caused severe damage as far as 110 km away; most notably in San Francisco, Oakland, the San Francisco Peninsula, and in areas closer to the epicenter in the communities of Santa Cruz, the Monterey Bay, Watsonville, and Los Gatos. Most of the major property damage in the more distant areas resulted from liquefaction of soil used over the years to fill in the waterfront and then built. The magnitude and distance of the earthquake from the severe damage to the north were surprising to geotechnologists. Subsequent analysis indicates that the damage was likely due to reflected seismic waves -the reflection from well-known deep discontinuities in the Earth's gross structure, about 25 km below the surface.\n\nThere were at least 66 deaths and 3,757 injuries as a result of this earthquake. The highest concentration of fatalities, 42, occurred in the collapse of the Cypress structure on the Nimitz Freeway (Interstate 880), where a double-decker portion of the freeway collapsed, crushing the cars on the lower deck. One 15 m section of the San Francisco-Oakland Bay Bridge also collapsed causing two cars to fall to the deck below and leading to a single fatality. The bridge was closed for repairs for 1 month.\n\nBecause this earthquake occurred during the evening rush hour, there could have been a large number of cars on the freeways at the time, which on the Cypress structure could\n\nThe art and science of large-scale disasters have endangered many hundreds of commuters. Very fortunately, and in an unusual convergence of events, the two local Major League Baseball teams, the Oakland Athletics and the San Francisco Giants, were about to start their third game of the World Series, which was scheduled to start shortly after 5:30 pm. Many people had left work early or were participating in early after work group viewings and parties. As a consequence, the usually crowded highways were experiencing exceptionally light traffic at the time.\n\nExtensive damage also occurred in San Francisco's Marina District, where many expensive homes built on filled ground collapsed. Fires raged in some sections of the city as water mains broke. The San Francisco's fireboat Phoenix was used to pump salt water from San Francisco Bay using hoses dragged through the streets by citizen volunteers. Power was cut to most of San Francisco and was not fully restored for several days. Deaths in Santa Cruz occurred when brick storefronts and sidewalls in the historic downtown, which was then called the Pacific Garden Mall, tumbled down on people exiting the buildings. A sample of the devastation is shown in Fig. 5. The earthquake also caused an estimated $6 billion in property damage, the costliest natural disaster in U.S. history at the time. It was the largest earthquake to occur on the San Andreas Fault since the Great Earthquake. Private donations poured in to aid relief efforts, and on 26 October 1986, President George H. W. Bush signed a 3.45-billion earthquake relief package for California.\n\nHotel was built in Kansas City, Missouri, in 1978. A state-ofthe-art facility, this hotel boasted a forty-story hotel tower and conference facilities. These two components were connected by an open-concept atrium, within which three suspended walkways connected the hotel and conference facilities on the second, third and fourth levels. Due to their suspension, these walkways were referred to as \"floating walkways\" or \"skyways.\" The atrium boasted an area of 1,580 m 2 and was 15-m high. It seemed incredulous that such an architectural masterpiece could be involved in the United States' most devastating structural failure (not caused by earthquake, explosion, or airplane crash) in terms of loss of life and injuries.\n\nIn 17 July 1981, the guests at Kansas City Hyatt Regency Hotel witnessed the catastrophe. Approximately 2,000 people were gathered to watch a dance contest in the hotel lobby. Although the majority of the guests were on the ground level, some were dancing on the floating walkways on the second, third and fourth levels. At about 7:05 pm, a loud crack was heard as the second-and fourth-level walkways collapsed onto the ground level. This disaster took the lives of 114 people and left more than 200 injured.\n\nWhat did we learn from this manmade disaster? The project for constructing this particular hotel began in 1976 with Gillum-Colaco International, Inc., as the consulting structural engineering firm. Gillum-Colaco Engineering (G.C.E.) provided input into various plans that were being made by the architect and owner, and were contracted in 1978 to provide \"all structural engineering services for a 750-room hotel project\". Construction began in the spring of 1978. In the winter of 1978, Havens Steel Company entered the contract to fabricate and erect the atrium steel for the project under the standards of the American Institute of Steel Construction for steel fabricators. During construction in October 1979, part of the atrium roof collapsed. An inspection team was brought in to investigate the collapse and G.C.E. vowed to review all steel connections in the structure, including that of the roof.\n\nThe proposed structure details of the three walkways were as follows:\n\n• Wide-flange beams were to be used on either side of the walkway, which was hung from a box beam. • A clip angle was welded to the top of the box beam, which connected to the flange beams with bolts. • One end of the walkway was welded to a fixed plate, while the other end was supported by a sliding bearing. • Each box beam of the walkway was supported by a washer and a nut that were threaded onto the supporting rod. Because the bolt connection to the wide flange had virtually no movement, it was modeled as a hinge. The fixed end of the walkway was also modeled as a hinge, while the bearing end was modeled as a roller.\n\nDue to disputes between the G.C.E. and Havens, design changes from a single-to a double-hanger, rod-box beam connection were implemented. Havens did not want to have to thread the entire rod in order to install the washer and nut. This revised design consisted of the following:\n\n• One end of each support rod was attached to the atrium's roof cross-beams. • The bottom end went through the box beam where a washer and nut were threaded on to the supporting rods. • The second rod was attached to the box beam 10 cm from the first rod. • Additional rods suspended downward to support the second level in a similar manner.\n\nWhy did the design fail? Due to the addition of another rod in the actual design, the load on the nut connecting the fourth-floor segment was increased. The original load for each hanger rod was to be 90 kN, but with the design alteration the load was doubled to 181 kN for the fourth-floor box beam. Because the box beams were longitudinally welded, as proposed in the original design, they could not hold the weight of the two walkways. During the collapse, the box beam split and the support rod pulled through the box beams resulting in the fourth-and second-level walkways falling to the ground level.\n\nThe following paradigm clarifies the design failure of the walkways quite well. Suppose a long rope is hanging from a tree, and two people are holding onto the rope, one at the top and one near the bottom. Under the conditions that each person can hold their own body weight and that the tree and rope can hold both people, the structure would be stable. However, if one person was to hold onto the rope, and the other person was hanging onto the legs of the first, then the first person's hands must hold both people's body weights, and thus the grip of the top person would be more likely to fail. The initial design is similar to the two people hanging onto the rope, while the actual design is similar to the second person hanging from the first person's legs. The first person's grip is comparable to the fourth-level hanger-rod connection. The failure of this grip caused the walkway collapse.\n\nWho was responsible? One of the major problems with the Hyatt Regency project was the lack of communication between parties. In particular, the drawings prepared by G.C.E. were only preliminary sketches but were interpreted by Havens as finalized drawings. These drawings were then used to create the components of the structure. Another large error was G.C.E.'s failure to review the final design, which would have allowed them to catch the error in increasing the load on the connections. As a result, the engineers employed by G.C.E., who affixed their seals to the drawings, lost their engineering licenses in the states of Missouri and Texas. G.C.E. also lost its ability to be an engineering firm.\n\nAn engineer has a responsibility to his or her employer and, most important, to society. In the Hyatt Regency case, the lives of the public were hinged on G.C.E.'s ability to design a structurally sound walkway system. Their insufficient review of the final design lead to the failure of the design and a massive loss of life. Cases such as the Hyatt Regency walkway collapse are a constant reminder of how an error in judgment can create a catastrophe. It is important that events in the past are remembered so that engineers will always fulfill their responsibility to society.\n\nOn 17 August 1999, the Izmit Earthquake with a magnitude of 7.4 struck northwestern Turkey. It lasted 45 seconds and killed more than 17,000 people according to the government report. Unofficial albeit credible reports of more than 35,000 deaths were also made. Within 2 hours, 130 aftershocks were recorded and two tsunamis were observed.\n\nThe earthquake had a rupture length of 150 km from the city of Düuzce to the Sea of Marmara along the Gulf of Izmit. Movements along the rupture were as large as 5.7 m. The rupture passed through major cities that are among the most industrialized and urban areas of Turkey, including oil refineries, several car companies, and nthe avy headquarters and arsenal in Gölcük, thus increasing the severity of the life and property loss.\n\nThis earthquake occurred in the North Anatolian Fault Zone (NAFZ). The Anatolian Plate, which consists primarily of Turkey, is being pushed west by about 2 to 2.5 cm/yr, because it is squeezed between the Eurasian Plate on the north, and both the African Plate and the Arabian Plate on the south. Most of the large earthquakes in Turkey result as slip occurs along the NAFZ or a second fault to the east, the Eastern Anatolian Fault.\n\nImpacts of the earthquake were vast. These included in the short term, 4,000 buildings destroyed, including an army barracks, an ice skating rink, and refrigerated lorries used as mortuaries; cholera, typhoid, and dysentery were spread; homelessness and post-traumatic stress disorder were observered in around 25% of those living in the tent city set up by officials for the homeless. An oil refinery leaked into the water supply and Izmit Bay and, subsequently, caught fire. Because of the leak and the fire, the already highly polluted bay saw a two-to three-fold increase in polycyclic aromatic hydrocarbon levels compared to 1984 samples. Dissolved oxygen and chlorophyll reached their lowest levels in 15 years. Economic development was set back 15 years, and the direct damage of property was estimated at $18 billion, a huge sum for a developing country.\n\nA series of coordinated suicide attacks upon the United States were carried out on Tuesday, September 11, 2001, in which 19 hijackers took control of four domestic commercial airliners. The terrorists crashed two planes into the World Trade Center in Manhattan, New York City, one into each of the two tallest towers, about 18 minutes apart. Within 2 hours, both towers had collapsed. The hijackers crashed the third aircraft into the Pentagon, the U.S. Department of Defense headquarters, in Arlington County, Virginia. The fourth plane crashed into a rural field in Somerset County, Pennsylvania, 129 km east of Pittsburgh, following passenger resistance. The official count records 2,993 deaths in the attacks, including the hijackers, the worst act of war against the United States on its own soil 8 .\n\nThe National Commission on Terrorist Attacks Upon the United States (9/11 Commission) states in its final report that the nineteen hijackers who carried out the attack were terrorists affiliated with the Islamic Al-Qaeda organization. The report named Osama bin Laden, a Saudi national, as the leader of Al-Qaeda, and as the person ultimately suspected as being responsible for the attacks, with the actual planning being undertaken by Khalid Shaikh Mohammed. Bin Laden categorically denied involvement in two 2001 statements, before admitting a direct link to the attacks in a subsequent taped statement.\n\nThe 9/11 Commission reported that these hijackers turned the planes into the largest suicide bombs in history. The 9/11 attacks are among the most significant events to have occurred so far in the twenty-first century in terms of the profound economic, social, political, cultural, psychological, and military effects that followed in the United States and many other parts of the world. Following the September 11 disaster, the Global War on Terrorism was launched by the United States, enlisting the support of NATO members and other allies, with the stated goal of ending international terrorism and state sponsorship of the same. The difficulty of the war on terrorism, now raging for more than five years, is that it is mostly a struggle between a super power and a nebulously defined enemy: thousands of stateless, loosely connected, disorganized, undisciplined religion fanatics scattered around the globe, but particularly in Africa, Middle East, South Asia, and Southeast Asia.\n\nA tsunami is a series of waves generated when water in a lake or a sea is rapidly displaced on a massive scale. Earthquakes, landslides, volcanic eruptions, and large meteorite impacts all have the potential to generate a tsunami. The effects of a tsunami can range from unnoticeable to devastating. The Japanese term \"tsunami\" means harbor and wave. The term was created by fishermen who returned to port to find the area surrounding the harbor devastated, although they had not been aware of any wave in the open water. A tsunami is not a subsurface event in the deep ocean; it simply has a much smaller amplitude offshore and a very long wavelength (often hundreds of kilometers long), which is why it generally passes unnoticed at sea, forming only a passing \"hump\" in the ocean.\n\nTsunamis have been historically referred to as tidal waves because as they approach land, they take on the characteristics of a violent onrushing tide rather than the more familiar cresting waves that are formed by wind action on the ocean. However, because tsunamis are not actually related to tides, the term is considered misleading and its usage is discouraged by oceanographers.\n\nThe 2004 Indian Ocean Earthquake, known by the scientific community as the Sumatra-Andaman Earthquake, was an undersea earthquake that occurred at 00:58:53 UTC (07:58:53 local time) on 26 December 2004. According to the U.S. Geological Survey (USGS), the earthquake and its tsunami killed more than 283,100 people, making it one of the deadliest disasters in modern history. Indonesia suffered the worse loss of life at more than 168,000. The disaster is known in Asia and the media as the Asian Tsunami; in Australia, New Zealand, Canada and the United Kingdom it is known as as the Boxing Day Tsunami because it took place on Boxing Day, although it was still Christmas Day in the Western Hemisphere when the disaster struck.\n\nThe earthquake originated in the Indian Ocean just north of Simeulue Island, off the western coast of northern Sumatra, Indonesia. Various values were given for the magnitude of the earthquake that triggered the giant wave, ranging from 9.0 to 9.3 (which would make it the second largest earthquake ever recorded on a seismograph), although authoritative estimates now put the magnitude at 9.15. In May 2005, scientists reported that the earthquake itself lasted close to 10 minutes even though most major earthquakes last no more than a few seconds; it caused the entire planet to vibrate at least a few centimeters. It also triggered earthquakes elsewhere, as far away as Alaska.\n\nThe resulting tsunami devastated the shores of Indonesia, Sri Lanka, South India, Thailand, and other countries with waves up to 30 m high. The tsunami caused serious damage and death as far as the east coast of Africa, with the furthest recorded death due to the tsunami occurring at Port Elizabeth in South Africa, 8,000 km away from the epicentre. Figs. 7 and 8 show examples of the devastation caused by one of the deadliest calamities of the twenty-first century. The plight of the many affected people and countries prompted a widespread humanitarian response.\n\nUnlike in the Pacific Ocean, there is no organized alert service covering the Indian Ocean. This is partly due to the absence of major tsunami events between 1883 (the Krakatoa eruption, which killed 36,000 people) and 2004. In light of the 2004 Indian Ocean Tsunami, UNESCO and other world bodies have called for a global tsunami monitoring system.\n\nHuman's actions caused this particular natural disaster to become more damaging than it would otherwise. The intense coral reef mining off the Sri Lankan coast, which removed the sort of natural barrier that could mitigate the force of waves, amplified the disastrous effects of the tsunami. As a result of such mining, the 2004 Pacific Tsunami devastated Sri Lanka much more than it would have otherwise. Fig. 8. Satellite photographs of a coastal area. The receding tsunami wave is shown in the bottom photograph 8.6. Hurricane Katrina. Hurricane Katrina was the eleventh named tropical storm, fourth hurricane, third major hurricane, and first category 5 hurricane of the 2005 Atlantic hurricane season. It was the third most powerful storm of the season, behind Hurricane Wilma and Hurricane Rita, and the sixth strongest storm ever recorded in the Atlantic basin. It first made landfall as a category 1 hurricane just north of Miami, Florida, on 25 August 2005, resulting in a dozen deaths in South Florida and spawning several tornadoes, which fortunately did not strike any dwellings. In the Gulf of Mexico, Katrina strengthened into a formidable category 5 hurricane with maximum winds of 280 km/h and minimum central pressure of 902 mbar. It weakened considerably as it was approaching land, making its second landfall on the morning of 29 August along the Central Gulf Coast near Buras-Triumph, Louisiana, with 200 km/h winds and 920 mbar central pressure, a strong category 3 storm, having just weakened from category 4 as it was making landfall.\n\nThe sheer physical size of Katrina caused devastation far from the eye of the hurricane; it was possibly the largest hurricane of its strength ever recorded, but estimating the size of storms from before the presatellite 1960s era is difficult to impossible. On 29 August, Katrina's storm surge breached the levee system that protected New Orleans from Lake Pontchartrain and the Mississippi River. Most of the city was subsequently flooded, mainly by water from the lake. Heavy damage was also inflicted onto the coasts of Mississippi and Alabama, making Katrina the most destructive and costliest natural disaster in the history of the United States and the deadliest since the 1928 Okeechobee Hurricane.\n\nThe official combined direct and indirect death toll now stands at 1,836, the fourth highest in U.S. history, behind the Galveston Hurricane of 1900, the 1893 Sea Islands Hurricane, and possibly the 1893 Chenier Caminanda Hurricane, and ahead of the Okeechobee Hurricane of 1928. As of 20 December 2005, more than 4,000 people remain unaccounted for, so the death toll may still grow. As of 22 November 2005, 1,300 of those missing were either in heavily-damaged areas or were disabled and \"feared dead\"; if all 1,300 of these were to be confirmed dead, Katrina would surpass the Okeechobee Hurricane and become the second-deadliest in U.S. history and deadliest in over a century.\n\nMore than 1.2 million people were under an evacuation order before landfall. In Louisiana, the hurricane's eye made landfall at 6:10 am CDT on Monday, 29 August. After 11:00 am CDT, several sections of the levee system in New Orleans collapsed. By early September, people were being forcibly evacuated, mostly by bus to neighboring states. More than 1.5 million people were displaced -a humanitarian crisis on a scale unseen in the United States since the Great Depression. The damage is now estimated to be about $81.2 billion (2005 U.S. dollars), more than double the previously most expensive Hurricane Andrew, making Katrina the most expensive natural disaster in U.S. history.\n\nFederal disaster declarations blanketed 233,000 km 2 of the United States, an area almost as large as the United Kingdom. The hurricane left an estimated 3 million people without\n\nThe art and science of large-scale disasters electricity, taking some places several weeks for power to be restored (but faster than the 4 months originally predicted). Referring to the hurricane itself plus the flooding of New Orleans, Homeland Security Secretary Michael Chertoff described on 3 September that the aftermath of Hurricane Katrina as \"probably the worst catastrophe, or set of catastrophes\" in U.S. history.\n\nA sample of the devastation of Katrina is depicted in Fig. 9. The aftermath of the hurricane produced the perfect political storm whose winds lasted long after the hurricane. Congressional investigations reaffirmed what many have suspected: Governments at all levels failed. The city of New Orleans, the state of Louisiana, and the United States let the citizenry down. The whole episode was a study in ineptitude -and in buckpassing that fooled no one. The then-director of the Federal Emergency Management Agency, Michael Brown, did not know that thousands of New Orleans were trapped in the Superdome with subhuman conditions. In the middle of the bungled response, President George W. Bush uttered his infamous phrase \"Brownie, you're doin' a heckuva job\". Several books were published in the aftermath of the calamity, mostly offering scathing criticism of the government as well as more sensible strategies to handle future crises (e.g., Cooper & Block, 2006; Olasky, 2006). 09:20:38 India Standard Time, 08:50:38 local time at epicenter) on 8 October 2005, with the epicenter in the Pakistanadministered region of the disputed territory of Kashmir in South Asia. It registered 7.6 on the Richter scale, making it a major earthquake similar in intensity to the 1935 Quetta Earthquake, the 2001 Gujarat Earthquake, and the 1906 San Francisco Earthquake.\n\nMost of the casualties from the earthquake were in Pakistan where the official death toll is 73,276, putting it higher than the one massive scale of destruction of the Quetta earthquake of 31 May 1935. Most of the affected areas are in mountainous regions and access is impeded by landslides that have blocked the roads. An estimated 3.3 million people were left homeless in Pakistan. According to Indian officials, nearly 1,400 people died in the Indian-administered Kashmir region. The United Nations (UN) reported that more than 4 million people were directly affected. Many of them were at risk of dying from cold and the spread of disease as winter began. Pakistan Prime Minister Shaukat Aziz made an appeal to survivors on 26 October to come down to valleys and cities for relief. It has been estimated that damages incurred are well more than $5 billion US. Three of the five crossing points have been opened on the line of control between India and Pakistan. Figure 10 depicts a small sample of the utter devastation. 8.8. Hurricane Wilma. In the second week of October 2005, a large and complex area of low pressure developed over the western Atlantic and eastern Caribbean with several centers of thunderstorm activity. This area of disturbed weather southwest of Jamaica slowly organized on 15 October 2005 into tropical depression number 24. It reached tropical storm strength at 5:00 am EDT on 17 October, making it the first storm ever to use a \"W\" name since alphabetical naming began in 1950, and tying the 1933 record for most storms in a season. Moving slowly over warm water with little wind shear, tropical storm Wilma strengthened steadily and became a hurricane on 18 October. This made it the twelfth hurricane of the season, tying the record set in 1969.\n\nHurricane Wilma was the sixth major hurricane of the record-breaking 2005 Atlantic hurricane season. Wilma set numerous records for both strength and seasonal activity. At its peak, it was the most intense tropical cyclone ever recorded in the Atlantic Basin. It was the third category 5 hurricane of the season (the other two being hurricanes Katrina and Rita), the only time this has occurred in the Atlantic, and only the third category 5 to develop in October. Wilma was the second twenty-first storm in any season and the earliest-forming twenty-first storm by nearly a month.\n\nWilma made several landfalls, with the most destructive effects experienced in the Yucatán Peninsula of Mexico, Cuba, and the U.S. state of Florida. At least 63 deaths were reported, and damage is estimated at between $18 billion and $22 billion, with $14.4 billion in the United States alone, ranking Wilma among the top ten costliest hurricanes ever recorded in the Atlantic and the fifth costliest storm in U.S. history.\n\nFigure 11 shows one aspect of Hurricane Wilma. Around 4:00 pm EDT on 18 October 2005, the storm began to intensify rapidly. During a 10-hour period, Hurricane Hunter aircraft measured a 78-mbar pressure drop. In a 24-hour period from 8:00 am EDT 18 October to the following morning, the pressure fell 90 mbar. In this same 24-hour period, Wilma strengthened from a strong tropical storm with 110 km/h winds to a powerful category 5 hurricane with 280 km/h winds. In comparison, Hurricane Gilbert of 1988 -the previous recordholder for lowest Atlantic pressure -recorded a 78-mbar pressure drop in a 24-hour period for a 3 mbar/h pressure drop. This is a record for the Atlantic Basin and is one of the most rapid deepening phases ever undergone by a tropical cyclone anywhere on Earth -the record holder is 100 mbar by Super Typhoon Forrest in 1983. During its intensification on 19 October 2005, the eye's diameter shrank to 3 km -one of the smallest eyes ever seen in a tropical cyclone. Quickly thereafter, Wilma set a record for the lowest pressure ever recorded in an Atlantic hurricane when its central pressure dropped to 884 mbar at 8:00 am EDT and then dropped again to 882 mbar 3 hours later before rising slowly in the afternoon, while remaining a category 5 hurricane. In addition, at 11:00 pm EDT that day, Wilma's pressure dropped again to 894 mbar, as the storm weakened to a category 4 with winds of 250 km/h. Wilma was the first hurricane ever in the Atlantic Basin, and possibly the first tropical cyclone in any basin, to have a central pressure below 900 mbar while at category 4 intensity. In fact, only two other recorded Atlantic hurricanes have ever had lower pressures at this intensity; these two storms being previous Atlantic record holder Hurricane Gilbert of 1988 and the Labor Day Hurricane of 1935.\n\nAlthough Wilma was the most intense hurricane (i.e., a tropical cyclone in the Atlantic, Central Pacific, or Eastern Pacific) ever recorded, there have been many more intense typhoons in the Pacific. Super Typhoon Tip is the most intense tropical cyclone on record at 870 mbar. Hurricane Wilma existed within an area of ambient pressure that was unusually low for the Atlantic Basin, with ambient pressures below 1,010 mbar. These are closer to ambient pressures in the northwest Pacific Basin. Indeed, under normal circumstances, the Dvorak matrix would equate an 890 mbar storm in the Atlantic basin -a current intensity (CI) number of 8 -with an 858 mbar storm in the Pacific. Such a conversion, if normal considerations were in play, would suggest that Wilma was more intense than Tip. However, Wilma's winds were much slower than the 315 km/h implied by an 8 on the Dvorak scale. A speeds of 280+ km/h may seem incredibly fast, but for an 882mbar hurricane it is actually quite slow. In comparison, Hurricane Gilbert had a pressure of 888 mbar but winds of 300 km/h. In fact, at one point after Wilma's period of peak intensity, it had a pressure of 894 mbar, but was actually not even a category 5, with winds of just 250 km/h. Before Wilma, it had been unheard of for a storm to go under 900 mbar and not be a category 5. These wind speeds indicate that the low ambient pressure surrounding Wilma caused the 882 mbar pressure to be less significant than under normal circumstances, involving a lesser pressure gradient. By the gradient standard, it is entirely possible that Hurricane Gilbert, and not Wilma, is still the strongest North Atlantic hurricane on record. Hurricane Wilma's southeast eyewall passed the greater Key West area in the lower Florida Keys in the early morning hours of 24 October 2005. At this point, the storm's eye was approximately 56 km in diameter, and the north end of the eye wall crossed into the south and central section of Palm Beach County as the system cut a diagonal swath across the southern portion of the Florida peninsula. Several cities in the South Florida Metropolitan Area, which includes Palm Beach, Fort Lauderdale, and Miami, suffered severe damage as a result of the intense winds of the rapidly moving system. The center of the eye was directly over the South Florida Metropolitan Area at 10:30' am on Monday, 24 October. After the hurricane had already passed, there was a 3-m storm surge from the Gulf of Mexico that completely inundated a large portion of the lower Keys. Most of the streets in and near Key West were flooded with at least 1 m of salt water, causing the destruction of tens of thousands of vehicles. Many houses were also flooded with 0.5 m of sea water.\n\nDespite significant wind shear in the Gulf, Hurricane Wilma regained some strength before making a third landfall\n\nThe art and science of large-scale disasters just north of Everglades City, Florida, near Cape Romano, at 6:30 am EDT, 24 October 2005, as a category 3 hurricane. The reintensification of Hurricane Wilma was due to its interaction with the Gulf Loop Current. At landfall, Wilma had sustained winds of 200 km/h. Over the Florida peninsula, Wilma weakened slightly to a category 2 hurricane, and exited Florida and entered the Atlantic at that strength about 6 hours later. Unexpectedly, Wilma regained strength over the Gulf Stream and once again became a category 3 hurricane north of the Bahamas, regaining all the strength it lost within 12 hours. However, on 25 October, the storm gradually began weakening and became extratropical late that afternoon south of Nova Scotia, although it still maintained hurricane strength and affected a large area of land and water with stormy conditions.\n\nThere have been many serious incidents during the Hajj that have led to the loss of hundreds of lives. The Hajj is the Islamic annual pilgrimage to the city of Mecca, Saudi Arabia. There are an estimated 1.3 billion Muslims living today, and during the month of the Hajj, the city of Mecca must cope with as many as 4 million pilgrims. The Muslim world follows a lunar calendar, and therefore, the Hajj month shifts from year to year relative to the Western, solar calendar.\n\nJet travel also makes Mecca and the Hajj more accessible to pilgrims from all over the world. As a consequence, the Hajj has become increasingly crowded. City officials are consequently required to control large crowds and provide food, shelter, and sanitation for millions. Unfortunately, they have not always been able to prevent disasters, which are hard to avoid with so many people. The worst of the incidents has occurred during the ritual stoning of the devil, an event near the tail end of the Hajj. Saudi authorities had replaced the pillar, which had represented the devil in the past, with an oval wall with padding around the edges to protect the crush of pilgrims. The officials had also installed cameras and dispatched about 60,000 security personnel to monitor the crowds.\n\nOn 12 January 2006, a stampede during the ritual stoning of the devil on the last day of the Hajj in Mina, Saudi Arabia, killed at least 346 pilgrims and injured at least 289 more. The stoning ritual is the most dangerous part of the pilgrimage because the ritual can cause people to be crushed, particularly as they traverse the massive two-layer flyover-style Jamarat Bridge that affords access to the pillars. The incident occurred shortly after 1:00 pm local time, when a passenger bus shed its load of travelers at the eastern access ramps to the bridge. This caused pilgrims to trip, rapidly resulting in a lethal crush. An estimated 2 million people were performing the ritual at the time. Tragically, the stampede was the second fatal tragedy of the Islamic month of Dhu al-Hijjah in 2006. On 5 January 2006, the Al Ghaza Hotel had collapsed. The death toll was seventy-six and the number of injured was sixty-four.\n\nThere is a long and tragic history for the Hajj stampede. The surging crowds, trekking from one station of the pilgrimage to the next, cause a stampede. Panic spreads, pilgrims jostle to avoid being trampled, and hundreds of deaths can result. A list of stampede and other accidents during the Hajj season follows.\n\n• In December 1975, an exploding gas cylinder caused a fire in a tent colony; 200 pilgrims were killed. • On 20 November 1979, a group of approximately 200 militant Muslims occupied Mecca's Grand Mosque. They were driven out by special commandos -allowed into the city under these special circumstances despite their being non-Muslims -after bloody fighting that left 250 people dead and 600 wounded. • On 31 July 1987, Iranian pilgrims rioted, causing the deaths of more than 400 people. • On 9 July 1989, two bombs exploded, killing one pilgrim and wounding sixteen. Saudi authorities beheaded sixteen Kuwaiti Shiite Muslims for the bombings after originally suspecting Iranian terrorists. • On 15 April 1997, 343 pilgrims were killed and 1,500 injured in a tent fire. • On 2 July 1990, a stampede inside a pedestrian tunnel -Al-Ma'aisim tunnel -leading out from Mecca toward Mina and the Plains of Arafat led to the deaths of 1,426 pilgrims. • On 23 May 1994, a stampede killed at least 270 pilgrims at the stoning of the devil ritual. • On 9 April 1998, at least 118 pilgrims were trampled to death and 180 injured in an incident on Jamarat Bridge. • On 5 March 2001, 35 pilgrims were trampled in a stampede during the stoning of the devil ritual. • On 11 February 2003, the stoning of the devil ritual claimed 14 pilgrims' lives. • On 1 February 2004, 251 pilgrims were killed and another 244 injured in a stampede during the stoning ritual in Mina. • A concrete multistory building located in Mecca close to the Grand Mosque collapsed on 5 January 2006. The building -Al Ghaza Hotel -is said to have housed a restaurant, a convenience store, and a hostel. The hostel was reported to have been housing pilgrims to the 2006 Hajj. It is not clear how many pilgrims were in the hotel at the time of the collapse. As of the latest reports, the death toll is 76, and the number of injured is 64.\n\nCritics say that the Saudi government should have done more to prevent such tragedies. The Saudi government insists that any such mass gatherings are inherently dangerous and difficult to handle, and that they have taken a number of steps to prevent problems.\n\nOne of the biggest steps, that is also controversial is a new system of registrations, passports, and travel visas to control the flow of pilgrims. This system is designed to encourage and accommodate first-time visitors to Mecca, while imposing restrictions on those who have already embarked on the trip multiple times. Pilgrims who have the means and desire to perform the Hajj several times have protested what they see as discrimination, but the Hajj Commission has stated that they see no alternative if further tragedies are to be prevented.\n\nFollowing the 2004 stampede, Saudi authorities embarked on major construction work in and around the Jamarat Bridge area. Additional accessways, footbridges, and emergency exits were built, and the three cylindrical pillars were replaced with longer and taller oblong walls of concrete to enable more pilgrims simultaneous access to them without the jostling and fighting for position of recent years. The government has also announced a multimillion-dollar project to expand the bridge to five levels; the project is planned for completion in time for the 1427 AH Hajj (December 2006 -January 2007).\n\nSmith and Dickie's [112] book is about engineering for crowd safety, and they list dozens of crowd disasters, including the recurring Hajj stampedes. Helbing et al. [10] discuss simulation of panic situations from the point of view of nonlinear dynamical systems theory. The vessel was rebuilt in 1991 by INMA at La Spezia, maintaining the same outer dimensions albeit with a higher superstructure, changing the draught to 5.90 m. At the same time, its automobile capacity was increased to 320, and the passenger capacity was increased to 1,300. The most recent gross registered tonnage was 11,799.\n\nThe Boccaccio was purchased in 1999 by al-Salam Maritime Transport, headquartered in Cairo, the largest private shipping company in Egypt and the Middle East, and renamed al-Salam Boccaccio 98; the registered owner is Pacific Sunlight Marine, Inc., of Panama. The Ferry is also referred to as Salam 98.\n\nAt the doomed voyage, the ship was carrying 1,312 passengers and 96 crew members, according to Mamdouh Ismail, head of al-Salaam Maritime Transport. Originally, an Egyptian embassy spokesman in London had mentioned 1,310 passengers and 105 crew, while the Egyptian presidential spokesman mentioned 98 crew and the Transport Minister said 104. The majority of passengers are believed to have been Egyptians working in Saudi Arabia. Passengers also included pilgrims returning from the Hajj in Mecca. The ship was also carrying about 220 vehicles.\n\nFirst reports of statements by survivors indicated that smoke from the engine room was followed by a fire that continued for some time. There were also reports of the ship listing soon after leaving port, and that after continuing for some hours the list became severe and the ship capsized within 10 minutes as the crew fought the fire. In a BBC radio news broadcast, an Egyptian ministerial spokesman said that the fire had started in a storage area, was controlled, but then started again. The ship turned around and as it turned the capsize occurred. The significance of the fire was supported by statements attributed to crew members who were reported to claim that the firefighters essentially sank the ship when sea water they used to battle the fire collected in the hull because drainage pumps were not working.\n\nThe Red Sea is known for its strong winds and tricky local currents, not to mention killer sharks. The region had been experiencing high winds and dust storms for several days at the time of the sinking. These winds may have contributed to the disaster and may have complicated rescue efforts.\n\nThere are several theories expressed about possible causes of the sinking:\n\n• Fire: Some survivors dragged from the water reported that there was a large fire on board before the ship sank, and there were eyewitness accounts of thick black smoke coming from the engine rooms. • Design flaws: The al-Salam Boccaccio 98 was a roll onroll off (ro-ro) ferry. This is a design that allows vehicles to drive on one end and drive off the other. This means that neither the ship nor any of the vehicles need to turn around at any point. It also means that the cargo hold is one long chamber going through the ship. To enable this to work, the vehicle bay doors must be very near the waterline, so if these are sealed improperly, water may leak through. Even a small amount of water moving about inside can gain momentum and capsize the ship, what is known as the free surface effect. • Modifications: In the 1980s, the ship was reported to have had several modifications, including the addition of two passenger decks, and the widening of cargo decks. This would have made the ship less stable than it was designed to be, particularly as its draught was only 5.9 m. Combined with high winds, the tall ship could have been toppled easily. • Vehicle movement: Another theory is that the rolling ship could have caused one or more of the 220 vehicles in its hold to break loose and theoretically be able to puncture a hole in the side of the ship.\n\nAt 23:58 UTC on 2 February 2006, the air -sea rescue control room at RAF Kinloss in Scotland detected an automatic distress signal relayed by satellite from the ship's position. The alert was passed on via France to the Egyptian authorities, but almost 12 hours passed before a rescue attempt was launched. As of 3 February 2006, some lifeboats and bodies were seen in the water. It was then believed that there were still survivors. At least 314 survivors and around 185 dead bodies have been recovered. Reuters reported that \"dozens\" of bodies were floating in the Red Sea.\n\nRescue boats and helicopters, including four Egyptian frigates, searched the area. Britain diverted the warship HMS Bulwark that would have arrived in a day and a half, but reports conflict as to whether the ship was indeed recalled. Israeli sources report that an offer of search-and-rescue assis-\n\nThe art and science of large-scale disasters tance from the Israeli Navy was declined. Egyptian authorities did, however, accept a United States offer of a P-3 Orion maritime naval patrol aircraft after initially having said that the help was not needed.\n\nThe sinking of al-Salam Boccaccio 98 is being compared to that of the 1987 M/S Herald of Free Enterprise disaster, which killed 193 passengers, and also to other incidents. In 1991, another Egyptian ferry, the Salem Express, sunk off the coast of Egypt after hitting a small habili reef; 464 Egyptians lost their lives. The ship is now a landmark shipwreck for SCUBA divers along with the SS Thistlegorm. In 1994, the M/S Estonia sank, claiming 852 lives. On 26 September 2002, the M/S Joola, a Senegalese government-owned ferry, capsized off the coast of Gambia, resulting in the deaths of at least 1,863 people. On 17 October 2005, the Pride of al-Salam 95, a sister ship of the al-Salam Boccaccio 98, also sank in the Red Sea, after being struck by the Cypriot-registered cargo ship Jebal Ali. In that accident, 2 people were killed and another 40 injured, some perhaps during a stampede to leave the sinking ship. After evacuating all the ferry passengers and crew, the Jebal Ali went astern and the Pride of al-Salam 95 sank in about 3 minutes.\n\nWhat is most tragic about the al-Salam Boccaccio 98's incident is the utter ineptness, corruption, and collusion of both the Egyptian authorities and the holding company staff, particularly its owner, a member of the upper chamber of Parliament and a close friend to an even more powerful politician in the inner circle of the president. The 35-year-old ferry was not fit for sailing, and was in fact prevented from doing so in European waters, yet licensed to ferry passengers despite past violations and other mishaps by this and other ships owned by the same company. The captain of the doomed ferry refused to turn the ship around to its nearer point of origin despite the fire on board, and a passing ship owned by the same company ignored the call for help from the sinking ferry. Rescue attempts by the government did not start for almost 12 hours after the sinking, despite a distress signal from the ship that went around the globe and was reported back to the Egyptian authorities. Many officials failed to react promptly because an \"important\" soccer game was being televised. Rescued passengers told tales of the ship's crew, including the captain, taking the few lifeboats available to themselves before attempting to help the helpless passengers. The company's owner and his family were allowed to flea the country shortly after the disaster despite a court order forbidding them from leaving Egypt. Local news media providedinaccurate reporting and then ignored the story altogether within a few weeks to focus on another important soccer event. Victims and their relatives were left to fend for themselves, all because they were the poorest of the poor, insignificant to the rich, powerful and mighty. Disasters occur everywhere, but in a civilized country, inept response as occurred in Egypt would have meant the fall of the government, the punishment of few a criminals and, most important, less tragic loss of life.\n\nA flu pandemic occurs when a new influenza virus emerges for which people have little or no immunity, and for which there is no vaccine. The disease spreads easily from person to person, causes serious illness, and can sweep across countries and around the world in a very short time. It is difficult to predict when the next influenza pandemic will occur or how severe it will be. Wherever and whenever a pandemic starts, everyone around the world is at risk. Countries might, through measures such as border closures and travel restrictions, delay arrival of the virus, but thry cannot prevent it or stop it.\n\nThe highly pathogenic avian H5N1 avian flu is caused by influenza A viruses that occur naturally among birds. There are different subtypes of these viruses because of changes in certain proteins (hemagglutinin [HA] and neuraminidase [NA]) on the surface of the influenza A virus and the way the proteins combine. Each combination represents a different subtype. All known subtypes of influenza A viruses can be found in birds. The avian flu currently of concern is the H5N1 subtype.\n\nWild birds worldwide carry avian influenza viruses in their intestines, but they usually do not get sick from them. Avian influenza is very contagious among birds and can make some domesticated birds, including chickens, ducks, and turkeys, very sick and even kill them. Infected birds shed influenza virus in their saliva, nasal secretions, and feces. Domesticated birds may become infected with avian influenza virus through direct contact with infected waterfowl or other infected poultry, or through contact with surfaces (e.g., dirt or cages) or materials (e.g., water or feed) that have been contaminated with the virus.\n\nAvian influenza infection in domestic poultry causes two main forms of disease that are distinguished by low and high extremes of virulence. The \"low pathogenic\" form may go undetected and usually causes only mild symptoms such as ruffled feathers and a drop in egg production. However, the highly pathogenic form spreads more rapidly through flocks of poultry. This form may cause disease that affects multiple internal organs and has a mortality rate that can reach 90% to 100%, often within 48 hours.\n\nHuman influenza virus usually refers to those subtypes that spread widely among humans. There are only three known A subtypes of influenza viruses (H1N1, H1N2, and H3N2) currently circulating among humans. It is likely that some genetic parts of current human influenza A viruses originally came from birds. Influenza A viruses are constantly changing, and other strains might adapt over time to infect and spread among humans. The risk from avian influenza is generally low to most people because the viruses do not usually infect humans. H5N1 is one of the few avian influenza viruses to have crossed the species barrier to infect humans, and it is the most deadly of those that have crossed the barrier.\n\nSince 2003, a growing number of human H5N1 cases have been reported in Azerbaijan, Cambodia, China, Egypt, Indonesia, Iraq, Thailand, Turkey, and Vietnam. More than half of the people infected with the H5N1 virus have died. Most of these cases are all believed to have been caused by exposure to infected poultry (e.g., domesticated chicken, ducks, and turkeys) or surfaces contaminated with secretion/excretions from infected birds. There has been no sustained human-tohuman transmission of the disease, but the concern is that H5N1 will evolve into a virus capable of human-to-human transmission. The virus has raised concerns about a potential human pandemic because it is especially virulent; it is being spread by migratory birds; it can be transmitted from birds to mammals and, in some limited circumstances, to humans; and similar to other influenza viruses, it continues to evolve.\n\nIn 2005, animals perished by the bird flu were left in the muddy streets of a village in Egypt, exasperating an already dire situation. Rumors were rampant about contaminating the entire water supply in Egypt, which comes from the Nile River. Cases of the deadly H5N1 bird flu virus have been reported in at least fifteen governorates, and widespread panic among Egyptians has been reported. The Egyptian government has ordered the slaughter of all poultry kept in homes as part of an effort to stop the spread of bird flu in the country. A ban on the movement of poultry between governorates is in place. Measures already announced include a ban on the import of live birds, and officials say there have been no human cases of the disease. The government has called on Egyptians to stay calm, and not to dispose of slaughtered or dead birds in the roads, irrigation canals, or the Nile River.\n\nSymptoms of avian influenza in humans have ranged from typical human influenza like symptoms (e.g., fever, cough, sore throat, muscle aches) to eye infections, pneumonia, severe respiratory diseases such as acute respiratory distress, and other severe and life-threatening complications. The symptoms of avian influenza may depend on which virus caused the infection.\n\nA pandemic may come and go in waves, each of which can last for six to eight weeks. An especially severe influenza pandemic could lead to high levels of illness, death, social disruption, and economic loss. Everyday life would be disrupted because so many people in so many places would become seriously ill at the same time. Impacts can range from school and business closings to the interruption of basic services such as public transportation and food delivery.\n\nIf a pandemic erupts, a substantial percentage of the world's population will require some form of medical care. Health care facilities can be overwhelmed, creating a shortage of hospital staff, beds, ventilators, and other supplies. Surge capacity at nontraditional sites such as schools may need to be created to cope with demand. The need for vaccine is likely to outstrip supply, and the supply of antiviral drugs is also likely to be inadequate early in a pandemic. Difficult decisions will need to be made regarding who gets antiviral drugs and vaccines. Death rates are determined by four factors: the number of people who become infected, the virulence of the virus, the underlying characteristics and vulnerability of affected populations, and the availability and effectiveness of preventive measures.\n\nThe U.S. government site (http://www.pandemicflu.gov/ general/) lists the following pandemic death tolls since 1900:\n\n• 1918-1919; United States 675,000+; worldwide 50 mil-lion+, • 1957-1958; United States 70,000+; worldwide 1-2 million, • 1968-1969; United States 34,000+; worldwide 700,000+.\n\nThe United States is collaborating closely with eight international organizations, including the UN's World Health Organization (WHO), the Food and Agriculture Organization also of the UN, the World Organization for Animal Health, and 88 foreign governments to address the situation through planning, greater monitoring, and full transparency in reporting and investigating avian influenza occurrences. The US and its international partners have led global efforts to encourage countries to heighten surveillance for outbreaks in poultry and significant numbers of deaths in migratory birds and to rapidly introduce containment measures. The U.S. Agency for International Development and the U.S. Department of State, Department of Health and Human Services, and Department of Agriculture are coordinating future international response measures on behalf of the White House with departments and agencies across the federal government. Together, steps are being taken to minimize the risk of further spread in animal populations, reduce the risk of human infections, and further support pandemic planning and preparedness. Ongoing detailed mutually coordinated onsite surveillance and analysis of human and animal H5N1 avian flu outbreaks are being conducted and reported by the USGS National Wildlife Health Center, the Centers for Disease Control and Prevention, the WHO, the European Commission, and others. 8.12. Energy crisis/global warming. The energy crisis and its intimately related global warming problem are two examples of slowly-evolving disasters that do not get the attention they deserve, at least until recently. Energy crisis is defined as any great shortfall (or price rise) in the supply of energy resources to an economy. There is no immediacy to this type of calamity, despite the adverse effects on the health, economic, and social well-being of billions of people around the globe. Herein I offer a few personal reflections on energy, global warming and the looming crisis, with the United States in mind. The arguments made, however, may apply with equal intensity to many other countries.\n\nNothing can move let alone survive without it. Yet, until a gallon of gas hit $4, the word energy was rarely uttered during the 2008 presidential campaign. Promises to effect somehow a lower price of gas at the pump, or of a Federal gas tax break during this summer, are at best a short-term band-aid to what should be a much broader and longer-term national debate. During two visits to Saudi Arabia that took place 15 January 2008 and 16 May 2008, President Bush pleaded with King Abdullah to open the oil spigots, while the Royal told his eminent visitor how worried he is about the impact of oil prices on the world economy. The spigots did not open; and even if they were, such pleas and worries are not going to solve the energy problem or the global warming crisis.\n\nMuch like company executives, politicians mind, envision and articulate issues in terms of years, not decades. A fouryear horizon is about right, as this is the term for a president, twice that for a representative, and two-third of a senate term.\n\nThe tenure of a typical CEO is even shorter than that for a senator. But the debate on energy should ideally be framed in terms of a human lifespan, currently about 75 years. The reason is two folds. First, fossil fuel, such as oil, gas and coal, is being consumed at a much faster rate than nature can make it. These are not renewable resources. Considering the anticipated population growth (with a conservative albeit unrealistic assumption of no increase in the per capita demand) and the known reserves of this type of energy sources, the world supply of oil is estimated to be exhausted in 0.5 lifespan, of gas in one lifespan, and of coal in 1.5 lifespan. Second, alternative energy sources must be developed to prevent a colossal disruption of our way of life. But, barring miracles, those cannot be found overnight, but rather over several decades of intensive research and development. The clock is ticking, and few people seem to be listening to the current whisper and, inevitably, the future thunder.\n\nUranium fission power plants currently supply about 8% of the U.S. total energy need, which is about 100 Quad/year or 10 20 Joule/year. (Total energy consumed is in the form of electricity, 40%, the burning of fossil fuel to directly generate heat for buildings and industrial processes, 30%, and mechanical energy for transportation systems, 30%.) Coal, natural gas and nuclear power plants respectively generate 50, 20 and 20% of our electricity need. The corresponding numbers in France are 4, 4 and 80%. Even at that modest rate of consumption and with current nuclear reactor technology, the United States will exhaust its supply of uranium in about two lifespan. Real and imagined concerns about the safety of nuclear energy and depositions of their spent fuel have brought to a halt all new constructions since the mid 1970s. Happily, 2007 breezed new life into the nuclear issue. There are now 7 new nuclear reactors in the early planning stages for the U.S. market, and over 65 more for China, Japan, India, Russia and South Korea.\n\nFission-based power generation not only can reduce the country's insatiable appetite for fossil fuel but also no carbon dioxide or any other heat-trapping gases is generated as a result of nuclear power generation. Along with other pollutants, a coal-fired power plant, in contrast, annually releases 10 billion kg of carbon dioxide into the atmosphere for each 1,000 MW of (fully utilized) electric capacity. Nuclear power generation must be part of the solution to both the energy and global warming crises.\n\nControlled nuclear fusion, also a non-polluting source of energy, has the potential to supply inexhaustibly all of our energy need, but, even in the laboratory, we are far from achieving the breakeven point (meaning getting more energy from the reactor than needed to sustain the reaction).\n\nWith 5% of the world population, the United States consumes 25% of the world annual energy usage, generating in the process a proportional amount of greenhouse gases. Conservation alone is not going to solve the problem; it will merely relegate the anticipated crises to a later date. A whopping 20% conservation effort this year will be wiped out by a 1% annual population increase over the next 20 years. But that does not mean it shouldn't be done. Without conservation, the situation will be that much worse.\n\nThe energy crises exemplified by the 1973 Arab oil embargo brought about a noticeable shift of attitudes toward energy conservation. During the 1970s and 1980s, governments, corporations and citizens around the world but particularly in the industrialized countries invested valuable resources searching for methods to conserve energy. Dwellings and other buildings became better insulated, and automobiles and other modes of transportation became more energy efficient. Plentiful fossil fuel supplies during the 1990s and the typical short memory of the long gas lines during 1973 have, unfortunately, somewhat dulled the urgency and enthusiasm for energy conservation research as well as practice. Witness -at least in the United States -the awakening of the long-hibernated gas-guzzler automobile and the recent run on house-size sport utility vehicles, a.k.a. land barges. The $140 plus barrel of crude oil this year has reignited interest in conservation. But in my opinion, the gas at the pump needs to skyrocket to a painful $10 per gallon to have the required shock value. The cost is close to that much in Europe, and the difference in attitudes between the two continents is apparent.\n\nConservation or not, talk of energy independence is just that, unless alternative energy sources are developed. The United States simply does not have traditional energy sources in sufficient quantities to become independent. In fact, our energy dependence has increased steadily since the 1973 oil crisis. The nontraditional sources are currently either nonexistent or too expensive to compete even with the $4 per gallon at the pump. But a $10 price tag will do the trick, one day.\n\nHow do we go from here to there? We need to work on both the supply side and the demand side. On the latter, consumers need to moderate their insatiable appetite for energy. Homes do not have to be as warm in the winter as a crowded bus, or as cold in the summer as a refrigerator. A car with a 300-horsepower engine (equivalent to 300 live horses, really) is not needed to take one person to work via congested city roads. Additionally, new technology can provide even more efficient air, land and sea vehicles than exist today. Better insulated buildings, less wasteful energy conversion, storage and transmission systems, and many other measures save energy; every bit helps.\n\nOn the supply side, we need to develop the technology to deliver nontraditional energy sources inexpensively, safely and with minimum impact on the environment. The U.S. and many other countries are already searching for those alternative energy sources. But are we searching with sufficient intensity? Enough urgency? I think not, simply because the problem does not affect, with sufficient pain, this or the next presidential election, but rather the 5 th or 10 th one down the road. Who is willing to pay more taxes now for something that will benefit the next generation? Witness the unceremonious demise of former President Carter's Energy Security Corporation, which was supposed to kick off with the issuance of $5 billion energy bonds. One way to assuage the energy problem is to increase usage taxes, thus help curb demands, and to use the proceeds to develop new supplies. Amazingly, few politicians are considering decreasing those taxes.\n\nLet us briefly appraise the nontraditional sources known or even (sparingly) used today. The listing herein is not exhaustive, and other technologies unforeseen today may be developed in the future. Shale oil comes from sedimentary rock containing dilute amounts of near-solid fossil fuel. The cost, in dollar as well as in energy, of extracting and refining that last drop of oil is currently prohibitive. Moreover, the resulting fuel is not any less polluting than other fossil fuels. There are also the so-called renewable energy sources. Though the term is a misnomer because once energy is used it is gone forever, those sources are inexhaustible in the sense that they cannot be used faster than nature makes them. The Sun is the source of all energy on Earth, providing heat, light, photosynthesis, winds, waves, life and its eventual albeit very slow decay into fossil fuel, etc. Renewable energy sources will always be here as long as the Sun stays alight, hopefully for a few more billion years.\n\nUsing the Sun radiation, when available, to generate either heat or electricity is limited by the available area, the cost of the heat collector or the photovoltaic cell, and the number of years of operation it takes the particular device to recover the energy used in its manufacturing. The U.S. is blessed with its enormous land, and can in principle generate all of its energy need via solar cells utilizing less than 3% of available land area. Belgium, in contrast, requires an unrealistic 25% of its land area to supply its energy need using the same technology. Solar cells are presently inefficient as well as expensive. They also require about 5 years of constant operation just to recover the energy spent on their manufacturing. Extensive R&D is needed to improve on all those fronts.\n\nWind energy though not constant is also inexhaustible, but has similar limitations to those of solar cells. Without tax subsidies, generating electricity via windmills currently cannot compete with fossil fuel or even nuclear power generation. Other types of renewable energy sources include hydroelectric power; biomass; geophysical and oceanic thermal energy; and ocean waves and tides. Food-based biomass is a low-carbon fuel when compared to fossil oil. Depending on how they are produced, however, biofuels may or may not offer net reduction of carbon dioxide emissions (Science, DOI: 10.1126/science.1152747, published online 7 February 2008). Hydrogen provides clean energy, but has to be made using a different source of energy, for example photovoltaic cells. Despite all the hype, the hydrogen economy is not a net energy saver, but has other advantages nevertheless. Even such noble cause as hydrogen-fueled or battery-powered automobiles will reduce pollution and dependence on fossil fuel only if nuclear power or other non-fossil, non-polluting energy sources are used to produce the hydrogen or to generate the electricity needed to charge the batteries.\n\nAre we investing enough to solve the energy crisis? We recite some alarming statistics provided in a recent article [113] by the then chair of the U.S. Senate Energy and Natural Resources Committee, Pete V. Domenici. Federal funding for energy Research and Development has been declining for years, and it is not being made up by increased private-sector R&D expenditure. Over the 25-year period from 1978 to 2004, federal appropriations fell from $6.4 billion to $2.75 billion in constant 2000 dollars, nearly 60% reduction. Private sector investment fell from about $4 billion to $2 billion during the period from 1990 to 2006. Compared to high-technology industries, energy R&D expenditure is the least intensive. For example, the private sector R&D investment is about 12% of sales in the pharmaceuticals industry and 15% in the airline industry, while the combined federal and private-sector energy R&D expenditure is less than 1% of total energy sales.\n\nWhat is now needed is a visionary leader that will inspire the nation to accept the pain necessary to solve its energy problems and in the process help the world slow down global warming. The goal is to reduce significantly the country's dependence on foreign and domestic fossil fuel, replenishing the deficit with renewable, non-polluting sources of energy. The scale of the challenge is likely to be substantially larger than that of the 1940s Manhattan Project or the 1960s Apollo program. In his 'malaise' speech of July 15, 1979, Jimmy Carter lamented, \"Why have we not been able to get together as a nation to resolve our serious energy problem?\" Why not indeed Mr. President.\n\nIn this subsection we evaluate the scope of the thirteen case studies (two earthquakes are discussed in a single subsection, 8.1) used as examples of natural and manmade disasters. The metric introduced in Section 2 is utilized to rank those disasters. Recall, the scope of a disaster is based on the number of people adversely affected by the extreme event (killed, injured, evacuated, etc.) or the extent of the stricken geographical area. The results are summarized in Table 1.\n\nFor Izmit earthquake the number of deaths reported by the government differs from that widely believed to be the case, hence the range shown in the table. Either number puts the disaster at the worst possible category, V, and therefore the number of injured or homeless becomes immaterial to the categorization; the scope cannot get any higher.\n\nOf note is the scope of the September 11 manmade disaster, which is less than the scope of, say, hurricane Katrina. The number of people directly and adversely affected by September 11 is less than those in the case of Katrina (number of deaths is not the only measure). On the other hand, September 11 has a huge after effect in the United States and elsewhere, shifting the geopolitical realities and triggering the ensuing war on terrorism that still rages many years later. The number of people adversely affected by that war is not considered in assigning a scope to September 11.\n\nThe avian influenza is still in its infancy and fortunately has not yet materialized into a pandemic, hence the relatively low scope. The energy crises and its intimately related global warming problem have not yet resulted in widespread deaths or injuries, but both events are global in extent essentially affecting the entire world. Thus the descriptor gargantuan assigned to both is based on the size of the adversely affected geographical area.\n\nOf course, the number of residents of Egypt was far less than 80 million when the disaster commenced in 1952.\n\nBull. Pol. Ac.: Tech. 57(1) 2009\n\nActually delayed by a few years due to World War I and relocation to France. Richardson chose that particular time and date because upper air and other measurements were available to him some years before. Bull. Pol. Ac.: Tech. 57(1) 2009\n\nNewtonian implies a linear relation between the stress tensor and the symmetric part of the deformation tensor (rate of strain tensor). The isotropy assumption reduces the 81 constants of proportionality in that linear relation to two constants. Fourier fluid is that for which the conduction part of the heat flux vector is linearly related to the temperature gradient, and again isotropy implies that the constant of proportionality in this relation is a single scalar.\n\nAn assumption that obviously needs to be relaxed for most atmospheric flows, where radiation from the Sun during the day and to outer space during the night plays a crucial rule in weather dynamics. Estimating radiation in the presence of significant cloud cover is one of the major challenges in atmospheric science.Bull. Pol. Ac.: Tech.\n\n57(1) 2009\n\nThe second-order tensor u i u k is obviously a symmetric one with only six independent components. Bull. Pol. Ac.: Tech. 57(1) 2009\n\nThe number of first-order ordinary differential equations, each of the form dx i /dt = f i (x 1 , x 2 , . . . , x N ), which completely describe the autonomous system's evolution, is in general equal to the number of degrees of freedom N . The latter number is in principle infinite for a dynamical system whose state is described by partial differential equation(s). For example, a planar pendulum has two degrees of freedom, a double planar pendulum has three, a single pendulum that is free to oscillate in three dimensions has four, and a turbulent flow has infinite degrees of freedom. The single pendulum is incapable of producing chaotic motion in a plane, the double pendulum does if its oscillations have sufficiently large (nonlinear) amplitude, the single, nonplanar, nonlinear pendulum is also capable of producing chaos, and turbulence is spatiotemporal chaos whose infinite degrees of freedom can be reduced to a finite but large number under certain circumstances.\n\nMeaning analytical solutions of the differential equations governing the dynamics are not obtainable, and numerical integrations of the same lead to chaotic solutions.\n\nThe Imperial Japanese Navy's surprise attack on Pearl Harbor, Oahu, Hawaii, on the morning of 7 December 1941 was aimed at the Pacific Fleet and killed 2,403 American servicemen and 68 civilians."
}