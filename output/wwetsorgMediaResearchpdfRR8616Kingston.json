{
    "title": "Financial support from the Graduate Record Examinations Board and Educational Testing Service is gratefully acknowledged. Our thanks to our many colleagues who made this research possible. The original plan for this research was designed by E. Elizabeth Stewart with the assistance of Madeline Wallmark and several other consultants. Many of the analyses were supervised or performed by",
    "publication_date": "N/A",
    "authors": [
        {
            "full_name": "Madeline Wallmark",
            "firstname": "Madeline",
            "lastname": "Wallmark",
            "affiliations": []
        },
        {
            "full_name": "Dorothy Thayer",
            "firstname": "Dorothy",
            "lastname": "Thayer",
            "affiliations": []
        },
        {
            "full_name": "Craig Mills",
            "firstname": "Craig",
            "lastname": "Mills",
            "affiliations": []
        }
    ],
    "abstract": "We are especially grateful for the organizational and programming assistance of Louann Benton. We also thank Frederic Lord, Martha Stocking, and Marilyn Wingersky with whom we consulted many times and several colleagues who reviewed an earlier draft of this paper. We wish to thank especially E. Elizabeth Stewart for her insightful comments. Nonetheless, the opinions expressed herein are solely those of the authors. methods, Tucker 2 True and Levine, appear to have worked as well in practice for the analytical measure as the random group method does in theory. A possible explanation for the generally poor results for the random group, preoperational section data collection design based equatings was the constant use of the last section of the test to collect equating data. It may be, now that the sections of the GRE General Test are administered in various orders in different editions of the test, that the extreme bias found in this study for the verbal and analytical random group preoperational section equatings will disappear or at least be substantially diminished.",
    "full_text": "The original purpose of this study was to address the test-disclosure-related need to introduce more Graduate Record Examinations (GRE) General Test editions each year than formerly, in a context of stable, or possibly declining examinee volume. The legislative conditions that created this initial concern regarding test equating have abated. However, several of the test equating models considered in this research might provide other advantages to the GRE Program. These potential advantages are listed in the body of the report. Equating can be considered to consist of three parts: (1) a data collection design, (2) an operational definition of the equating transformation, and (3) the specific statistical estimation techniques used to obtain the equating transformation. Currently, the GRE General Test collects data using an equivalent groups design. Typically, a linear equating method is used, and the specific estimation technique is setting means and standard deviations equal. For this research, two other data collection designs were studied: nonrandom group, external anchor test, and random group, preoperational section. Both item response theory (IRT) and linear equating definitions were used. IRT true score equating was based on item statistics for the three-parameter logistic model as estimated using LOGIST. Linear models included section pre-equating using the EM algorithm, Tucker's observed score model, and several true score models developed by Tucker and Levine. For each of the three GRE measures, verbal, quantitative and analytical, all equating methods were assessed for bias and root mean squared error by equating a test edition to itself through a chain with six equating links. Bias and root mean squared error were extremely large for equating the verbal and analytical measures using section pre-equating or IRT equating with data based on the random group preoperational section data collection design. For the quantitative measure, this data collection design produced a small amount of bias, but moderate amount of root mean squared error. Using the nonrandom group, external anchor test data collection design, quantitative equatings had moderate amounts of both bias and root mean sq=lared error. Verbal nonrandom group, external anchor test equatings showed relatively small amounts of bias and root mean squared error, with the Tucker observed score model performing particularly well. Bias was small for the analytical anchor test equatings, and root mean squared error ranged from small to moderate. All nonrandom group, external anchor test methods worked about as well in practice for the verbal measure as the currently used random group method does in theory. The current random group method, however, has never been subjected to an empirical check comparable to that used in this study for the experimental equating methods. Two anchor test INTRODUCTION Purpose of this study The original purpose of this study was to address the testdisclosure-related need to introduce more Graduate Record Examinations (GRE) General Test editions each year than formerly, in a context of stable, or possibly declining examinee volume. Since then, the legislative conditions that created the initial concern regarding test equating have abated. However, several of the test equating models considered in this research might provide other advantages to the GRE Program, such as: improved test security, greater accuracy of equating, shorter timeschedule requirements for score reporting, additional test analysis information, and possible improvement of the test development process. Equating Test developers usually try to make the various editions of a test interchangeable with regard to content coverage, item format, and difficulty so that examinees are neither advantaged nor disadvantaged by the edition of the test they happen to take. Unfortunately, because of the large number of constraints under which test developers operate and because of the quality of the statistical information that is available at the time editions of a test are constructed, inevitably, some test editions are easier than others. To make sure that groups of examinees taking different editions of a test are treated equitably, statistical techniques known as test equating are used to adjust scores on each edition of the test so that they are comparable to scores earned on other editions of the test. There are several different equating models used by psychometricians. These models make different assumptions about the data they use and vary in their appropriateness for any given examination, such as the verbal, quantitative, and analytical measures of the GRE General Test. There are three major aspects of any equating model: (1) the data collection design, (2) the operational definition of the equating transformation, and (3) the specific statistical estimation techniques used to obtain the equating transformation. Data collection designs. Two data collection des specifically for this study. For the first design, ref EonRandom group External Anchor Test (NREAT), two editi administered, -one to each of twononrandom groups consi who chose to take the test on one or the other of two t dates, and a common short test that did not count towar scores was administered to both groups. This design is referred to as Design IV from Angoff (1984). igns were used erred to as ons of a test were sting of examinees est administration *d the examinees' sometimes For the second design, referred to as Random Group, Pre-Operational Section (RPOS), essentially, one test was administered to a group of examinees. That group was further divided into two equivalent subgroups through the spiraling of test booklets. That is, different versions of that test edition were packaged in an alternating fashion (e.g., 1,2; 1,2; 1,2; . ..). Research has shown that spiraling results in essentially equivalent groups, sometimes even more effectively (because of a stratification effect) than does true random assignment. In addition, one subgroup received one-half of a second edition of the test, and the other subgroup received the other half of the second edition. The two half-tests were designed to be as similar as possible in content and difficulty. For further information on this design, see Holland and Thayer (1981, 1985), Holland and Wightman (1982), or Petersen, Hoover, and Kolen (in press). The data collection design used currently for operational GRE General Test equatings is referred to as Eandom groups (RG). Two editions of the test are given, one to each of two random, or otherwise equivalent, groups. The GRE Program regularly uses spiraling to ensure equivalence of the two (or more) groups. This design was used in this research to provide some comparison equatings. More detail regarding the data collection designs used in this study is given in the Procedures section of this report. Equating transformations. Three operational definitions of the equating transformation are commonly used: (1) linear equating, which provides a transformation such that scores from two tests will be considered equated if they correspond to the same number of standard deviations from the mean in some population of examinees, (2) equipercentile equating, which provides a transformation such that scores from two tests will be considered equated if they correspond to the same percentile rank in a specified population of examinees, and (3) item response theory (IRT) equating, which provides a transformation such that scores from two tests will be considered equated if they correspond to the same level of the latent trait underlying the two tests. Only linear and IRT transformations were used in this study. Statistical estimation techniques. A number of different techniques have been developed to estimate the intercept and slope parameters for a linear equating. Each technique attempts to estimate the first two moments of the score distributions for an old edition (one whose scores are already on scale) and a new edition of the test on some common group of examinees. These estimation techniques differ in the assumptions that they require. A primary difference is that some linear methods estimate the means and standard deviations of observed scores and others estimate the true score moments. Estimating true score moments is considered particularly appropriate when the two editions of the test to be equated have been administered to groups with very different ability distributions (Angoff, 1984, p. 113). In this study, various statistical estimation techniques, which will be described later in this report, were used with the NREAT data collection design, and one, EM algorithm, to estimate the first two observed score moments (Holland and Wightman, 1982) was used with the RPOS data collection design. This latter method is commonly referred to as section pre-equating, or SPE. This study investigated only one IRT equating method, IRT true score equating (Lord, 1980, pp. 199-200). There are three aspects of statistical estimation for IRT true score equating: (1) estimating item and person parameters, (2) putting the parameter estimates from separate calibration runs on a single scale, and (3) setting equal the true scores that correspond to the same level of the latent ability, theta. The GRE General Test' The GRE General Test measures and yields separate scores for the general verbal, quantitative, and analytical abilities students should have acquired to be successful at the graduate level of education. Scores for each measure are based on the number of correct answers and are scaled to fall between 200 and 800. The test consists of seven 30-minute sections of multiple-choice questions. At the time data were collected for this research, sections 1 and 2 constituted the verbal measure; sections 3 and 4, the quantitative measure; and sections 5 and 6, the analytical measure. The remaining section does not count toward any of the reported scores, and usually consists of verbal, quantitative, or analytical pretest items. For this research, the remaining section was used to collect data for the equating experiments. The specially constructed versions of this section are described in the Research Design section of this report. Since the data for this research were collected, the ordering of sections of the General Test has changed. For current editions of the GRE General Test, the seven sections may be arranged in various orders. The verbal measure employs four types of questions: antonyms, analogies, sentence completions, and reading comprehension sets. The quantitative measure employs three type of questions: discrete quantitative questions, data interpretation questions, and quantitative comparison questions. The quantitative questions measure basic mathematical skills, understanding of elementary mathematical concepts, and the ability to reason quantitatively and solve problems in a quantitative setting. These questions require arithmetic, algebra, and geometry at a level not beyond that taught in a first high school level course. The analytical measure employs two types of questions: analytical reasoning and logical reasoning. Analytical reasoning questions test the ability to understand a given structure of arbitrary relationships among fictitious persons, places, things, or events; to deduce new information from the relationships given; and to assess the conditions used to establish the structure of relationships. Logical reasoning questions test the ability to understand, analyze, and evaluate arguments: recognizing the assumptions on which an argument is based, drawing conclusions from given premises, inferring material missing from given passages, applying to one argument principles governing another, identifying methods of argument, evaluating arguments and counter-arguments, and analyzing evidence. 1 This section of the paper was adapted from the Guide to the Use of the Graduate Record Examinations Program 1985-86 (ETS, 1985a). RESEARCH DESIGN In this section the database (test editions and examinee samples) is described, and the various procedures used in this research are detailed. The equating models used and the assumptions upon which they are based are explained. The rationale for the criterion used to judge the adequacy of the equating models is developed. Database Six editions of the GRE General Test were administered on seven different occasions; the edition given at the first and last administration was the same. Test editions. For ease of reading, the six editions of the GRE administered as part of this research will be referred to in this report as El, E2, E3, E4, E5, and E6. (The ETS designations for these test editions are 3DGR3, 3DGR1, 3DGR2, 3EGR1, 3EGR4, and 3EGR2, respectively, for the verbal and quantitative measures. For the analytical measure, edition 3EGR3 was used instead of 3EGR4.) One of several different experimental sections was administered along with each edition. These experimental sections were used as either anchor tests for the NREAT equating data collection design or as pre-operational sections for the RPOS data collection design. The use of these data for equating will be further explained later in this section of the report. Table 1 describes the characteristics of the verbal, quantitative, and analytical measures for each test edition. It shows when each edition was administered as part of this research. For each measure it gives the number of items contributing to the reported score for that edition and the mean and standard deviation of the difficulty of the items in that measure. In addition, the number of items in each experimental section and their difficulty are presented. Appendix A presents (among other information) the ETS form and subform designations and codes for each test. _-----------weem-w---v---\n\nTable 1 About Here ___- __------________-----Examinee samples. Samples consisted of all examinees who took the appropriate test editions at one of the seven administrations, with the exception of all of the following: l examinees who had taken the GRE General Test more than once and who received any of the same test sections at two or more administrations, 1 Item difficulty is presented in terms of equated deltas, that is deltas put onto a common scale for a given measure for all test editions. See Henrysson (1971, pp. 139-140) for a description of the delta statistic. examinees who did not respond to at least five i of the seven secti .ons of the test, .tems in each examinees without an item res ponse record (i.e l 9 any whose answer sheets were not machine scored), and examinee s who administ rative t ook the test at a center for which an irregularity was reported . examinees Tables 2, 3, and 4 present information regarding the examinees tested as part of this research -the sample sizes and means and standard deviations of their scaled verbal, quantitative, or analytical scores for the subgroups tested at each administration. The statistics are based on only those examinees used in each equating. Appendix A presents (among other things) the number of examinees in the sample for each external anchor test and each preoperational section. Anchor test samples ranged from 3,583 to 4,408 examinees. Preoperational section samples ranged from 1,745 to 2,561. The approximately two-to-one ratio of sample sizes for anchor test and preoperational section samples was planned, since two preoperational section samples, but only one anchor test sample, are needed for each score being equated at a given test administration.\n\nNote, however, that all data to be used for equating based on preoperational sections can be collected in one administration, but data for equating using external anchor tests must be gathered from two test administrations.\n\n- ---------_------_---~~~~~~~--------\n\nTables 2, 3, and 4 About Here ---------------------~~~~~~--~------Figures 1 and 2 present the NREAT and RPOS data collection designs, respectively. For the NREAT design, at each administration (other than the first and last) six forms of the edition administered were spiraled, two for each of the General Test measures. In each pair, the operational test edition was the same, but one form contained in the seventh section an anchor test (containing items of the same types as the measure being equated) in common with the previously administered test edition, and the other contained an anchor test in common with a test edition scheduled for a future date. For the RPOS data collection design, at each administration six forms of the test were spiraled, two for each of the three General Test measures. In each pair the operational test edition was the same, but one form contained in the seventh section one of the two operational sections of either the verbal, quantitative, or analytical measure from a previously administered edition, and the other form contained the other operational section. Note, this is the opposite of what is normally done for an RPOS data collection design. Usually, these sections would contain halves of future editions instead of previous editions. Figures 1 and 2 About Here Procedures Equating methods. Results from seven different equating methods are presented in this report -one RG method: setting means and standard deviations equal; four NREAT methods: Tucker, Tucker True 2, Levine (equally reliable, or unequally reliable, as appropriate), and IRT Anchor Test True Score with theta metric set using concurrent calibration; and two RPOS methods: IRT Preoperational True Score with theta metric set using concurrent calibration, and EM algorithm to estimate means and standard deviations. Three-parameter logistic IRT estimates were performed with the program LOGIST (Wingersky, Barton, & Lord, 1982). A brief overview of these methods is given in Table 5, and a detailed description of the linear methods is presented in Appendix B (Appendix B is adpated from appendix A of Marco, Petersen, & Stewart, 1983). Detailed information on IRT true score equating is available in Lord (1980, chapter 13) and Hambleton and Swaminathan (1985, chapter 10). Information on the use of the EM algorithm for equating with RPOS data collection designs is available in Holland and Wightman (1982).\n\n------__--___--_____-----\n\nTable 5 About Here ------___---________-----Some additional linear equating methods were used in early stages of this study as was a second method of establishing a common IRT metric. Appendix C presents some notes on these methods.\n\nAssessing the adequacy of equating methods. A good equating method should have certain characteristics. As with many other statistical estimation techniques, these desirable characteristics include minimal bias and mean squared error. Assessing the bias and mean squared error of one or more equating methods, however, first requires one to know the true equating relationship between test editions. In most real-life equating situations, this is not possible. For the purpose of this research, such a criterion was constructed. For each equating method in this study, a chain of six equatings was performed. E2 was equated to El, E3 was equated to E2, E4 to E3, ES to E4, E6 to ES, and then finally El was administered again and equated to E6. If the function that equates E2 to El is called f(x), and the function that equates E3 to E2 is g(x), then the composite function g(f(x)) will put scores from E3 on the El scale. Likewise, if h(x) equates E4 to E3, i(x) ES to E4, etc., then k(j(i(h(g(f(x)))))) equates El to El through the chain (or circle) of six equatings. Since the equating of El to El should be an identity function, it can be determined how close each equating method came to the true equating relationship. The equating criterion was used in several ways. First, equatings were compared graphically. These graphs were summarized statistically. In doing so, it was decided that inaccuracies in equating were inconsequential if few or no examinees were affected by them, and SO discrepancies were weighted at each raw score by the number of examinees (whose data for edition El was used in this research) who obtained that score. Two summary statistics were calculated: bias (equation l), the weighted mean difference between an experimental equating and the criterion equating, and root mean squared error (see equation 2), equivalent to the weighted mean standard error of equating. Note that root mean squared error includes bias and thus can never be smaller than bias. bias = = (ii,$ -ii,, RMSE = 2i i where = ( ~ difi> i=l / ~ fi i=l n is the maximum obtained score for the measure, (1) (2) X2i is the score (equated through the six-link chain) corresponding to raw score i for the April 1983 administration of form El, fi is the frequency of raw scores i in the October 1981 group, Xii is score corresponding to raw score i for the October 1981 administration of form El. In addition to the calculation of bias and root mean squared error on the raw score metric, to facilitate comparisons of equating methods across the three General Test measures, bias and root mean squared error (RMSE) were standardized by dividing each by the scaled score standard deviation for the appropriate test score. Also, to provide a context familiar to most score users, bias and root mean squared error were transformed to the appropriate GRE scaled score metric (verbal, quantitative, or analytical) by multiplying them by the slope of the scaling function used to place raw scores for edition El on the GRE score scale. For the verbal measure this transformation is nonlinear, so a linear approximation was used. This approximation differed from the actual scaling primarily at the high end of the scale where there are few data. Figures 3, 4, and 5 graphically present the equating lines (raw score new to raw score old) for the six methods used in this study, for the verbal, quantitative, and analytical measures, respectively. Figures 6, 7, and 8 present the raw score differences between each equating line and the true equating function.\n\nSo, for example, for the verbal measure, the SPE method would convert a raw score of 72 to a 66 (even though the equating should have been an identity function yielding an equated score of 72).\n\nSixty-six minus 72 is negative six, and this can be seen in Figure 6.\n\nEssentially, these difference graphs simply magnify the discrepancies between equating methods. Note that the scale for Figure 6 is different from that for Figures 7 and 8. ----------------------~ Table 6 presents the bias and root mean squared error for the six equating methods for each General Test measure.\n\nTable 7 presents bias and root mean squared error in the GRE General Test scaled score metric, so that equating error can be viewed in a context familiar to GRE score users.\n\nTable 8 presents the standardized bias and root mean squared error for the six equating methods. --_-------------e---w-\n\nTables 6, 7 and 8 About Here ------------___--___~-~~~~~~~~~~~~~ Several findings stand out in these tables. l For the verbal and analytical measures, the equating methods that used a RPOS data collection design (IRT and SPE) had relatively large bias and root mean squared error compared to those that used a nonrandom group external anchor test design. l For the quantitative measure, however, the absolute standardi_zed bias was least for the SPE and IRT RPOS data collection designs. l The standardized root mean squared errors for the quantitative equatings were not too different (ranging from .07 to .ll) regardless of the data collection design or equating transformation method. l When making comparisons within each of the three measures that were equated, all models using NREAT equating performed about equally well for the verbal and quantitative measures. For the analytical measure IRT performed somewhat less well than Tucker True 2 and Levine (.lO for IRT, compared to .04 for the other two methods). l Overall, NREAT equating models did less well in terms of both bias and root mean squared error for the quantitative equatings than for the verbal or analytical equatings. 0 For the verbal equatings, for each equating method, the root mean squared error was substantially accounted for by a consistent negative bias: that is, items in the preoperational section were systematically more difficult than when they appeared in an earlier operational section. This bias was small, however, for all of the NREAT equating methods. 0 For the quantitative equatings based on NREAT methods, the root mean squared error was primarily accounted for by a consistent positive bias. For the RPOS data collection design, however, the bias was small and inconsistent, and did not account for substantial amounts of the root mean squared error. 0 Overall, the analytical equatings based on NREAT methods, bias was small and accounted for very little of the root mean squared error. For the RPOS methods, a consistent large negative bias accounted for most of the root mean squared error. DISCUSSION Comparison of empirical root mean squared error with the standard error of a chain of operational equatings In order to answer the question, \"did any of the NREAT or RPOS equatings work well enough,\" some context is needed. One such context is the standard error of a chain of equatings based on the method currently used to equate the GRE General Test: random groups, setting means and standard deviations equal (Angoff, 1984, design IA-l, pp. 94-97). The standard error of equating is affected by the size of the sample on which the equating experiment is performed. For operational GRE General Test equatings, the samples for each edition of the test range from 10,000 to 20,000 and thus the total sample size ranges from 20,000 to 40,000. Verbal equatings. For the GRE verbal measure, NREAT equating methods worked quite well, and RPOS methods did not. The scaled score root mean squared error of the chain of equatings for the four NREAT equating methods was about 5 scaled score points. This figure can be compared to the standard error of a chain of six equatings for the operational equating method --random groups, setting means and standard deviations equal. The operational standard error was estimated using Lord's formula for the standard error of a single RG means and standard deviations equating (Lord, 1950; Angoff, 1984, p. 97) and Theorem 6, by Braun and Holland (1982), for the standard error of a chain of equatings. Assuming that the slopes of the raw score to raw score equating functions are close to one, the standard deviation of the equated scores for each new form group was 123 (the average for the equating groups used in this research), the total number of examinees upon which each operational equating is based is 30,000, and test scores are normally distributed, then the median standard error of equating for the chain of six equatings is about 4 scaled score points. (The median standard error is the standard error at .675 standard deviations from the mean. Under the previously mentioned assumptions, this median is comparable to the empirical weighted root mean squared error.) If the total number of examinees upon which each equating was based were 20,000 or 40,000, the median standard error of the chain of equatings would be about 5 or 3 scaled score points, respectively. Of course, the figures for the median standard error of equating depend on the previously listed assumptions as well as on the assumptions of RG linear equating. In practice these assumptions may be violated. This would probably increase the empirical RG linear standard errors of equating to a value at least somewhat larger than the theoretically derived numbers presented here. Thus, for the purpose of evaluating the root mean squared error, the median standard errors should be considered conservatively low. For the two RPOS equating methods, the average verbal equating root mean squared error was 26 scaled score points, considerably worse than the estimated standard error of the chain of operational equatings. Quantitative equatings. For the quantitative measure, all equating methods had about the same root mean squared error, on the average about 13 scaled score points. Assuming a scaled score standard deviation of 133, a normal distribution, and sample sizes of 20,000, 30,000, and 40,000, the median standard errors of equating for the chain of six quantitative equatings using the operational method are about 5, 4, and 4 scaled score points, respectively. Thus, the NREAT equatings do not appear to have worked as well for the quantitative measure as they did for the verbal measure. And, although for the quantitative measure the RPOS equating methods worked as well as did the NREAT equatings with regard to root mean squared error and worked better with regard to bias, they did not perform as well as one would expect a random groups linear equating to perform. Analytical equatings. For the analytical measure the IRT NREAT equating had the largest root mean squared error, 13 scaled score points. The linear NREAT equating methods had root mean squared errors ranging from 5 to 9 scaled score points. The two RPOS equatings had an average root mean squared error of 26 points. With a scaled score standard deviation of 126 and sample sizes of 20,000, 30,000, and 40,000, the median standard errors of the chain of analytical RG linear equatings would be 5, 4, and 4 scaled score points, respectively. Therefore, the linear NREAT methods did reasonably well, but the IRT NREAT equatings, and to a much larger extent the RPOS equatings, did poorly. Factors That May Have Affected These Results Only for the verbal measure NREAT equatings did the root mean squared error appear reasonable in light of the theoretical standard error of the current GRE equating procedure. This might be due to any of at least three factors. First, the samples used in this study are considerabl GRE scores. y smaller than the samples used in the operational equating of This might be compensated for, however, by the increased power of NREAT and RPOS equating designs. Second, sampling error might have produced large root mean squared errors in the groups used in this study even though in another set of equatings the root mean squared errors might be smaller. Third, the root mean squared errors of the experimental equatings are based on real data and not on statistical assumptions. The effect of real data is likely to be an increase in the size of the root mean squared error of the operational procedure beyond the theoretical standard error of equating. If the empirical root mean squared error of a chain of random group, means and standard deviations equatings were calculated, it might also be somewhat larger than the theoretical standard error of equating. This might occur because of violations of the assumptions of the equating model. In particular, to some extent examinees are advantaged if they have previously taken the same edition of a test (Kingston & Turner, 1984). This can occur for the old edition, but not for the new edition, in an RG equating. 1 The choice of sample sizes in this study was intentional and reflects administrative constraints such as current administration volumes and pretesting needs. It should be noted that if NREAT or RPOS data collection designs were used operationally for the GRE General Test, it is likely that double-part score equating would be used. For NREAT data collection, this would entail using two anchor tests in order to equate to two different old editions of the test and then averaging the two equatings. Likewise, for an RPOS data collection design, a new edition of the test would be preoperationally equated to two different old editions, and the average of the two equatings would be used. Although the statistical properties of double-part score equating are not well understood, such an equating would be expected to have reduced root mean squared error and would be expected to reduce certain sources of bias (although not the sources of bias that appear to have affected the RPOS equatings in this study). Effect of smaller sample sizes. If the standard errors were calculated for the equatings performed in this study, then the effect o the smaller samples used in this research could be addressed directly. Unfortunately, no method has yet been devised to assess the standard error for IRT or section pre-equating. Several methods have been proposed for estimating the standard error of NREAT linear equatings (Lord, 1975; Kolen, 1985). Table 9 presents for the verbal, quantitative, and analytical measures, the standard error of the chain Tucker equatings based on the delta method developed by Lord. The medi standard errors (assuming normality of the score distributions) are 4, and 5 scaled score points, respectively, for the verbal, quantitative, and analytical measures. Thus, at least for the linear NREAT equatings the smaller sample sizes appear to have been compensated for by the reduction of sampling error from the use of anchor test data. That is, the median standard errors of the linear NREAT equatings based on sampl of about 4,000 are about the same as the median standard errors of the random group, means and standard deviations equatings based on samples 20,000 to 30,000.\n\n-_________-____----_-----\n\nTable 9 about Here ------------_-----------f of an 4, 3 es of The RPOS equatings were based on samples approximately one-half the size of the NREAT samples. Could this explain the particularly poor performance, at least in the case of the verbal and quantitative measures, of the RPOS equating methods? Since no standard errors could be estimated for the RPOS equatings, this was assessed by dividing each of the NREAT samples in half, performing Tucker and Levine equatings on each half-sample, and estimating the bias and root mean squared error for each chain of half-sample equatings. Although three of the four half-sample equating chains had greater bias and root mean squared error than did their respective full-sample chains (unexpectedly, one of the Levine half-sample equatings had smaller bias and root mean squared error), all four had considerably smaller bias and root mean squared error than the RPOS equating chains. In summary, it appears that the performance of the NREAT and RPOS equating methods relative to the standard error of the operational RG equatings cannot be explained by the sample sizes used in this study. Effect of sampling error. Six different sets of data were used for the equatings in this study. That is, different examinees made up the samples for the NREAT and RPOS groups for each of the three GRE General Test measures. Thus, the results of the 18 equating chains presented in Tables 6 through 8 are based on only six independent sets of data. Still, of those 18 chains, only one (Tucker for the verbal measure) has a root mean squared error smaller than the theoretical standard error of a comparable chain of RG equatings based on total samples of 30,000 examinees. It appears highly unlikely that chance in the selection of samples explains these results. Effect of real data. All statistical models, including equating models, are based on assumptions that are not strictly met by the data. Thus, standard errors of equating, which are based on these unmet assumptions, are usually unrealistically small compared to corresponding root mean squared errors that are empirically derived. The magnitude of the discrepancy between the standard error of equating and the empirically derived root mean squared error will depend on the magnitude of the discrepancy between the assumptions and the data. The assumptions of the various equating models used in this research are given in Appendix B. Some of these assumptions are untestable, given the available data: for example, the assumption that the regression of total test on equating test for test X in population Q (the population that took test Y) is the same as the regression of test X on the equating test for population P. For other assumptions, such as the local independence assumption of IRT, good methods of testing the assumptions did not exist at the time this research was carried out. Previous research has demonstrated the reasonableness of the three-parameter logistic model for the GRE verbal and quantitative measures (Kingston and Dorans, 1982a). Analysis of item-ability regressions (for an example of such an analysis see Kingston and Dorans, 1985) and a slightly modified Yen's Q statistic (see Yen, 1981, 1984) indicated that the three-parameter logis ic model k is probably reasonable for the current GRE General Test. One assumption buried in RPOS equating is that examinee responses to items will be the same when the items appear in the preoperational sections and when they appear in the operational test. Examinees' responding behavior might vary, for example, if they knew that the preoperational items did not count toward their score and therefore they decided not to waste too much time and energy on those items. More generally, behavior might vary if there were any kind of context or location effect, perhaps caused by fatigue or practice in one setting, but not in the other (Kingston and Dorans, 1984; Whitely and Dawis, 1976; Yen, 1980). In this research , preoperational data were always gathered in the seventh (last) section of the test. These \"preoperational\" data were for test material from a previously administered edition (that is, the old form in the equating relationship) in which the items always appeared prior to the last section. The considerable negative bias found for the verbal and analytical RPOS equatings indicates that for a given raw score, scaled scores for El when administered operationally in April 1983 were lower than for El when administered \"preoperationally\" in December 1981. For two editions of a test, the scaled score for a given raw score will be higher for the more difficult edition. Therefore, the verbal and analytical items appeared more difficult when they were administered in section 7 than when they were administered operationally in sections 1 and 2 (verbal) or 5 and 6 (analytical). Note, however, that this effect, which appears clear from the equatings, is not clear from the mean deltas presented in Table 1. Some coaching schools have advised examinees to determine which section or sections of a standardized test do not count toward their score and save their energy by not working too hard on those sections (Owen, 1985 pp. 135-136). If a large number of examinees followed this advice, it might explain the results for the verbal and analytical measures. This potential explanation is weakened considerably, however, because'it is unclear why this would occur for those measures but not for the quantitative measure. Alternatively, it might be that verbal items (particularly reading comprehension) and analytical items are susceptible to a fatigue effect. If an examinee's attention span diminishes at the end of a long test, this might affect, in particular, items that refer to relatively long passages or that require the juxtaposition of diverse elements of a question. Such an effect would be more likely to influence responses to reading comprehension, analytical reasoning, and logical reasoning items than other item types. Previous research on the effect of item location on IRT parameter estimates and equating results has been performed on the pre-October 1981 version of the GRE General Test (Kingston and Dorans, 1982b, 1984). That research was based on an RPOS data collection design very similar to the one used in the current study, but it differed in that the test was administered under formula-scoring instructions, the verbal measure was slightly longer and more speeded, and the analytical measure had two additional item types and only a few logical reasoning items. Kingston and Dorans found that location effects were item-type specific. Analysis of explanations and logical diagrams items, two item types no longer used in the analytical measure, showed very large practice effects; that is, they were considerably easier when answered after another section of such items. Reading comprehension items were more difficult for the examinees when they appeared in the preoperational section at the end of the test (the mean difference between b-estimates was .14). Although for two different test editions the magnitude of the effect was consistent, it was statistically significant at the .05 level in only one. Analogy, antonym, and quantitative comparison items all appeared easier in the RPOS.position in both editions of the test, but the differences were statistically significant at the .05 level only for antonyms and only in one of the two test editions. Because too few logical reasoning items were administered, no results for that item type are available. Results for the other item types were inconsistent. Given the change in scoring , directions and the inconsistency of these results, the Kingston and Dorans study does not appear to shed too much light on the RPOS equating results. While Kingston and Dorans did not find consistent statistically significant results for the item types that constitute the verbal and analytical measures of the current General Test, item location effects caused by fatigue or practice cannot be ruled out as an explanation for the RPOS equating results. Even if location effects were too small to be found statistically significant, given the power of the statistical test that would be used, the effects might be consistent, and the sum of such effects over six equatings might be large enough to explain the bias in verbal and analytical RPOS equatings. Comparison of individual SPE and Tucker equatings. Since for this research the preoperational and operational administration of test items involved different populations, it is not possible to assess directly the magnitude and consistency of any item location effects. One way to assess this indirectly would be to compare each of the six SPE and Tucker equatings in the chain for each measure. This is reasonable for the verbal and analytical measures, since for them the results for the Tucker model were quite good.\n\nThese results are presented in Table 10. Since for the quantitative measure the Tucker results were not satisfactory, such a comparison will not be presented.\n\nTable 10 about Here\n\nTable 10 shows that for four out of six of the verbal equatings, SPE showed a large negative bias compared to Tucker (items appeared more difficult in their preoperational administration than in their operational administration). For the other two equatings, the bias was positive but smaller in magnitude. SPE shows a negative bias in five of the six analytical equatings. For the E4 to E3 and E6 to E5 equatings the bias is particularly large, 12 and 16 scaled score points, respectively. The one instance of positive bias was also large, about 10 scaled score points. Comparison of SPE and linear random group equatings. For three of the equating links, E6 to E5, E3 to E2, E2 to El, there exists for comparison the operational RG linear equating. Such an equating is performed on equivalent samples from a single population and is based on fewer asssumptions than any of the other equating methods presented in this report. Table 11 presents the bias and root mean squared error for the verbal, quantitative, and analytical SPE and Tucker equatings, using the linear random group method as a criterion. It should be noted that this criterion is a relative one. That is, the linear random group equating suffers from some amount of sampling error, it may be more population dependent than some other equating methods, and it assumes linearity of equating relationships (as does SPE).\n\n-----________-------------\n\nTable 11 about Here --------------------------From Table 11 it appears that for the verbal measure, bias was introduced in the E3 to E2 section pre-equating and not in either of the other two equatings. For the quantitative measure, it seems that a moderate negative bias is introduced in two of the three equatings, but a large positive bias is introduced in the E6 to E5 equating. In fact, for all other linear NREAT and RPOS quantitative equatings (data not presented here), the E6 to E5 equatings had large positive bias (between 8 and 11 scaled score points) and root mean squared error (between 9 and 11 points) compared to the linear RG equating. Data for the E6 to E5 equatings were studied closely to try to ascertain why they appeared to produce so much bias. The two test editions were matched well on difficulty and had essentially equal reliability. Test analysis data from the original administrations of the two editions confirmed their statistical parallelism (Wallmark, 1982, 1984). The samples for the NREAT equatings were well matched with regard to anchor test score distributions. Although no cause for the poor results for the NREAT equatings is apparent, Table 3 provides evidence as to the possible culprit for the RPOS results. The mean quantitative scores on edition E6 administered in February 1983 were different for the two groups who took the \"preoperational\" half versions of E5. The means of 526 and 534 differ by about 2.6 standard errors of the mean: a difference that should occur less than one time out of one hundred by chance. The standard deviations for the two groups also differed: 132 versus 129. Although these differences may well be due to chance, this may have affected the quality of the equatings. For the analytical measure, a very large amount of negative bias was introduced in the E6 to E5 SPE equating (about 15 scaled score points). Also, a moderate amount of bias was introduced in both the SPE and Tucker E2 to El equatings. No cause for this bias is apparent. Less bias was introduced in the E3 to E2 equating. SUMMARY AND CONCLUSIONS NREAT equating, both IRT based and traditional, worked well for the GRE verbal measure. The average estimated bias for the four equating methods was about 4 scaled score points and the root mean squared error was about 5 points. This appears to be small, given that the verbal measure scaled score standard error of measurement is about 34 points (ETS, 1985a), and that score users are advised not to make distinctions between individuals based on small differences. Also, the NREAT verbal measure root mean squared error is about the same size as the theoretical value for the operational RG General Test equating method. NREAT double-part score equating for the verbal measure might well be psychometrically somewhat superior to the current GRE equating method. For the quantitative and analytical measures, the NREAT equatings worked somewhat less well. There was essentially no bias over the chain of six equatings for the analytical measure, but the root mean squared error, depending on the equating method, varied between 5 and 13 scaled score points (average root mean squared error was 8 points). Since there is no discernible bias, groups of examinees would not be disadvantaged because of the test edition they happened to take. The random error, however, would be expected to inflate by about 20 percent the alternate form standard error of measurement beyond the internal consistency based estimate. This is somewhat larger than would be expected to occur with the current operational equating method. NREAT double-part score equating is unlikely to be psychometrically superior to the current RG-based equating method for the analytical measure. The quantitative NREAT equatings had a positive bias of about 12 scaled score points which accounted for almost all of the root mean squared error. This bias is equal to about 30 percent of the 40-point standard error of measurement (compared to about 21 percent for the NREAT verbal bias). The bias and root mean squared error found for the quantitative NREAT equatings is considerably larger than those expected to occur with the operational RG-based equating method, and there is no reason to expect that double-part score NREAT equating would be sufficiently better to justify its use solely on its psychometric merits. RPOS equating worked poorly for the verbal and analytical sections because of a large negative bias. That is, tests equated from data gathered in the seventh (last) section of the General Test appeared more difficult than when they were operationally administered. There was no such bias, however, for the quantitative RPOS equatings. For the verbal measure the estimated bias was larger for the IRT equatings than for the SPE equatings (27 scaled score points compared to 17 scaled score points), but for the analytical measure the bias was larger for SPE than for IRT (28 points compared to 17 points). These biases are fairly large compared to the standard error of measurement: an average of 72 percent of the standard error of measurement for verbal and an average of 45 percent for analytical. It also appears likely that these biases would continue to propagate and that the verbal and analytical score scales would drift considerably over time. The quantitative RPOS equatings appeared to work reasonably well. SPE, for the quantitative measure, had a bias of 7 scaled score points, which was small compared to the NREAT biases, as was the root mean squared error of 10 points. The IRT RPOS equatings had no discernible bias, but a root mean squared error of 15 points. Summary In summary, only the verbal NREAT equatings and none of the RPOS equatings appear to work as well as the operational RG equating works in theory. But all of the NREAT equating methods presented appear to work acceptably well for the analytical measure, and the RPOS equatings appear marginally acceptable for the quantitative measure. Note, however, that the RG methods have not been subjected to an empirical check as have the NREAT and RPOS methods. It is almost certain that RG equating methods will not work quite as well in practice as they do in theory. The important question is whether they work better than the NREAT and RPOS equating methods. Now that the possible cause of bias in the RPOS equatings has been removed, that is, the constant use of section seven for experimental items, it is likely that RPOS equating methods (based on careful selection of experimental sections in order to balance position) will work better. Recommendations Two additional research studies are recommended. First, bias and root mean squared error for linear and equipercentile equating using the operational RG data collection design with a six-link equating chain should be designed and performed. The proposed study would provide a meaningful comparison for the results in the current study. Such a study is also recommended by the ETS Standards for Quality and Fairness (ETS, 1983, p. 17), which specifies that testing programs should, \"Periodically assess the results of methods used to achieve comparability of scores and evaluate the stability of the score scale.\" Item location effects appear to be a thornier problem than previous research has indicated. Additional research leading to models that account for such effects are needed. Such models would increase our knowledge of test-taking behavior and ultimately lead to fairer, more accurate test scores.\n\nTable 1 Description of Test Editions1 Test Edition Verbal E 1 E 2 E3 E4 ES E6 E 1 Admin. Date lo/81 72 12/81 76 2/82 76 4182 76 lo/82 76 2/83 76 4183 72 Quantitative E 1 lo/81 E2 12181 E3 2182 E4 4/82 E5 lo/82 E6 2/83 E 1 4183 Analytical E 1 lo/81 E2 12/81 E3 2/82 E4 4182 E5 10182 E6 2183 E 1 4183 Operational x of Mean S.D. Items Delta Delta --12.0 2.7 11.9 2.4 11.8 2.5 11.9 2.6 11.8 2.9 11.9 2.6 12.0 2.7 60 11.3 2.7 60 11.3 2.7 60 11.4 2.5 59 11.2 2.5 60 11.3 2.6 59 11.0 2.7 60 11.3 2.7 45 13.2 2.1 50 12.4 2.2 50 12.6 2.0 50 12.7 2.3 50 12.5 2.0 50 12.9 2.1 45 13.2 2.1 1st Anchor 2nd Anchor P of Mean S.D. 1 of 1st Preop. 2nd Preop. Mean S.D. # of Mean S.D. # of Mean S.D. Items Delta Delta Items ---38 11.9 2.4 38 11.9 2.4 38 12.0 2.5 38 12.1 2.7 38 11.8 2.5 38 12.2 2.3 Delta Delta Items ---12.1 2.6 11.8 2.3 36 11.9 2.5 38 12.3 2.6 38 11.8 2.6 38 11.9 2.5 38 38 Delta Delta Items. Delta Delta -----38 38 38 38 38 38 11.9 2.5 36 11.8 2.4 11.8 2.3 38 12.0 2.2 12.1 2.7 38 11.8 2.2 11.8 2.8 38 11.9 2.6 12.1 2.7 38 11.6 2.6 12.2 2.5 38 12.2 2.7 30 10.8 2.7 30 11.3 2.4 30 11.2 2.9 30 11.5 2.7 30 11.2 2.5 30 11.3 2.6 25 13.3 2.2 24 12.6 2.2 25 12.6 2.2 25 12.9 2.7 25 12.8 2.2 25 12.8 2.7 30 30 30 30 30 30 25 24 25 25 25 25 11.1 2.6 11.3 2.6 30 11.2 2.7 30 11.3 2.7 30 11.2 2.5 30 11.4 2.7 30 30 13.4 2.2 12.7 2.0 24 12.4 2.2 25 12.7 2.6 25 12.7 2.1 25 12.7 2.8 25 25 1 Anchor -Preop. -Delta -external anchor teet preoperational section Item difficulties on delta scale, equated within CRE measure Deltas for operational items are based on the item analysis prepared the first time that the test edition was administered; thus the statistics for the two administrations of El are the same. Deltas for the anchor test and preoperational sections are based on their admlnlstratlon as part of this research. 11.1 2.6 30 11.1 2.8 11.6 2.8 30 11.2 2.6 11.4 2.4 30 11.5 2.6 11.4 2.4 29 11.2 2.6 11.4 2.4 30 11.2 2.5 10.8 2.7 29 11.1 2.7 12.7 2.0 12.1 2.3 12.7 1.9 12.5 2.3 13.4 1.9 13.0 2.2 21 13.1 25 12.6 25 12.8 25 13.0 25 12.3 25 12.9 1.6 2.1 2.0 2.3 1.9 1.6\n\nTable 2 Description of Samples Used for Verbal Equatings' N N N N Test Admin. 1st ; 2nd -X 1st ; 2nd x Edition Date Anchor s Anchor s Preop. s Preop. s 4,408 El lo/81 ------------Al 500 - ---------------------126 4,096 4,180 2,062 2,076 E2 12/81 Al 473 A2 477 Ela 476 Elb 475 123 122 121 122 3,746 3,602 1,808 1,760 E3 2182 A2 484 A3 484 E2a 483 E2b 480 119 118 120 120 3,647 3,604 1,789 1,747 E4 4182 A3 463 A4 462 E3a 464 E3b 465 124 125 125 125 4,331 4,230 1,713 1,654 E5 lo/82 A4 518 A5 516 E4a 523 E4b 522 127 126 126 125 3,825 3,671 1,808 1,904 E6 2/83 A5 478 A6 478 E5a 478 E5b 482 121 120 124 122 4,209 2,083 2,073 El 4/83 A6 474 ----------E6a 475 E6b 477 125 127 126 1 Anchor -external anchor test Preop. -preoperational section Table 4 Description of Samples Used for Analytical Equatings' N N N N Test Admin. 1st -X 2nd x 1st ;; 2nd x Edition Date Anchor s Anchor s Preop. s Preop. s 4,357 El lo/81 ------------Al 523 - -------------------a-130 4,179 6,009 6,009 1,943 E2 12/81 Al 511 A2 514 Ela 514 Elb 517 125 124 124 126 3,594 3,523 1,839 1,798 E3 2182 A2 503 A3 504 E2a 501 E2b 501 126 126 125 120 3,652 3,596 1,783 1,745 E4 4182 A3 489 A4 489 E3a 489 E3b 488 128 125 125 125 4,285 4,339 2,561 2,459 E5 lo/82 A4 525 A5 521 E4a 524 E4b 525 130 129 130 133 3,698 3,829 1,813 1,797 E6 2183 A5 510 A6 513 E5a 513 E5b 511 124 125 122 127 3,945 2,011 1,940 El 4/83 A6 502 ----------E6a 504 E6b 503 128 128 133 1 Anchor -external anchor test Preop. -preoperational section Table 61 Bias and Root Mean Squared Error in the Raw Score Metric for Various Equating Models Equating Method Verbal Quantitative Analytical Bias RMSE Bias RMSE Bias RMSE Section Pre-equating -1.65 2.31 .50 .74 -1.59 1.60 IRT RPOS -2.56 2.69 .02 1.12 -.95 1.37 IRT NREAT -.52 .63 1.03 1.10 .04 .76 Tucker -.06 .ll 1.11 1.11 .30 .52 Tucker True 2 -.49 .57 .67 .70 .03 .33 Levine, as appropriate 2 -.56 .61 .84 .85 -.03 .31 1 In raw score units, see text for definition 2 Chain of Levine equatings, using parameters based on equally reliable model or unequally reliable model, based on whether or not the old and new editions of the test are the same length\n\nTable Bias and Root Mean Squared Error 1 in the Scaled Score Metric for Various Equating Models Equating Method Verbal Quantitative Analytical Bias RMSE Bias RMSE Bias RMSE\n\n_----__----______---~-~~~~--~---~~----~-~~~--~-----~-----~~~~~~~~~~~~-~~\n\nSection Pre-equating -17 24 7 10 -28 28 IRT RPOS -27 28 0 15 -17 24 IRT NREAT -5 7 14 15 1 13 Tucker -1 1 15 15 5 9 Tucker True 2 -5 6 9 10 1 6 Levine, as Appropriate -6 6 11 12 -1 5 1 See text for definitions of bias and root mean squared error. 2 Chain of Levine equatings, using parameters based on equally reliable model or unequally reliable model, based on whether or not the old and new editions of the test are the same length Table 8 Standardized Bias and Root Mean Squared Error' (x100) for Various Equating Models Equating Method Verbal Quantitative Analytical Bias RMSE Bias RMSE Bias RMSE Section Pre-equating -14 20 5 8 -22 22 IRT RPOS -22 23 0 11 -13 18 IRT NREAT -4 5 10 11 0 10 Tucker -0 1 11 11 4 7 Tucker True 2 2 -4 5 7 7 0 4 Levine, as Appropriate -5 5 8 9 0 4 1 See text for definitions of bias and root mean squared error, both of which are given in hundredths of a standard deviation to avoid decimals. 2 Chain of Levine equatings, using parameters based on equally reliable model or unequally reliable model, based on whether or not the old and new editions of the test are the same length.\n\nTable 10 Weighted Mean Scaled Score Difference (Bias) Between SPE and Tucker Equatings for the Verbal and Analytical Measures Equating E2 --> El E3 --> E2 E4 --> E3 E5 --> E4 E6 --> E5 El --> E6 Weighted Mean Scaled Score Difference Verbal Analytical +l -3 -6 -7 -7 -12 +3 +lO -4 -16 -7 -3 Table 11 Scaled Score Bias and RMSE of SPE and Tucker Equatings Using Linear Random Groups Equating as a Criterion Verbal Quantitative Analytical SPE Tucker SPE Tucker SPE Tucker Bias E2 --> El 0 -2 -3 4 5 8 E3 --> E2 -6 -1 -4 -3 -1 5 E6 --> E5 1 4 9 11 -15 2 RMSE E2 --> El 3 2 3 4 7 9 E3 --> E2 6 2 4 6 2 6 E6 --> E5 1 5 9 11 15 2 Figure 1 NREAT Data Collection Design Admin. Date Operational Test Editions and External Anchor Tests lo/81 12181 12181 2/82 2/82 4182 4182 lo/82 lo/82 2/83 2183 4183 Figure 4 Equating the GRE Quantitative Measure to Itself Through o Six-Link Chain: Conversion Lines for Six Equating 4-. . . . . . 29 . c-e--w Raw Score . **. on El in 4/83 -. \\ Minus -0 --__ Equivalent Raw Score onE1 in lo/82 --Raw Score on El In 4/83 LEGEND : DATA ---. IRT NREAT -------IRT RPOS --Levine .__-. SPE"
}