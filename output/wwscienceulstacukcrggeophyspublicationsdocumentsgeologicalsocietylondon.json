{
    "title": "Fluid velocity fields in 2D heterogeneous porous media: empirical measurement and validation of numerical prediction",
    "publication_date": "1995",
    "authors": [
        {
            "full_name": "Rachel Cassidy",
            "firstname": "Rachel",
            "lastname": "Cassidy",
            "affiliations": [
                {
                    "organization": "School of Environmental Sciences, Geophysics Research Group, University of Ulster",
                    "address": {
                        "city": "Coleraine Derry",
                        "country": "N. Ireland",
                        "postcode": "BT52 1SA"
                    }
                }
            ]
        },
        {
            "full_name": "John Mccloskey",
            "firstname": "John",
            "lastname": "Mccloskey",
            "affiliations": [
                {
                    "organization": "School of Environmental Sciences, Geophysics Research Group, University of Ulster",
                    "address": {
                        "city": "Coleraine Derry",
                        "country": "N. Ireland",
                        "postcode": "BT52 1SA"
                    }
                }
            ]
        },
        {
            "full_name": "Philip Morrow",
            "firstname": "Philip",
            "lastname": "Morrow",
            "affiliations": [
                {
                    "organization": "School of Software Engineering, University of Ulster",
                    "address": {
                        "city": "Coleraine Derry",
                        "country": "Ireland",
                        "postcode": "BT52 1SA"
                    }
                }
            ]
        }
    ],
    "abstract": "The scale invariance of geological material and the consequent absence of a length scale on which to base the upscaling of measurements made on geological samples represent a serious challenge to the prediction of fluid behaviour in rock at economically interesting scales. Numerical simulation is an important tool for understanding constraints in this problem and current discrete fluid models in which complex boundary conditions can be represented have the potential for testing many possible upscaling schemes. At present, however, there are no accurate empirical data on the distributions of fluid velocities in complex, scale-invariant geometries, with which to validate such models.\nTo address this, fluid velocity fields in complex 2D media with fractal heterogeneity were measured. Digital models of rock geometries were created and translated into physical form using electric discharge machining and stereolithography. These physical models were then enclosed between parallel sheets of glass and Perspex forming a Hele-Shaw cell which was permeated with water, doped with small neutrally buoyant spheres and pumped at accurately steady and reproducible velocities. Local velocity vectors were estimated by the analysis of sequential images captured using high-resolution video. Precision digital control systems were used to move the cell relative to the camera and repeated measurements allowed the construction of full 2D velocity fields. The accuracy of the technique was assessed by comparison between automated and manual measurements, confirming the accuracy over approaching three orders of magnitude in velocity.\nThe results have been compared to the output of lattice Boltzmann (LB) simulations of flow in identical geometries, showing that the correlation between simulated and measured velocity fields is strongly dependent on the viscosity used in the numerical simulation. In particular, the LB scheme used in these tests is incapable of simulating correct viscosities for complex geometries. Some important effects are shown to be strongly viscosity dependent and it is concluded that some simulations may be able to predict the behaviour of high viscosity fluids only. Nonlinear effects between fracture and matrix flow are likely to be more important in these cases.",
    "full_text": "Though still the subject of some debate, it is widely accepted that, at least in many important situations, heterogeneity in the Earth's crust is scale invariant. In particular, matrix porosity (e.g. Katz & Thompson 1985;Jacquin & Adler 1987), fracture length distributions (e.g. Davy 1993;Sornette et al. 1993), the spatial distribution of fracture networks (e.g. Barton 1995;Odling 1997), fracture roughness (e.g. Brown 1987;Power & Tullis 1991;Power & Durham 1997) and aperture (e.g. Belfield 1994;Me Â´heust & Schmittbuhl 2001) have been shown to conform to power-law or fractal frequency-size statistics (see, for example, Bonnet et al. 2001). These properties together largely control the movement of fluids through the crust and understanding of this control is of primary importance in several economically and socially important areas of earth science, perhaps the subject of most current study being the efficient recovery of hydrocarbon from fractured porous reservoirs. The central aim of many such studies is the development of schemes for the extrapolation or interpolation of material properties across scales. Thus it would be extremely useful if, for example, accurate predictions of the permeability of a reservoir at a length scale of 10 km could made on the basis of samples taken from boreholes (e.g. with a length scale of 10 cm). It would be equally useful if the fracture connectivity through a reservoir, often controlled by relatively small structures, could be robustly interpolated from seismic sections which have very limited smallscale resolution. This scale-changing problem is seriously complicated by the scale invariance of material properties. Basically, so that it is possible to model the large-scale properties of a rock body without measuring every feature of that body, it is vital to assume a homogenization scale above which the rock mass properties can be assumed to be uniform. Thus, the average of the property under consideration must converge with scale. In fractal distributions, however, no representative sample size can be defined, there is no convergence of average properties with scale and simple extrapolation is impossible. This lack of a homogenization scale has profound implications for understanding and predicting the behaviour of fluids in the crust. Indeed, attempts to calculate accurately the expected yield from oil reservoirs, to predict and remediate groundwater pollution and to identify permanent, safe repositories for nuclear waste are inhibited by the inherent scale-invariance of many flow-relevant fields. Approaches to the solution of this problem, to finding a method for upscaling in complex, possibly scale-invariant geometries have been varied and field studies, laboratory investigations and numerical modelling have all been used. This paper concentrates on the latter and, in particular, on the validation of numerical simulations which are being used to test upscaling schemes.\n\nNumerical modelling of fluid transport in geological materials has become an important method of assessing the importance of complexity and in testing theoretical methods for dealing with the upscaling problem. Unfortunately, models based on continuum methods, such as traditional finite element simulations, rely on the definition of spatial averages and, therefore, must assume the existence of a homogenization scale. Relatively recently, lattice gas (Frisch et al. 1986) and lattice Boltzmann (Benzi et al. 1992) methods, in which a fluid is represented by the movement of particles on a regular grid in two or three dimensions, have been shown to yield solutions to the Navier Stokes equations. The ease with which complex boundaries and boundary conditions can be incorporated in such a lattice make them particularly suitable for solving flow problems in complex materials and it can be argued that they offer the greatest potential for progress in the upscaling problem since, at least in principle, they are not reliant on defining a representative sample size. The validation of simulations in complex geometries is, however, not straightforward and, while it has been shown that these models do make accurate predictions for flow in simple geometries, their predictions in complex and scale-invariant geometries has not been adequately demonstrated (see Dardis (1998) for a review). Models of flow in complex geometries are, generally, reliant on comparison with existing empirical data for their validation and such data, when available, are generally limited to non-unique, bulk measurements of properties such as permeability and are not adequate to validate complex numeric schemes fully.\n\nHere, an experimental system is described which has been developed to measure detailed 2D velocity fields in precisely defined complex physical geometries and an attempt is made to validate a numerical simulation in an identical system. Specifically, the following sections describe a procedure for the production of 2D physical models with precisely defined geometries and a method through which measurements of fluid velocity are made at precise locations in these geometries. The first stage involves the generation of 2D digital models of rock. These digital models are translated into a physical format using stereolithography or wire electric discharge machining (EDM) and encapsulated between transparent sheets to form a Hele-Shaw cell. The physical models are then placed in an experimental rig and permeated with water, pumped at steady and reproducible rates using a high pressure liquid chromatography (HPLC) pump. The experimental system includes a fully automated positioning system which moves the flowcell relative to a camera and illumination system to analyse small subareas. Velocity measurements are taken at each node of a regular grid using particle image velocimetry (PIV), a technique which uses the sequential imaging and tracking of neutrally buoyant particles to quantify the fluid velocity over a small volume. Schemes are also described which have been used to assess the accuracy of the velocity measurements. Some sample results are then presented and detail provided on the formats in which information about the geometry of the void space is stored, together with the corresponding components of the velocity vectors. The results are then discussed, with particular reference to their limitations and work is described which is presently being carried out with this system. Finally, a velocity field, measured for a fracture with self-affine fractal walls, is used to validate a lattice Boltzmann simulation of flow in the same fracture.\n\nEach physical model, whose geometry is defined in accordance with the objectives of a specific investigation, is first represented as a binary image of solid and void space. This Boolean image acts as the basis for numerical simulations while simultaneously providing a template for the production of the physical model using computerized machining and rapid prototyping techniques. The results allow the comparison of predictions of any particular numerical scheme with measurements of the velocity field in an analogous physical model.\n\nThe basis of the physical model is a Hele-Shaw cell, a pair of rigid, transparent plates separated by a narrow gap, used to examine fluid behaviour under quasi-two-dimensional conditions. In this application a 2 mm thick representation of the digital image is placed between the plates, allowing fluid motion throughout the void space to be quantified by saturating the model with a fluid containing reflective micro-particles and measuring the temporal displacement of particles. The physical model is generated in two ways. Simpler geometries with fewer component parts (e.g. Fig. 1a) are cut from aluminium plate by wire EDM. Geometries consisting of many unconnected elements, such as a model of rock matrix where numerous individual grains must be represented (e.g. Fig. 1b), are produced in plastic directly onto a glass plate by stereolithography. Both techniques ensure reproduction of the numerical model in a physical format as a 200 Ã 200 mm transparent flowcell containing a 2 mm thick layer of either resin or anodized aluminium matching the numerical geometry defined at the outset.\n\nThe geometry of a profile, taken across a fracture surface is well described by a self-affine fractal trace. The log-log plot of the power spectral density against spatial frequency for a selfaffine trace has the slope b, which relates to the fractal dimension D for the trace as (5 2 b)/2. Similar power-law relationships have been quantified in outcrop roughness with relationships spanning 10 23 -1 m for field measurements and 10 25 -10 22 m for laboratory measurements (Brown & Scholz 1985). Power et al. (1987) reported fractal scaling of roughness extending over 11 orders of magnitude by combining measurements of the power spectral density of fault surfaces over the wavelength of 10 25 -1 m and measurements of the roughness of the San Andreas Fault. A self-affine fractal geometry such as this was chosen as one of the models of complexity. It should be pointed out, however, that the detail of choice of rule for producing heterogeneous models is not a central concern of this work. This study need only guarantee a complex structure so that tests are made of the predictive capability of any numerical prediction. A self-affine fractal satisfies this fundamental criterion.\n\nThe trace is produced following the method described in Turcotte (1993). First n elements of a one-dimensional array are filled with Gaussian-distributed random numbers (h n ). A discrete Fourier transform is then taken of these values to map the n real numbers (h n ) into n complex numbers (H m ). The coefficients are given by\n\nh n e 2pinm=N\n\n(1)\n\nThe transform of the set produces a flat spectrum, where all amplitudes are equal within the statistical scatter. This flat spectrum must then be modulated by a given power law by filtering the resulting Fourier coefficients H m using the relation\n\nAn inverse discrete Fourier transform is then taken of the filtered Fourier coefficient to generate the trace. The sequence of points is given by\n\nThese points constitute the fractional Brownian noise and must then be normalized to the scale of interest and a 300 mm section extracted and output as a script file to AutoCAD where the model is constructed as a 2D image and prepared for cutting in 2 mm aluminium using wire EDM. This technique uses a copper electrode to generate an electric discharge and erode a trace (0.1 mm wide) through the 2 mm aluminium sheet. The resolution of the resulting model is on the order of 0.1 mm (c. 2% of the minimum amplitude). When complete, the aluminium sheet is removed from the machine, trimmed to size and anodized to prevent the aluminium oxidizing when saturated with fluid. The fracture sections are then bolted in place between transparent plates (Fig. 2) to match the digital image of the model. Where any disparities result from the manufacturing process, these are corrected in the numerical geometry to ensure complete correspondence and allow point-to-point comparison with numerical simulations of the flow fields.\n\nA complex, dual porosity digital model (Fig. 1b) is defined, comprising a porous matrix superposed with a fracture set, that permits the nonlinear interaction between matrix and fracture flow to be investigated (Mattisson et al. 1997;Dardis & McCloskey 1998a). Again, any debate as to the importance of the particular geometry employed here is irrelevant; it is simply complexity that must be guaranteed. Matrix porosity is represented by a distribution of nonoverlapping discs of different radii that are packed through a simulated compression using a discrete element method (Cundall & Strack 1979) until a specified porosity is attained. The strength of interaction among discs is controlled by elastic constants whose value is defined locally as a fractally correlated field which is mapped onto the distribution of discs, allocating an elastic constant to each disc based on its position within the field. The model causes discs with larger elastic constants to repel each other more strongly, producing looser packing and, therefore, a higher porosity in that area. Conversely, where the elastic constant is small, the discs contact and reduce porosity. The discrete element model compresses the discs both vertically and horizontally until the required porosity is achieved. Once the digital model of the matrix is completed using this technique, an area of 200 Ã 200 mm is removed and the coordinates of each disc are written to a formatted script file as a point with a given radius. A fracture network is then defined consisting of a distribution of line segments positioned randomly but with specified length, orientation and thickness. The precise rules employed are completely controlled by the modeller. Since the number-length distribution of natural fractures is, in general, well described by a power law (e.g. Bonnet et al. 2001), here such a fracture set is generated by defining lengths l n for random fractures according to:\n\nwhere x n is a random number between 0.0 and 1.0 and D is the fractal dimension of the length distribution. Here, three preferred fracture orientations were also chosen, with allowance for a random variation of 58 on the orientation of each. Fracture aperture is defined as a linear function of length. All fractures are then written to a script file which is used to reconstruct the pattern in AutoCAD. Initially, in AutoCAD, the completed matrix and fracture pattern are treated as separate objects. These are superposed, the fractures cut from the matrix and the resulting form extruded to a 2 mm thick layer in stereolithography (STL) format for production using stereolithography (Fig. 3). The basis of stereolithography is the use of a computer-controlled laser beam to polymerize a liquid photosensitive monomer formulation, layer by layer, according to a 3D representation of the object in a CAD system (Bernhard et al. 1994). To generate a model a glass base-plate (200 Ã 200 mm) is first placed on a platform and submerged in liquid resin to a depth of 0.1 mm. This provides a base on which the medium is built and subsequently forms one side of the flowcell. Following the digital map of the medium a laser then moves across the area, selectively curing areas corresponding to the solid areas of the model. Once a layer is complete, the model is again submerged in the resin to coat the previous layer and the process is repeated, continuing until the layer is 2 mm thick. The model is then removed from the vat, rinsed with alcohol to remove uncured resin and fully cured in a UV oven. It is estimated that the accuracy of the physical model produced using this technique is approximately 0.1 mm (c. 2% of the smallest diameter disc used).\n\nPrior to encapsulation by bonding a Perspex top plate, the physical model is compared with the digital image of the medium to identify flaws and then cleaned thoroughly to ensure no residue remains on the glass base-plate that would impair measurement. To bond the top plate, epoxy resin is prepared and applied across a 200 Ã 200 mm square in the centre of a frame covered with tensioned nylon mesh. This is repeatedly screeded using a spreader, both on the top and from beneath, so that every pore in the mesh is filled and all excess removed. The frame is then placed so the glued mesh rests 5 mm above the surface of the base plate and is then repeatedly rolled and depressed to apply small deposits of epoxy to the model through the pores of the mesh. When the top plate is applied and weighted these deposits flatten to cover each section with a thin layer which bonds to the top plate. The model is then left to cure for 48 hours before use.\n\nThe transparency of the flowcell allows fluid movement in the void space of each physical model to be visualized. Measurements of fluid velocity are made throughout each model by seeding the fluid with neutrally buoyant microparticles and applying an imaging technique which exploits the relationship between the shift in particle positions in temporally separate images and the velocity of the fluid. A strong steel rig houses the components of the measurement system (Fig. 4). The flowcell is attached to a pair of moveable trolleys on the central support of the rig which allow the position of the flowcell to be adjusted relative to the camera and lighting, which are mounted on stands to the front and rear. Moving the camera and lighting systems relative to the flowcell was unfeasible due to the requirement for two linked positioning systems to co-align the camera and lighting and the increased potential for damage to the camera charged-coupled device (CCD) sensor from persistent movement. The fluid system is arranged around the flowcell supplying a continuous, controlled flow for the duration of the experiment. These components ensure the systematic measurement of fluid velocity in small sub-areas to produce a full, high-resolution map of velocity throughout each model.  8) BNC (Bayonet-Nut-Connector) cable to frame grabber and CPU.\n\nA reinforced, clamped unit seals the flowcell and provides inlets and outlets for the fluid, which is circulated through the flowcell at a constant rate for the duration of each analysis. A 10 mm Perspex back plate forms the base of the unit to which foam-lined, brass clamps are fitted that compress against the edges of the flowcell and seal it. The clamps at the top and base of the unit are fitted with connectors to the inlet and outlet pipes. The clamped unit bolts vertically to trolleys at the front of the rig and fluid is pumped from the base of the system through the flowcell in a constant, controlled flow with a HPLC pump. The volumetric flow rate through the rig is kept low (c. 2-4 ml min 21 , depending on model porosity) to ensure flow is globally laminar with a parabolic velocity profile between the plates.\n\nThe fluid is seeded with 10 mm neutrally buoyant, hollow, glass spheres that follow the movement of the fluid and allow fluid motion to be visualized. The particles contrast strongly when backlit in water and are large enough to be visible within the measurement limits of the camera (for an 8 mm field of view a pixel is c. 7 mm wide). The particles are added to 100 ml of water along with 1 ml of weak detergent solution to inhibit flocculation of the marker particles, producing a well-mixed suspension which is gradually added to another 1.5 l of water in the system, monitoring particle concentration to ensure particle density is optimal for analysis (a density of c. 20 particles per mm 2 ).\n\nParticle image velocimetry, a non-intrusive, image-based measurement technique, has been widely applied in engineering for 20 years (for a review see Grant (1997)). The technique involves seeding a fluid with neutrally buoyant particles which have a strong contrast in refractive index when compared to the permeant so that fluid motion can be visualized by following their progress. Using a camera and illumination system to record the displacement of the moving particles in temporally separated images, the 2D velocity of the fluid is determined remotely, without interfering with its motion. In this system, fluid velocities are low (the maximum measured velocity is c. 15 mm s 21 ) making it feasible to capture single exposures of the particle field in each image frame using a continuous light source and a precisely triggered camera. This simplifies the process of image analysis used to determine the displacement of the moving particles, as the order in which a sequence of images is taken indicates the direction of flow. This is not possible in high-speed flows where, due to limitations in camera speed, multiple particle images are captured in each image frame using a pulsed light source.\n\nThe camera and illumination set-up involves indirect cross-focused backlighting of a subarea of the flowcell, which is imaged by a mega-pixel camera with a minimum field of view of 8 mm, focused on the plane central to the flowcell and recording the positions of the flow marker particles at intervals. To maximize the contrast between the micro-particles and the fluid, a custom-designed back-lighting system is used, which provides two intense, collimated light sources that are cross-focused precisely on the measurement plane of the camera, illuminating the marker particles moving through that area. The region of interest for measurements is a narrow (c. 50 mm thick) layer centred on the midpoint of the 2 mm space between the plates of the flowcell. Under laminar flow conditions this midpoint represents the peak of the parabolic velocity profile which develops between the plates and, thus, measurements are confined to a zone across this peak that is 2.5% the width of the aperture between plates. By restricting the camera and lighting to ensure that only particles in this narrow region of maximum velocity are recorded, a quasi-two-dimensional measure of velocity is possible. Particles beyond this region are out of focus, allowing their exclusion using image-processing techniques. Any out-ofplane motion of particles at the boundaries of the 50 mm layer has minimal impact on the measured velocity, as the cross-correlation algorithm returns an averaged value based on the best fit to the shift of a population of c. 20 marker particles in a 1 mm 2 interrogation area. Any particles not present in both of the image pairs will add noise to the returned correlation field, but repeated measurement and stacking (described in the next section) negate this effect.\n\nThe camera, a mega-pixel, analogue Adimec MX12P, is fixed at the front of the flowcell unit and focused on the central plane. The CCD array comprises 1024 Ã 1024 pixels and delivers up to 30 frames per second through a composite video output signal to the frame grabber memory. A telecentric lens with a 2X-extender sets the minimum field of view to 8 Ã 8 mm and ensures a constant magnification across this area, eliminating the viewing angle error (fisheye effect) inherent with conventional lenses. The minimum field of view determines the measurement resolution of the system, as the flow marker particles must be clearly visible within the 1024 Ã 1024 pixel image frame (a pixel resolution of 7.8125 mm). Image acquisition is in a controlled mode, where an externally generated trigger input signal supplied from the controlling software triggers the camera to acquire each image. This ensures that velocity measurements, made on the basis of comparing particle positions in temporally separate images, relate precisely to a velocity in mm s 21 .\n\nThe flowcell is measured sequentially by defining 8 mm square sub-areas and positioning the flowcell relative to the fixed imaging system so that each lies within the field of view of the camera. Each sub-area is further subdivided into 1 mm interrogation areas (IA) within which particle displacements will be integrated to determine the local velocity, spaced at 0.5 mm intervals.\n\nThe drive system controls the analysis of the entire flowcell by systematically moving each sub-area into the field of view of the camera and engaging position correction software to ensure the location corresponds to that section of the digital image within tolerance. The tolerance is set at 0.1 mm throughout these experiments to correspond with the resolution of the model construction techniques. The flowcell unit is attached to a pair of trolleys resting on linear guide ways and is driven horizontally and vertically by geared stepper motors, bringing the flowcell to the correct position relative to the camera. The stepper motors turn a prescribed amount producing an up-down, left-right shift of the flowcell unit. All movement is controlled by generating pulsed commands to the stepper motors from within the software using a digital I/O (input/output) and counter timer, each output pulse producing a shift of 9.26 mm in the position of either trolley.\n\nTo initiate the analysis of a flowcell, an 8 mm reference area is located in the digital model and the same area is located in the physical model. The flowcell is then moved to bring this area within the field of view of the camera. The coordinates of this area are then set within the software as a reference and all movements to other sub-areas throughout the analysis are relative to this. On each move to a new sub-area, the accuracy of the position relative to the digital model is assessed. A software routine assesses the goodness of fit between an image of the field of view after each movement of the flowcell and the corresponding section of the digital model and ensures that each sub-area is located, again within tolerance, and automatically adjusted where required.\n\nCross-correlation operates on pairs of images, separated by a time interval that is adjusted iteratively to ensure maximum measurement accuracy. The technique relies on a particle population captured in the first of a pair of images, captured at time t, also being present in the second image captured at t Ã¾ Dt, so that comparing images allows the offset corresponding to particle displacement during the interval to be quantified. One of the primary considerations in the analysis was, therefore, to devise a means of optimizing the separation time, Dt, between images to suit the range of velocities in each model. The definition of the size of the IA and the separation time are crucial to the success of this approach and must be optimized to achieve a balance between maximizing the number of particles common to both images while ensuring a significant particle displacement between images. The IA was restricted to 1 mm 2 and velocity variations were accounted for by applying an adaptive separation time. Within any 8 mm sub-area, there can be a significant variation in fluid velocity; for example a slow moving pore throat with velocities ,1 mm s 21 can occur in proximity to a high velocity fracture with velocities of c. 15 mm s 21 . This precludes using a single separation time per sub-area and requires, instead, that each IA be dealt with individually to ensure the displacement between images for any IA is within a range of 10-40 pixels in each direction. The returned velocity vectors in each IA are assessed to locate those with minimal displacement and these are resampled at separation times up-scaled to ensure a significant displacement. In sub-areas exhibiting a broad range of velocities, up to six different separation times may be necessary to analyse the velocity field accurately.\n\nImage quality is critical to accurate measurement and all models are affected to some degree by areas that cannot be measured due to poor contrast between the marker particles and the fluid. This is generally due to impurities such as residual resin from the stereolithographic process on the transparent plates or diffraction of light on some edges of the physical model from back illumination. Identifying these areas and excluding them from analysis prevents meaningless attempts at velocity measurement and provides a measure of clarity that can be used in post-processing to eliminate erroneous measurements. Prior to a measurement run, all IA within each model are examined automatically and assigned a DIRT (Degraded IA Removal Threshold) value that refers to the fraction of each image that is over-illuminated. The basis of determining the DIRT for each IA is that all clean, fluid-filled areas of the model should have a low greyscale value. By assessing the fraction of the void space in each IA with a high greyscale value, out of a worst case where the entire area is obscured, problem areas can be excluded from analysis.\n\nThe initial step in extracting velocity information from an image pair involves subtraction and thresholding to remove noise and isolate the flow marker particles. Each pixel has a greyscale value from 0 (black) to 255 (white). Under illumination, the flow marker particles appear white (high-greyscale values) while the fluid appears grey and the solid areas either black or white (dependent on whether a resin or aluminium material layer is used). Direct subtraction of the image pair isolates all altered pixels between images and eliminates stationary values. The main disparity between the images is caused by the moving particles, creating a strong signal by their presence at a location in the first image and their absence in the next. Those particles which are in focus (within the depth of field) generate the strongest signal and changes due to noise are excluded by careful optimization of a threshold. The threshold assigns a unit value to all subtracted pixel values above a limit and a zero to those below. This is performed for both the positive and negative returns from subtraction, the former corresponding to the first image in the pair and the latter to the second image. The threshold is carefully optimized to ensure that only particles within the depth of field, which have the highest greyscale levels, are included in the analysis. Following this process, two binary images are output containing the isolated flow marker particles corresponding to the first and second images captured.\n\nFollowing primary processing, individual IA (corresponding to an area of 128 Ã 128 pixels) in each of the processed image pairs are extracted and analysed using a frequency domain crosscorrelation algorithm. The maximum value in the resultant 2D correlation function represents the physical displacement, in pixels, between image sections producing the best fit and relates directly to fluid velocity in the area. A single analysis is not, in general, sufficient to obtain an accurate determination of velocity for all IA in any sub-area. The density of marker particles in an IA may be too high or low during capture, resulting in a spurious peak being returned. In addition, the effect of resin residue in narrow pore spaces will affect the quality of the image for analysis.\n\nIn order to strengthen the signal-to-noise ratio of the cross-correlation and obtain a more accurate velocity measurement, multiple image pairs are analysed and the successive correlation fields for each IA are stacked by addition.\n\nAfter the correlation fields for five image pairs are stacked, the vector field for the sub-area is assessed, first to determine areas requiring analysis at longer separation times to account for lower velocity regions and, secondly, to identify areas requiring further analysis to improve the signal peak due, in general, to contamination of the field of view. After the required iterations, the accumulated peak for each IA is interpolated to a sub-pixel scale to provide a higher resolution estimate of the location of the cross-correlation maximum.\n\nThe x and y-components of velocity are converted from the displacement in pixels, d p , to velocity, v, (in mm s 21 ) by the relation\n\nwhere Dt is the separation time and p is the size of a pixel in millimetres (for an 8 mm field of view captured on a 1024 2 CCD, this corresponds to 0.0078125 mm).\n\nThe PIV method is the cornerstone of the system described here and its accuracy is clearly crucial to the success of the project. This accuracy was tested thoroughly by comparison of the velocities measured by the automated cross-correlation technique for different locations and velocities in the flowcell, with measurements taken by hand from a live video of flow in the same areas for a wide range of flow rates (Fig. 5). This exercise was the real test of the quality of the experimental system and demonstrates excellent agreement over almost three orders of magnitude, confirming confidence in the robustness and accuracy of the technique.\n\nEach measurement run takes up to 48 hours and produces an output data file containing on the order of 80 000 velocity measurements each referenced to the centre of the corresponding IA.\n\nA range of digital models and equivalent physical models has been produced and the fluid velocity fields measured. To disseminate these data as widely as possible, all results from the project, along with full details of the model geometries, are archived on the web at http://www.errigal. ulst.ac.uk/NERC_m2M/index.htm. Formatting standards have been applied to facilitate the use of the measured velocity fields and the numerical geometries and all are freely available to download.\n\nThis paper confines a description of the measured geometries to those described in the preceding section. Data are displayed as vector plots overlain on the digital image of the medium; each vector is centred on the corresponding IA and is scaled according to colour and length. Areas for which accurate measurements do not exist are left vacant; interpolation is never used.\n\nThe completed velocity field for the fracturedporous medium is shown in Figure 6a. The measurement resolution of this approach allows the strong interaction between matrix and fracture flow to be observed clearly and measured. The channelling of flow is apparent, as are the intricacies of fluid interaction at pore junctions where fractures act as preferential flow paths into which the matrix fluid is fed, enhancing flow in some areas while retarding it in others. While the fractures dominate flow, the matrix contribution, connecting areas of high permeability is significant. Observations such as this highlight the inadequacy of modelling techniques that consider matrix and fracture flow separately and simply sum the flow fields to obtain a global measure of permeability. They support the findings of Mattisson et al. (1997), who investigated the effect of a discrete fracture on a porous matrix composed of sintered glass spheres and found the combined permeability to differ by one order of magnitude from that of the matrix and fracture considered separately.\n\nThis model also exemplifies some of the problems with resin residue that have emerged with recent models produced using stereolithography. This is manifested in the areas of the model for which there are no plotted vectors, predominantly in the narrow interstices of the matrix where accumulated hardened resin could not be removed from the glass base-plate. This is attributed to a change in the resin and equipment used by the stereolithographers and is a problem currently being worked on. As will be seen later, these deviations in detail between the digital models and the physical models produced by stereolithography mean that comparisons between observed velocity fields in these models and those predicted by numerical simulations for the same geometry were not carried out. For these comparisons to be meaningful and informative it is vital that the flow paths in the physical and simulated media are identical. In fact, the predictions of the numerical model employed here were very significantly different than the observed velocity fields for the stereolithographic models. This strongly suggests that the small variations in pore throat geometries produced by weaknesses in the production process may have unexpected and significant effects on fluid transport. This interesting phenomenon is the subject of more detailed study at present and will be reported elsewhere.\n\nThe models produced using wire EDM, consisting of larger sections that are emplaced manually in the flowcell, are not affected by contamination to the same extent as those produced by stereolithography. A typical velocity field is shown in Figure 6b. The absence of measurements along the edges of the fractures here are the result of flow stagnation close to the fracture wall. While the velocity measuring techniques employed here, by increasing the image separation times, are able to measure extremely low flow speeds, the time taken for the measurements increases rapidly as the minimum measurable velocity is decreased. Lowering the minimum measurable velocity is also subject to diminishing returns since more and more time is spent on measuring smaller and smaller areas near the fracture walls. For the purposes of this study the lowest velocity measured here was Fig. 6. Measured velocity fields: (a) (i) the velocity field for the porous-fractured geometry shown in Figure 1b and beneath (ii) an enlarged section showing the complex interaction between fracture and matrix pore space; (b) (i) velocity field for the model of three fracture apertures (Fig. 1a) and beneath (ii) enlarged sections of a fracture with and without offset.\n\nthought to be an acceptable compromise between data volume and areal coverage, and the increased time required to measure slow speeds. Two enlarged sub-areas are shown of fractures with and without offset between the traces. With a constant actual aperture at zero offset, the effective aperture and, thus, the flow field varies significantly with the angle of the fracture trace relative to flow direction.\n\nThe quality and resolution of this model were high, as wire EDM ensures very accurate reproduction of the fracture traces. In addition, any errors in the positioning of the aluminium sections between the transparent sheets can be corrected in the digital model to ensure correspondence. The authors are confident that the error in the position of any point between the numerical and digital model is not greater than 0.1 mm. As the physical model corresponds very closely with the digital model of the medium, a comparison of the data with the simulated velocity field is possible and work on the validation of a numerical model is presented in the next section.\n\nThis section investigates the effectiveness of a 2D lattice Boltzmann (LB) fluid model of the fluid velocity field in a complex fracture whose walls consist of self-affine traces with variable offset (Fig. 2). The LB scheme is a development of the earlier lattice gas models in which the fluid is represented by discrete particles moving between the nodes of regular grid. Particle interactions take place using a set of rules which are designed to conserve mass and fluid momentum. Lattice Gas (LG) models have been shown correctly to reproduce macroscopic fluid behaviour for large grids. In the LB scheme, the discrete particles of the LG scheme are replaced by the probability of the presence of a particle at any node, a technique which significantly reduces the computation time required for any simulation. In this study, the lattice Bhatagnar-Gross-Krook (BGK) scheme (Qian et al. 1992) is tested, in which the fluid is represented by a real valued particle density N i (x, t) at location x, time t, and moving in direction i, on a regular hexagonal lattice. The evolution of N i at each time step is given by:\n\nwhere e i is the lattice unit vector in direction i. The first term on the right represents the advection of fluid and the second term results in the relaxation of highs or lows in fluid density towards the equilibrium density, N eq i , at a rate which is governed by the relaxation parameter v. Thus, v is a surrogate for the viscosity of the fluid, large values resulting in rapid decay of density anomalies (low viscosity). The kinematic shear viscosity, v, is related to v, by:\n\nIncreasing v increases the rate at which momentum is transferred on the lattice, lowering viscosity, which vanishes at v Â¼ 2. Conversely, decreasing v reduces the transfer of momentum and increases viscosity. This scheme has been tested extensively (see for example Dardis (1998) and Dardis & McCloskey (1998a,b)) and validated by comparison with analytical solutions to the flow equations in simple geometries for which solutions are possible, and with available bulk, experimental measurements of permeability from rock samples. In these tests, the lattice BGK scheme has given accurate predictions. Validation of predicted fluid velocities in media with geologically feasible complexity is addressed here.\n\nFigure 7 shows two velocity fields for an enlarged sub-section of the model shown in Figure 1a. The velocity field for the measured physical model is shown in Figure 7a and the predicted velocity field for the same section of the digital model is shown in Figure 7b. Qualitatively, the correspondence is excellent; the vectors coincide for all areas, signifying that the LB simulates the behaviour of the fluid realistically. The colour scale differs, however, indicating a difference in the structure of the flow field, with simulated velocities in areas of the flow field significantly higher than the equivalent measurements. As the measured velocities have been extensively validated, this is clearly due to inaccuracies in the numerical simulation and, as the difference is systematic and the flow field is qualitatively similar, the difference may be attributable to incorrect definition of the model parameters. In the measured velocity field, the area of fracture over which the majority of flow takes place is noticeably smaller than that of the simulated velocity field. The correlation length in the simulated flow field is longer; the effect of the constricted part of the fracture extends over a larger area. This figure is the best possible match between predicted and observed velocities; to obtain this fit the viscosity of the fluid in the simulation has been systematically decreased, bringing the predicted field closer to the observed. For this model, however, the simulation becomes unstable for v . 1.85 and the simulation is not able to reproduce the correct velocity field accurately.\n\nTo quantify this mismatch, the measured ycomponent of velocity for the fracture section is plotted against the predicted y-component of velocity in Figure 8a. The main points made here are equally valid for the x-direction. While the plot of predicted against observed velocity (Fig. 8a), is poor, it is not random; there is a clear, underlying structure. It was concluded above that the failure to model the velocity field in the complex geometry correctly is due to inadequate resolution of viscosity in the numerical simulation. The structure in Figure 8a must then be due to a systematic effect of the incorrect value of viscosity, probably reflecting the geometry of the area under study. If this were so, it should be possible to reproduce this non-random structure by deliberately simulating the field with the wrong Fig. 7. Enlarged sections of the EDM model in Figure 1a showing (a) the measured velocity field and (b) the predicted velocities at the same locations. The correspondence is good in as far as the fluid accelerates in more constricted parts of the channel but the correlation length is greater in the predicted field, with the fluid velocities relatively greater at the edges of the fractures compared with the measured field. This is consistent with a viscosity difference, the viscosity being higher in the simulated field.\n\nviscosity. Consider the situation in which a fluid flowing in the geometry of Figure 7 is simulated at a low viscosity but one which is manageable with the LB scheme. Let this be called the 'observed' field. Let this field be simulated using an v value which is deliberately set too low and then the results plotted as above (Fig. 8b). There is quite a remarkable similarity between this plot and Figure 8a.\n\nThis exercise illustrates a couple of important points. First, the lattice BGK model is unable to simulate low viscosity flows in complex media correctly. Secondly, and of more importance, the inaccuracies produced by the viscosity mismatch are non-random and are related to the geometry of the medium. This becomes clearer when the relationship of the mis-prediction is exposed by identifying the location of the worst points. Each lobe corresponds to a particular area of the fracture, the greatest deviations relating to the narrower parts of the channel and relating to the boundaries of the fluid with the medium. Chen et al. (1991) showed that the lattice BGK scheme incorrectly modelled the fluid-solid interaction for v = 1.0. Here, it is shown that the limitations on the available viscosities severely degrade the prediction accuracy and, what is more, this degradation is dependent on material heterogeneity. These results suggest strongly that the promise of accurate prediction of fluid transport in complex materials using lattice Boltzmann simulations will require a more physically sound control on fluid viscosity.\n\nThe core objective of this project was to provide reliable, high-resolution data on fluid velocity within known 2D geometries as a resource for the validation of existing numerical models. In addition, a first attempt was made to validate a numerical simulation using these data.\n\nA methodology has been realised to define a numerical model as a binary image and translate it to a physical model, a transparent flowcell, containing a precise copy of the digital model generated using automated machining techniques. This flowcell is then incorporated in a purpose-built experimental rig, which controls analysis by maintaining a constant, controlled flow of seeded fluid through the flowcell and sequentially positioning the flowcell relative to a camera and illumination system to analyse velocities in sub-areas of the model, using an algorithm based on cross-correlation. A positioning system controls the systematic location and measurement of all sub-areas to produce a complete, accurate map of velocity. Checks imposed at all stages ensure measurements are as accurate as possible and correspond to precisely defined points within each model. The system is now fully operational and capable of measuring detailed velocity fields throughout a range of complex geometries, incorporating the realistic characteristics of rock. Considerable time has been spent checking and optimizing the system and ensuring the accuracy of the technique and the quality of the output datasets. It has been shown that the system produces a detailed, high quality assessment of the distribution of fluid velocities in a complex medium with scale-invariant heterogeneity.\n\nThese datasets consist of a map of up to 80 000 points across the void space of each medium, each with an associated velocity vector. Quality control in the form of careful checks of the correlation between automated and manual measurements have been undertaken. The results of these checks show that the method returns very accurate velocity measurements that are reliable over a wide range of velocities. Furthermore, it is believed that many of the areas excluded in the analysis because of their slow velocities can be measured accurately if longer image separations are allowed. It is only high velocity measurements that are limited by the speed of the camera.\n\nIt is believed that this system has produced accurate data measurements of fluid velocities in complex media for which the material geometry is known accurately. The results should be of real benefit to the numerical modelling community. Preliminary comparisons with a modified lattice Boltzmann model have highlighted problems with the scheme and identified several issues for further consideration. Based on the results of the comparison of the simulated velocity fields with real data, the efficacy of the relaxation parameter, v (which controls the transfer of momentum on the lattice), in controlling the viscosity of the modelled fluid has been shown to be limited and, particularly in deviations from unity, v has been shown systematically to distort the predicted velocity fields in proximity to the solid boundaries. This is consistent with the findings of Li et al. (2003) who showed the BGK scheme to predict permeability wrongly for v = 1.0. This study emphasizes the strong and nonlinear control of viscosity on the flow field and the need, in real applications, to model the fluid viscosity properly.\n\nSeveral limitations of the system are currently being resolved. A recent change in the resin and equipment used by the manufacturers has resulted in deposits of hardened resin residue lodged among the solid elements in certain models, which cannot be removed in all instances and thus make comparison of measurements with predictions in the originally defined geometry impossible. Blocking a pore throat significantly alters the velocity field, especially in complex geometries close to the percolation threshold and, as it is impossible to alter the digital model file to match, the use of these data for validation is limited. This problem has, however, produced the interesting result that small differences in model geometry produce very significant differences in the flow pattern, resulting in changes in global permeability. This suggests that, in general, the problem of predicting bulk permeability by reductionist methods, such as LB modelling of accurate representations of rock geometry, are unlikely to be successful.\n\nComparisons of measurement with predictions for the models analysed to date are consequently restricted to those models produced using wire EDM. As accurate copies of the media were obtained before the recent deterioration in quality, this issue should be resolved either through negotiations with the company or sourcing a new supplier. It is believed, however, that stereolithography is a potentially powerful technique for addressing many important problems in the earth sciences. Further work in fluid transport in complex media includes the examination of the effects of the growth of biofilms on fluid flow through pore throats and how this affects total medium permeability with time. The possible construction of 3D models whose material properties and internal geometry are known in detail, also offers great possibilities for the examination of the effect of stress and pore fluid pressure on permeability. Both of these issues are currently the subject of much research.\n\nIn conclusion, accurate measurements of fluid velocity fields for complex media with fractal heterogeneity have been produced. These data, with the appropriate meta-data, are stored on the web and are available for general use. Results to date have shown, first, that very small changes in material geometry can produce very significant changes in global flow patterns and significantly alter global permeability. This nonlinear control of flow at large scales by changes at small scales further compounds the upscaling problem. Secondly, it has been shown that a standard LB simulation for flow in a complex medium fails to reproduce the detail of the fluid velocity field. The source of the error has been identified as unphysical representation of fluid viscosity, which incorrectly models the fluid -solid interaction. This indicates the need for development of more physically sound techniques for viscosity representation in the LB models."
}